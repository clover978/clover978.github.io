{"meta":{"title":"clover978","subtitle":"","description":"","author":"clover978","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2021-01-06T01:11:33.000Z","updated":"2021-01-06T01:12:35.121Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-01-06T01:12:03.000Z","updated":"2021-01-06T01:12:33.378Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"COCO dataset format","slug":"COCO-dataset-format","date":"2021-01-06T11:28:43.000Z","updated":"2021-01-06T11:46:06.267Z","comments":true,"path":"2021/01/06/COCO-dataset-format/","link":"","permalink":"http://example.com/2021/01/06/COCO-dataset-format/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556- annotations - &#x27;info&#x27;: dict - &#x27;licenses&#x27; list - &#x27;images&#x27; [ &#123; - &#x27;filename&#x27;: str - &#x27;id&#x27;: int - &#x27;height&#x27;: int - &#x27;width&#x27;: int - &#x27;flickr_url&#x27;: str - &#x27;license&#x27;: int - &#x27;coco_url&#x27;: str - &#x27;date_captured&#x27;: str &#125;, ... ... ] - &#x27;annotations&#x27; [ &#123; - &#x27;segmentation&#x27;: list - &#x27;num_keypoints&#x27;: int - &#x27;area&#x27;: float - &#x27;is_crowd&#x27;: bool - &#x27;keypoint&#x27;: - x1, y1, k1 # k=0 不可见未标注；k=1 不可见标注； k=2 可见标注 - x2, y2, k2 - ...... - &#x27;bbox&#x27;: list - ( x1, y1, w1, h1 ) - ( x2, y2, w2, h2 ) - ...... - &#x27;category_id&#x27;: int - &#x27;image_id&#x27;: int - &#x27;id&#x27;: int &#125;, ... ... ] - &#x27;categories&#x27;: [ &#123; - &#x27;id&#x27;: int - &#x27;keypoints&#x27;: - kpt-name1 - kpt-name2 - ... - &#x27;skeleton&#x27;: - (pair1_index1, pair1_index2) - (pair2_index1, pair2_index2) - ... - &#x27;name&#x27;: &#x27;person&#x27; - &#x27;supercategory&#x27;: &#x27;person&#x27; &#125; ]","categories":[{"name":"others","slug":"others","permalink":"http://example.com/categories/others/"}],"tags":[{"name":"COCO","slug":"COCO","permalink":"http://example.com/tags/COCO/"}]},{"title":"Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition","slug":"Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition","date":"2021-01-06T06:10:48.000Z","updated":"2021-01-06T06:13:40.906Z","comments":true,"path":"2021/01/06/Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition/","link":"","permalink":"http://example.com/2021/01/06/Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition/","excerpt":"Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition CVPR 2016 这篇文章提出了一种使用车辆 3D box 作为网络输入，来识别车型的方法。首先检测出车辆的 3D包围框，然后展开成 2D 图形，最后加入视角信息作为额外输入进行训练。同时还发布了一个新的数据集 BoxCars","text":"Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition CVPR 2016 这篇文章提出了一种使用车辆 3D box 作为网络输入，来识别车型的方法。首先检测出车辆的 3D包围框，然后展开成 2D 图形，最后加入视角信息作为额外输入进行训练。同时还发布了一个新的数据集 BoxCars 3D 包围框：文中没有介绍获得车辆的 3D 包围框 的方法。这是作者之前的一个工作。Automatic camera calibration for traffic understanding 车辆图片平展：获得 3D 包围框后，将其展开成 2D 图像，具体方法类似于将立体的纸盒子在平面上铺开。很简单的一个思路。 车辆的视角信息：作者用三个向量来对车辆的视角信息进行编码。分别是 车尾-车头向量、车内-车外向量，车底-车顶向量。通过这三个向量，可以完整表示出车辆的视角。 栅格化包围框：作者同时提出另外一种方法描述车辆的视角。将车辆的矩形包围框中用四种颜色进行表示。其中 车顶用黄色，车侧用红色，车头/车尾同蓝色，其他位置用白色。这样一幅新的栅格化图像同样可以确定出车辆的视角。 平铺图 + 视角辅助信息 训练：平铺图直接进入 CNN 进行卷积操作；视角编码通过 6x6 的矩阵表示，使用三向量表示的情况下，矩阵的第一行表示 3 个二维向量，其他行用 0 填充；使用栅格化包围框表示的情况下，直接将 bbox rescale 成 6x6 的矩阵。文中只说 view encoding 是加在卷积操作之后，但是具体怎么加没有说明。 Bbox Cars 数据集：作者同时还公布了一个新的数据集，数据集中的图片是从道路车辆监控视频中获取的，主要特点就是包含了车辆的 3D box 信息。 实验结果：作者提出的方法需要数据集标注了车辆的 3D box 信息，具体怎么在 CompCars 数据集上做的实验阐述的不太清楚，下面是实验结果： top1 top5 baseline 0.767 0.917 ours 0.848 0.954","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"A Large-Scale Car Dataset for Fine-grained Categotozation and Verification","slug":"A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification","date":"2021-01-06T06:10:34.000Z","updated":"2021-01-06T06:12:08.456Z","comments":true,"path":"2021/01/06/A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification/","link":"","permalink":"http://example.com/2021/01/06/A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification/","excerpt":"A Large-Scale Car Dataset for Fine-grained Categotozation and Verification CVPR 2015 这篇文章提出了一个大型的数据集 CompCars，里面包含了从网络上收集的 13.6w 张整车图片， 2.7w 张车辆局部图片，还有从监控场景下拍摄的 5k 张车辆正脸图片。 实验部分使用这个数据集分别用于 车辆细粒度分类、车辆属性所识别、车辆验证 三个任务。","text":"A Large-Scale Car Dataset for Fine-grained Categotozation and Verification CVPR 2015 这篇文章提出了一个大型的数据集 CompCars，里面包含了从网络上收集的 13.6w 张整车图片， 2.7w 张车辆局部图片，还有从监控场景下拍摄的 5k 张车辆正脸图片。 实验部分使用这个数据集分别用于 车辆细粒度分类、车辆属性所识别、车辆验证 三个任务。 CompCars 数据集： 品牌（Make）：163 车型（Model）：1716 年份（Year）：4455 整车图片数：136,727 F ：18431 R ：13513 S ：23551 FS：49301 RS：31150 局部图片数：27,618 outter part: headlight, rearlight, fog light, air intake inter part: console, steering wheel, dashboard, gear lever Fine-grained classification 数据集： 车型： 431 Train Test Total 图片数（整车） 16016 14939 30955 Baseline： model: overfeat dataset: Fine-grained classification result: view F R S FS RS all top1 0.524 0.431 0.428 0.563 0.598 0.767 top5 0.748 0.647 0.602 0.769 0.777 0.917 Make 0.701 0.521 0.507 0.680 0.656 0.829 单视角识别车型： RS 效果最好； 单视角识别品牌： F 效果最好； 多视角 Baseline： model: 0.767, make: 0.829 Supplementary experiment:在整个 CompCars 数据集上补充一个实验，比较三种模型的效果 | model | AlexNet | Overfeat | GoogLeNet | |- |- |- |- | | top1 | 0.819 | 0.879 | 0.912 | | top5 | 0.949 | 0.969 | 0.981 |","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition","slug":"Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition","date":"2021-01-06T05:54:07.000Z","updated":"2021-01-06T06:06:22.166Z","comments":true,"path":"2021/01/06/Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition/","link":"","permalink":"http://example.com/2021/01/06/Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition/","excerpt":"Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition arxiv(Baidu) 这篇文章的思路是 attention + 度量学习。其中，attention 部分跟 SENet 如出一辙；度量学习比较像 contrastive loss。文章中定义了四种类型的特征，sasc, sadc, dasc, dadc, （s: same, a: attention, d: different, c: class），进行度量学习的时候，定义只有 sasc 是正样本，约束相同类别在相同attention区域学习到相似的特征。 在 CUB 数据集的准确率达到了 86.5。从 这篇文章 和 MSRA的WS-LAN 来看，attention + 度量学习 的方法是细粒度分类里面比较好的一个研究方向。","text":"Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition arxiv(Baidu) 这篇文章的思路是 attention + 度量学习。其中，attention 部分跟 SENet 如出一辙；度量学习比较像 contrastive loss。文章中定义了四种类型的特征，sasc, sadc, dasc, dadc, （s: same, a: attention, d: different, c: class），进行度量学习的时候，定义只有 sasc 是正样本，约束相同类别在相同attention区域学习到相似的特征。 在 CUB 数据集的准确率达到了 86.5。从 这篇文章 和 MSRA的WS-LAN 来看，attention + 度量学习 的方法是细粒度分类里面比较好的一个研究方向。 OSME (One-Squeeze Multi-Excitation Attention Module)：这一部分，按照我的理解，可以说是完全照搬的 SENet，没有任何创新点。作者说明了 SENet 中 SE module 的原理。文中提出的网络结构，将提取到的特征，分别经过两个 SE module，得到的两个 re-weighted feature map，称为 attention1， attention2.我理解的 OSME module 与 SE module 的区别就是一个是单路的，一个是双路的。 Multi-Attention Multi-Class Constraint：这一部分，作者使用了度量学习的方法，每次训练的时候，网络会输入 2N 对图片，其中每一对图片都来自于同一类别。然后一对图片分别经过 OSME 的上半支和下半支，这样可以得到四种类别的特征。sasc, sadc, dasc, dadc，在 softmax 的基础上，作者对 hinge loss 进行改进，提出 MAMC Constraint，四种特征。挑选出一个特征作为 anchor，然后分三种情况： 正样本是 sasc， 负样本是 {sadc, dasc, dadc}; 正样本是 sadc， 负样本是 {dadc}; 正样本是 dasc， 负样本是 {dadc};在每种情况下，定义出 MAMC loss，最小化与正样本的特征距离，最大化与负样本的特征距离。 实验结果： acc VGG19 79.0 ResNet50 81.7 ResNet101 82.5 ResNet-50 + OSME 84.9 ResNet-50 + OSME + MAMC_1 85.4 ResNet-50 + OSME + MAMC 86.2 ResNet-50 + OSME_3 + MAMC 86.3 ResNet-101 + OSME + MAMC 86.5 RACNN 85.3 MACNN 86.5 OSME_3: 使用 3 个 attentionMAMC_1：定义 MAMC loss 的时候，只区分第一种情况。 使用 ResNet50 作为 backbone， OSME 提升了 3.2%， MAMC 提升了 1.3% 的准确率。值得一提的是，相比于 MACNN，文中提出的网络结构要简单的多，在效率上具有很大的优势。","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Fine-grained Image Classification by Visual-Semantic Embedding","slug":"Fine-grained-Image-Classification-by-Visual-Semantic-Embedding","date":"2021-01-06T05:53:57.000Z","updated":"2021-01-06T06:06:54.889Z","comments":true,"path":"2021/01/06/Fine-grained-Image-Classification-by-Visual-Semantic-Embedding/","link":"","permalink":"http://example.com/2021/01/06/Fine-grained-Image-Classification-by-Visual-Semantic-Embedding/","excerpt":"Fine-grained Image Classification by Visual-Semantic Embedding IJCAI 2018 这篇文章的创新点是利用到了细粒度分类中子类别的语义信息。文中提到了两种语义信息，一种是 text context，以 CUB 数据集为例，wiki 上对某一子类鸟的描述就是 text context；另一种是 knowledge based context，收集子类鸟的各种属性，建立知识库，有点属性学习的意思。 这篇文章的想法很有创新，并且也有很好的效果，在 CUB 数据集上达到 86.2 的准确率。但是没有验证在其他数据集上是否同时有效。","text":"Fine-grained Image Classification by Visual-Semantic Embedding IJCAI 2018 这篇文章的创新点是利用到了细粒度分类中子类别的语义信息。文中提到了两种语义信息，一种是 text context，以 CUB 数据集为例，wiki 上对某一子类鸟的描述就是 text context；另一种是 knowledge based context，收集子类鸟的各种属性，建立知识库，有点属性学习的意思。 这篇文章的想法很有创新，并且也有很好的效果，在 CUB 数据集上达到 86.2 的准确率。但是没有验证在其他数据集上是否同时有效。 Two leval CNN:作者设计了一个双层的网络结构，F(a) 是定位网络，F(b) 是回归排序网络，网络结构图参考论文。 Localization Network：这一部分是传统的目标检测网络模型。提取 定位网络的特征，与回归网络提取的特征点乘，起到 Attention 的作用。 Regression Ranking Network：这一部分通过 CNN 提取网络的特征，然后加入定位网络提取的特征作为 Attention，得到视觉特征，接下来通过 FC 层，将视觉特征映射到语义空间，网络的约束条件就是特征在语义空间中的距离，优化网络，减小学习到的语义特征与 gt 在语义空间中的特征距离。作者使用了两种语义空间，因此网络的视觉特征同时平行通过了两个 FC 层。 Knowledge Base Embedding：这里参考了 Learning Entity and Relation Embeddings for Knowledge Graph Completion 中提出的 TransR 方法。并在此基础上针对细粒度分类问题提出了 Attribute Base Embedding。这一部分属于 知识图谱 的领域，看得不是很明白。首先，传统的 知识库嵌入 中，知识库由 三元组(h, r, t) 组成，其中 h，t 表示两个实体，h 是起点，t 是终点；r 表示是实体之间的关系。实体(h,t) 由 d 维数组表示，关系(r) 由 r 维数组表示，映射矩阵 M(r) 是一个 d*r 的矩阵，将实体空间映射到关系空间。知识图谱的约束条件定义为：f(h, t) = ||h(r) + r - t(r) ||。基于 知识库嵌入 改进的 属性知识库嵌入，三元组(h, r, t)中 实体h 是样本标签y， 关系r 是 has_property_of， 实体t 是样本的属性。通过优化映射矩阵，将样本标签映射到 属性知识库空间。 Text Embedding：文本嵌入部分通过 word2vec 实现。作者首先 finetune 了一个 word2vec 模型，然后利用模型将 类别名称 映射到 文本语义空间。 实验结果：作者在 CUB 数据集上做的实验，按照论文所述，训练的过程中既没有使用 bbox 信息，也没有使用 part annotation 信息，这一点不是很明白，和我理解的训练过程不太一样。 CUB 准确率： 0.862","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses","slug":"Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses","date":"2021-01-06T05:53:45.000Z","updated":"2021-01-06T06:06:51.075Z","comments":true,"path":"2021/01/06/Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses/","link":"","permalink":"http://example.com/2021/01/06/Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses/","excerpt":"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses TNNLS 2018 这篇文章提出了两个创新点，一个是 级联softmax结构，另一个是 泛化 large-margin loss。主要的思想就是细粒度分类中的标签是具有层次结构的，large-margin loss 是度量学习中提出的概念，这里在细粒度分类问题中进行了改进。 文章摆了大量的公式，所以我是没怎么仔细看的，并且它的结果并不很出色。使用 VGG 作为 backbone，在 CUB 上准确率是 77.0，结合 bilinear-CNN，从 84.1 提升到了 85.4","text":"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses TNNLS 2018 这篇文章提出了两个创新点，一个是 级联softmax结构，另一个是 泛化 large-margin loss。主要的思想就是细粒度分类中的标签是具有层次结构的，large-margin loss 是度量学习中提出的概念，这里在细粒度分类问题中进行了改进。 文章摆了大量的公式，所以我是没怎么仔细看的，并且它的结果并不很出色。使用 VGG 作为 backbone，在 CUB 上准确率是 77.0，结合 bilinear-CNN，从 84.1 提升到了 85.4 Cascaded Softmax Loss：级联softmax，见文中的 插图3，其实是一个很简单的结构，假设识别任务中有 50 个粗分类，每个分类中有 4 个细分类，总共 200 分类。传统的网络，直接通过 fc8(200)+softmax 进行训练，这里改成了 fc8(200)+ softmax + fc9(50)+softmax 训练，并且在 fc7 和 fc(9) 之间添加了一个 skip connection. 网络的 loss 则变成了每一层 softmax 的 loss 相加。 Generalized Large-Margin Loss：泛化 large-margin loss，这个用起来其实也很简单，就是在 fc7 添加一个 loss层 进行监督，large-margin loss 的作用就是增加类间距离，减小类内距离，在标签具有分层结构的情况下，对每个 level 的标签都进行这样的约束，这一部分文中使用了大量的公式，没有仔细看。 实验结果：作者的这个改动可以应用到任何 CNN 结构中，所以作者做了大量的实验： w/o bbox with bbox googlenet+SM 73.6 77.4 googlenet+CSM 74.6 78.4 googlenet+SC+CSM 75.3 79.0 googlenet+SM+GLM 76.8 80.5 googlenet+CSM+GLM 77.1 81.3 googlenet+SC+CSM+GLM 77.6 82.0 VGG+SM 72.5 78.6 VGG+SC+CSM+GLM 77.0 82.4 B-CNN 84.1 84.8 B-CNN+SC+CSM+GLM 85.4 85.7 googlenet+SM+contrastive 74.1 77.8 googlenet+SM+triplet 74.1 78.0 googlenet+SM+center loss 74.5 78.4 googlenet+SM+min-max 75.1 78.9 SM: softmaxCSM: cascaded softmaxSC: skip connectionGLM: generlized large-margin loss 通过对比可以看出： GLM 对结果的影响是最大的， 76.8 单独 CSM 的效果并不明显，加上 SC 后还能提高一下。 74.6 -&gt; 75.3 在 VGG， googlenet 上有明显的提高，但是在 B-CNN 上的提高就比较小了。 GLM 相对于其他的 度量学习 方法效果也是最好的。","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Weakly Supervised Local Attention Network for Fine-Grained Visual Classification","slug":"Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification","date":"2021-01-06T05:53:32.000Z","updated":"2021-01-06T06:06:35.915Z","comments":true,"path":"2021/01/06/Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification/","link":"","permalink":"http://example.com/2021/01/06/Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification/","excerpt":"Weakly Supervised Local Attention Network for Fine-Grained Visual Classification arxiv (MSRA) 这篇文章提出一种 LAP(Local Attention Pooling) 的机制，利用 attention map 去提取更具区分性的特征图；文中还提出一种弱监督的方式去训练网络的方法，在训练过程中加入了 attention dropout 和 attention center loss。 文中提出的网络 WS-LAN(Weakly Supervised Local Attention Network)在 CUB 数据集上准确率达到了 87.9","text":"Weakly Supervised Local Attention Network for Fine-Grained Visual Classification arxiv (MSRA) 这篇文章提出一种 LAP(Local Attention Pooling) 的机制，利用 attention map 去提取更具区分性的特征图；文中还提出一种弱监督的方式去训练网络的方法，在训练过程中加入了 attention dropout 和 attention center loss。 文中提出的网络 WS-LAN(Weakly Supervised Local Attention Network)在 CUB 数据集上准确率达到了 87.9 Local Attention Pooling:这一部分通过文中的 插图1 很容易就能看懂。作者首先通过 CNN 分别提取到图片的 特征图 和 注意力图，其中每一个注意力图用于聚焦目标的一个 part 上。然后将特征图与 k 个注意力图分别点乘，得到 k 个 part 的特征图。对 k 个特征图进行卷积核池化操作，得到每一个 part 的特征，将 k 个 part 特征合并到一起形成最终的特征。这一部分实际上与 SE-Net 很相似。SE-Net 将特征经过卷积之后，再经过一个 SE 结构，等同于在卷积过程中对卷积核的不同 channel 赋予不同的权重，上一层的特征会分别与卷积核的不同 channel 进行卷积操作。假设将上一层特征看做 LAN 中的特征图，channel间 attention 最大的 topk 个卷积核看做 LAN 中的注意力图，那么两个网络的区别就是一个是进行 卷积操作，一个是进行 点乘操作。所以这一部分实际上可以看做是一个稍加改动，更为复杂的 SE-Net。但是这种改动是必要的，因为下面的 WS-LAN 需要在这个网络结构上进行训练。 WS-LAN：这一部分，作者将两个传统网络训练中的 trick 迁移到了上面的 LAN 中，这应该是文章最大的两个提升点。这部分的两个 trick 实际上想解决的问题只有一个，就是 提取到的 attention map 很容易只聚焦到目标的一两个最具区分度的区域。 attention dropout：作者将 dropout 加入到 LAP 操作中。attention map 和 feature map 进行点乘的时候，attention map 以 概率(1-p) 被 drop 掉，以概率 p 被保留，并乘以 1/p，这个操作与传统的 dropout 如出一辙。 attention center loss：人脸识别文章中提出过一个 center loss，作者同样将其迁移到 WS-LAN 中。attention map 与 feature map 点乘之后得到 k 个 part feature，作者为这 k 个 feature 维护 k 个 center，然后每次训练的时候，计算每个 feature 和 center 的距离，最小化同一个 part 的 feature distance，最大化不同 part 的 feature distance，作为 attention center loss 的约束条件。 实验结果： acc LAN 85.5 WS-LAN 87.9","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Embedding label structures for fine-grained feature representation","slug":"Embedding-label-structures-for-fine-grained-feature-representation","date":"2021-01-06T05:53:11.000Z","updated":"2021-01-06T06:06:58.769Z","comments":true,"path":"2021/01/06/Embedding-label-structures-for-fine-grained-feature-representation/","link":"","permalink":"http://example.com/2021/01/06/Embedding-label-structures-for-fine-grained-feature-representation/","excerpt":"Embedding label structures for fine-grained feature representation CVPR 2016 这篇文章提出两个创新点，一个是使用 “structured label” 改进 triple loss；另一个是同时使用 triple loss 和 softmax loss 进行训练。","text":"Embedding label structures for fine-grained feature representation CVPR 2016 这篇文章提出两个创新点，一个是使用 “structured label” 改进 triple loss；另一个是同时使用 triple loss 和 softmax loss 进行训练。 同时使用 softmax loss 和 triple loss 约束训练：这个没什么好说的，一个很简单的想法。使用三路的网络，提取特征，然后分为两支，一支过 fc+softmax 然后分类，一支对比 anchor, positive, negative 的特征距离，然后用 triple loss 进行约束。网络结果参照论文链接。 结构化目标嵌入（embed label structures）:这一部分是对 triple loss 的一个改进，在细粒度分类问题中，目标的分类可以看做层级的，以车辆分类为例，目标结构由粗到细可以是 品牌-模型-年份，因此做模型分类的时候，可以使用四元组进行训练：r(reference), p+(same model), p-(same make, diffrent model), n(diffrent make)，四元组的 loss 等价于 L(r, p+, p-) + L(r, p-, n)。 实验结果：作者在 standFord Cars 上面进行实验，实验结果略（笔记只记录 CUB 数据集的结果）","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"The unreasonable effectiveness of noisy data for fine-grained recognition","slug":"The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition","date":"2021-01-06T05:46:06.000Z","updated":"2021-01-06T06:06:30.316Z","comments":true,"path":"2021/01/06/The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/","link":"","permalink":"http://example.com/2021/01/06/The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/","excerpt":"The unreasonable effectiveness of noisy data for fine-grained recognition ECCV 2016 这篇文章提出使用网络爬取的数据进行细粒度分类任务。与其说是提出一种新的方法，不如说是新建了一个数据集。 网络爬取的图片并不完全准确，因此讨论了对这些噪声的处理方法。 加入大量网络数据后，训练的效果得到了大幅度的提升，目前在 CUB 数据集上准确率达到了 92.3","text":"The unreasonable effectiveness of noisy data for fine-grained recognition ECCV 2016 这篇文章提出使用网络爬取的数据进行细粒度分类任务。与其说是提出一种新的方法，不如说是新建了一个数据集。 网络爬取的图片并不完全准确，因此讨论了对这些噪声的处理方法。 加入大量网络数据后，训练的效果得到了大幅度的提升，目前在 CUB 数据集上准确率达到了 92.3 cross-domain noise:cross-domain noise 指的是不属于这一大类的图片，例如搜索某种鸟，出来的结果是昆虫的图片。通过人工标注量化了这种噪声，发现这种情况比较少，并且当图片数目增多的时候，噪声占的比例也会减少。这种噪声对结果的影响也比较小。 cross-category noise:cross-category noise 指的是将其他子类的图片混入搜索结果的情况。这种噪声的比例难以量化，并且对结果的影响比较大，作者将搜索结果中重复的图片去除掉来减少这些噪声。 active learning：作者还提出两种标注方法辅助去除噪声。1）使用预训练好的模型，挑选搜索结果中置信度高的结果；2）人工筛选搜索结果。 实验结果： CUB web-raw web-filter L-bird L-bird(MC) L-bird+CUB L-bird+CUB(MC) acc 84.4 87.7 89.0 91.9 92.3 92.2 92.8 `CUB`：CUB 数据集 `web-raw`：web 爬取数据集 `web-filter`： 去除 `cross-category noise` `L-Bird`: 爬取所有鸟类的图片，进行预训练，然后在 web-filter 上 finetune `MC`：测试的时候使用 multi-crop","categories":[{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"hexo installation","slug":"hexo-installation","date":"2021-01-06T01:32:59.000Z","updated":"2021-01-06T06:08:35.376Z","comments":true,"path":"2021/01/06/hexo-installation/","link":"","permalink":"http://example.com/2021/01/06/hexo-installation/","excerpt":"1. 安装 git, node.js, Hexo 参考 Hexo 官方教程 2. 添加 github pages 新建 github repository 新建 hexo 分支，并设置为默认分支 添加 ssh hey 12ssh-keygen -t rsacat .ssh\\id_rsa.pub 在 settings -&gt; SSH and GPG keys -&gt; New SSH key 里面添加公钥。 新建 hexo 博客 123git clone clover978.github.iohexo init clover978.github.ionpm install hexo-deployer-git --save 在 clover978.github.io/_config.yml 里设置如下字段： 1234deploy:type: &#39;git&#39;repo: https:&#x2F;&#x2F;github.com&#x2F;clover978&#x2F;clover978.github.iobranch: master","text":"1. 安装 git, node.js, Hexo 参考 Hexo 官方教程 2. 添加 github pages 新建 github repository 新建 hexo 分支，并设置为默认分支 添加 ssh hey 12ssh-keygen -t rsacat .ssh\\id_rsa.pub 在 settings -&gt; SSH and GPG keys -&gt; New SSH key 里面添加公钥。 新建 hexo 博客 123git clone clover978.github.iohexo init clover978.github.ionpm install hexo-deployer-git --save 在 clover978.github.io/_config.yml 里设置如下字段： 1234deploy:type: &#39;git&#39;repo: https:&#x2F;&#x2F;github.com&#x2F;clover978&#x2F;clover978.github.iobranch: master 3. 安装 theme1git clone https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;hexo-theme-next themes&#x2F;next 4. 其他设置 参考 _config.yml 和 themes/next/_config.yml 里的各种字段，设置页面布局。 5. 发布 blog 编写 blog1234hexo new &lt;title&gt;# 编辑 _post 里面生成 md 文件hexo ghexo d 同步 hexo 123git add .git commit -m &quot;add new blog&quot;git push","categories":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[{"name":"others","slug":"others","permalink":"http://example.com/categories/others/"},{"name":"Paper notes","slug":"Paper-notes","permalink":"http://example.com/categories/Paper-notes/"},{"name":"FGVC","slug":"Paper-notes/FGVC","permalink":"http://example.com/categories/Paper-notes/FGVC/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/categories/hexo/"}],"tags":[{"name":"COCO","slug":"COCO","permalink":"http://example.com/tags/COCO/"},{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}