{"meta":{"title":"clover978","subtitle":"","description":"","author":"clover978","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2021-01-06T01:11:33.000Z","updated":"2021-01-06T01:12:35.121Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-01-06T01:12:03.000Z","updated":"2021-01-06T01:12:33.378Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"The Devil Is in the Details-Delving Into Unbiased Data Processing for Human","slug":"Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human","date":"2021-01-13T12:27:05.000Z","updated":"2021-01-18T04:28:47.847Z","comments":true,"path":"2021/01/13/Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human/","link":"","permalink":"http://example.com/2021/01/13/Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human/","excerpt":"The Devil Is in the Details-Delving Into Unbiased Data Processing for Human 本文用数学方法描述了 Pose Estimation 中的 Coordinate System Transformation（坐标系转换）和 Keypoint Format Transformation（关键点格式转换）过程，并分析其中的误差。文章发现：Coordinate System Transformation 过程中，传统方法中的 flip test 操作是 Biased Coordinate System Transformation，会导致误差。Keypoint Format Transformation 过程中，传统方法由于使用的量化坐标，也会导致关键点格式转换过程出现 bais。作者针对上述问题，提出的 Unbiased Data Processing 方法，消除误差。 UDP 一种模型无关的算法，可以用在任意现有的算法上，提升算法效果。","text":"The Devil Is in the Details-Delving Into Unbiased Data Processing for Human 本文用数学方法描述了 Pose Estimation 中的 Coordinate System Transformation（坐标系转换）和 Keypoint Format Transformation（关键点格式转换）过程，并分析其中的误差。文章发现：Coordinate System Transformation 过程中，传统方法中的 flip test 操作是 Biased Coordinate System Transformation，会导致误差。Keypoint Format Transformation 过程中，传统方法由于使用的量化坐标，也会导致关键点格式转换过程出现 bais。作者针对上述问题，提出的 Unbiased Data Processing 方法，消除误差。 UDP 一种模型无关的算法，可以用在任意现有的算法上，提升算法效果。 1. Method1.1 连续空间 与 离散空间 现实中的图像处于连续空间中，图像由 2维空间 中无数个点构成。一个 RR 的图像中关键点的坐标由一个实数对(x,y) 表示。 计算机中的图像处于离散空间中，图像由 若干像素点组成，一个 MM 的图像中的关键点的坐标由一个 整数对(i,j) 表示。 连续数据通过采样得到离散数据：现实中的图像通过采样（感知），形成像素点，变成离散数据，存储在计算机中。 离散数据通过插值得到连续数据：计算机中的图像矩阵，可以将通过插值，将整数域的数据表示为实数域的数据，变成连续数据。 1.1.1 连续图像 与 离散图像 的转换 连续数据 到 离散数据假设2维连续空间中有一幅图像，大小为 8cmx8cm，现在对其进行离散采样，我们考虑 x 轴方向上的采样情况。在连续空间中，图像的 x 轴坐标范围为 [0,8]，以 x=1 处的数据为例，解释采样的过程。传感器感知 x=1 位置的数据，存储为 1 个像素，数据被转化为离散数据。可以发现，在连续空间中长度为 8 的数据，在离散空间中占据 9 个像素，离散空间中，图像的 x 轴坐标范围为 {0, 1, 2, 3, 4, 5, 6, 7, 8}。 离散数据 到 连续数据将上面的例子反过来，假设计算机中存储了一幅图像，大小为 9x9，我们通过插值的方式还原图像在现实连续空间中的样子，只考虑 x 轴方向上的情况。每个像素的中心点，在连续空间中为 x=0,1,2…,7,8 的位置，连续空间中非整数部分的数据，通过插值算法生成，假设采用最邻近插值算法，图像在连续空间中的表示如图所示。可以发现，9x9 大小的离散图像，在连续空间中表示时，图像的宽度为 8 个单位长度。注意，上面的例子中，图像在连续空间中的范围不是 [-0.5, 8.5]，我们在转换的过程中，坐标轴是始终对齐的，或者说，连续图像 与 离散图像 所在的坐标空间是一致的，只是数据的表示形式不同。 1.2 坐标系统转换偏差分析（Coordinate System Transformation） 通过数学推导，UDP 得出，在姿态估计中，如果使用 flip test，关键点坐标会出现偏移，在网络输出的 heatmap 上，x 轴方向会出现一段 的偏移 $ \\frac{(1-s)}{s} $ （ s 为输入输出像素比）；在原始图像的最终结果上，x 轴方向会出现 $ \\frac{bw_{s}(s-1)}{W_{i}^{p}} $ （ $ bw_{s} $为 ROI 的宽像素数，$ W_{i}^{p} $为网络输入大小的宽像素数）的偏移。下面用实际例子验证上述结论。 假设： 原始图像大小为 ：18x18 像素； 网络输入图像大小为： 12x12像素； 关键点位置为：x 轴第 10 个像素点，x=9。 flip test 的流程如下： 将原始图像 resize 为 12x12 大小； 将 12x12 图像进行 flip； 将 flip 后的图像输入网络，得到heatmap，在heatmap 上定位关键点输出坐标； 将 heatmap 进行 flip back 操作，得到heatmap上关键点最终坐标； 将关键点映射到原始图像上。 逐步分析最终结果（分别假设网络的输入输出像素比 s 为 2:1 和 3:1）： 原始图像：大小 18x18，关键点位置 x=9； Resize，得到网络输入图像：大小 12x12，关键点位置 x=6； flip，得到翻转图像：大小 12x12，关键点位置 x=5； 推理，得到 heatmap：大小为 6x6，关键点位置 x=5/2；（或者 大小为 4x4，关键点位置 x=5/3）； flip back，得到未翻转图像对应的heatmap：大小为 6x6，关键点位置 x=5/2；（或者 大小为 4x4，关键点位置 x=4/3）； 映射到原始图像：关键点位置 x=15/2；或者（关键点位置 x=6） 首先分析 heatmap 上关键点的位置偏差，在没有 flip test 的情况下，即去掉步骤3和步骤5，可以得到，heatmap：大小为 6x6，关键点位置 x=3；（或者大小为 4x4，关键点位置 x=2），这种情况下heatmap 上关键点的位置与原始图像是对齐的。按照理论推导，s=2 时，flip test heatmap上关键点偏差为 -1/2；s=3时，flip test 在heatmap 上引入的关键点偏差为 -2/3。观察步骤5的计算结果，偏差分别为 {5/2 vs. 3} 和 {4/3 vs. 2}，与理论推导一致。继续分析在原始图像上关键点的位置偏差，根据论文给出的公式，s=2时，flip test 在原始图像上引入的关键点偏差为 ；s=3时，flip test 在原始图像上引入的关键点偏差为 。观察步骤6的计算结果，偏差分别为 {15/2 vs. 9} 和 {6 vs 9}，与理论推导一致。 1.3 关键点格式转换偏差分析（Keypoint Format Transformation） 人体姿态估计方法中，一般我们无法直接得出网络输出heatmap上，关键点的精确坐标，现有的方法一般是，首先找到最大值坐标，然后找到最大值领域的次大值坐标，关键点坐标为最大值向次大值偏移 0.25 像素的结果。这个过程显然存在量化误差。 一般情况下，heatmap 与原图的比例为 1：4，heatmap上一个像素对应原图4个像素，通过上面的操作，输出结果的最小间隔由1像素变成了0.5像素，等价于heatmap上一个像素对应原图2个像素，因此可以消除一定的误差。 论文分析结果，现有方法在输出的heatmap上，产生关键点位置的误差分布服从 E=0.125，V=1/192。映射到原图后，误差与heatmap上的误差线性相关，系数为 $ \\frac{bw_{s}}{W_{o}} $ （ $ bw_{s} $为 ROI 的宽像素数，$ W_{o} $为heatmap的宽像素数） 结合上一节 flip test 中的坐标系转换偏差分析，最终分析得到，坐标系转换和关键点格式转换两部分引入的偏差，分布服从 E=0.125，V=1/48。 1.4 无偏数据处理 作者发现姿态估计中这两种误差后，分别提出了消除误差的无偏数据处理方法，推荐看原论文中的公式。 1.4.1 无偏坐标系统转换 坐标系转换的误差主要来源于，传统方法没有注意到连续数据和离散数据的表示区别，在原图坐标系、网络输入图像坐标系、heatmap图像坐标系三者之间的转换过程，图像并没有对齐。论文提出的方法是将传统方法中缩放时的因数，由之前的 像素数比 改成 像素长度比 ，也就是分子分母同时减1。（例如上面提到的例子中的第一步，关键点的坐标应当乘以 11/17，而不是 12/18） 1.4.2无偏关键点格式转换 关键点格式转换的误差主要来源于量化操作，作者提出的方法是，在现有的基础上，引入回归loss，回归关键点在heatmap附近的offset。 2. Result COCO val Method Backbone Input size IPS/PPS AP AP50 AP75 APM APL AR Bottom-up methods HigherHRNet [26] HRNet-W32 512x512 0.8 64.4 - - 57.1 75.6 - +UDP HRNet-W32 512x512 4.9 (×6.1) 67.0 (+2.6) 86.2 72.0 60.7 76.7 71.6 HigherHRNet [26] HigherHRNet-W32 512x512 1.1 67.1 86.2 73.0 61.5 76.1 718 +UDP HigherHRNet-W32 512x512 2.9 (×2.6) 67.8 (+0.7) 86.2 72.9 62.2 76.4 72.4 HigherHRNet [26]† HRNet-W48 640x640 0.6 67.9 86.7 74.4 62.5 76.2 73.0 +UDP HRNet-W48 640x640 4.1 (×6.8) 68.9 (+1.0) 87.3 74.9 64.1 76.1 73.5 HigherHRNet [26] HigherHRNet-W48 640x640 0.75 69.9 87.2 76.1 65.4 76.4 - +UDP HigherHRNet-W48 640x640 2.7 (×3.6) 69.9 87.3 76.2 65.9 76.2 74.4 Bottom-up methods with multi-scale ([×2, ×1,×0.5]) test as in HigherHRNet [26] UDP HRNet-W32 512x512 - 70.4 88.2 75.8 65.3 77.6 74.7 HigherHRNet [26] HigherHRNet-W32 512x512 - 69.9 87.1 76.0 65.3 77.0 - +UDP HigherHRNet-W32 512x512 - 70.2 (+0.3) 88.1 76.2 65.4 77.4 74.5 HigherHRNet [26]† HRNet-W48 640x640 - 71.6 88.6 77.9 67.5 77.8 76.3 +UDP HRNet-W48 640x640 - 71.3 (-0.3) 89.0 77.1 66.9 77.7 75.7 HigherHRNet [26] HigherHRNet-W48 640x640 - 72.1 88.4 78.2 67.8 78.3 - +UDP HigherHRNet-W48 640x640 - 71.5 (-0.6) 88.3 77.3 67.9 77.2 75.9 Top-down methods Hourglass [40] Hourglass 256x192 - 66.9 - - - - - CPN [27] ResNet-50 256x192 - 69.4 - - - - - CPN [27] ResNet-50 384x288 - 71.6 - - - - - MSPN [28] MSPN 256x192 - 75.9 - - - - - SimpleBaseline [24] ResNet-50 256x192 23.0 71.3 89.9 78.9 68.3 77.4 76.9 +UDP ResNet-50 256x192 23.0 72.9(+1.6) 90.0 80.2 69.7 79.3 78.2 SimpleBaseline [24] ResNet-152 256x192 11.5 72.9 90.6 80.8 69.9 79.0 78.3 +UDP ResNet-152 256x192 11.5 74.3(+1.4) 90.9 81.6 71.2 80.6 79.6 SimpleBaseline [24] ResNet-50 384x288 20.3 73.2 90.7 79.9 69.4 80.1 78.2 +UDP ResNet-50 384x288 20.3 74.0(+0.8) 90.3 80.0 70.2 81.0 79.0 SimpleBaseline [24] ResNet-152 384x288 11.1 75.3 91.0 82.3 71.9 82.0 80.4 +UDP ResNet-152 384x288 11.1 76.2(+0.9) 90.8 83.0 72.8 82.9 81.2 HRNet [25] HRNet-W32 256x192 6.9 75.6 91.9 83.0 72.2 81.6 80.5 +UDP HRNet-W32 256x192 6.9 76.8(+1.2) 91.9 83.7 73.1 83.3 81.6 HRNet [25] HRNet-W48 256x192 6.3 75.9 91.9 83.5 72.6 82.1 80.9 +UDP HRNet-W48 256x192 6.3 77.2(+1.3) 91.8 83.7 73.8 83.7 82.0 HRNet [25] HRNet-W32 384x288 6.2 76.7 91.9 83.6 73.2 83.2 81.6 +UDP HRNet-W32 384x288 6.2 77.8(+1.1) 91.7 84.5 74.2 84.3 82.4 HRNet [25] HRNet-W48 384x288 5.3 77.1 91.8 83.8 73.5 83.5 81.8 +UDP HRNet-W48 384x288 5.3 77.8(+0.7) 92.0 84.3 74.2 84.5 82.5 COCO test-dev Method Backbone Input size AP AP50 AP75 APM APL AR Bottom-up methods AE [49] Hourglass [40] 512x512 56.6 81.8 61.8 49.8 67.0 - G-RMI [47] ResNet-101 353x257 64.9 85.5 71.3 62.3 70.0 69.7 PersonLab [51] ResNet-152 1401x140 66.5 88.0 72.6 62.4 72.3 - PifPaf [61] - - 66.7 - - - - - HigherHRNet [26] HRNet-W32 512x512 64.1 86.3 70.4 57.4 73.9 - +UDP HRNet-W32 512x512 66.8 (+2.7) 88.2 73.0 61.1 75.0 71.5 HigherHRNet [26] HigherHRNet-W32 512x512 66.4 87.5 72.8 61.2 74.2 - +UDP HigherHRNet-W32 512x512 67.2 (+0.8) 88.1 73.6 62.0 74.3 72.0 HigherHRNet [26]† HRNet-W48 640x640 67.4 88.6 74.2 62.6 74.3 72.8 +UDP HRNet-W48 640x640 68.1 (+0.2) 88.3 74.6 63.9 74.1 73.1 HigherHRNet [26] HigherHRNet-W48 640x640 68.4 88.2 75.1 64.4 74.2 - +UDP HigherHRNet-W48 640x640 68.6 (+0.2) 88.2 75.5 65.0 74.0 73.5 Bottom-up methods with multi-scale ([×2, ×1,×0.5]) test as in HigherHRNet [26] UDP HRNet-W32 512x512 69.3 89.2 76.0 64.8 76.0 74.1 HigherHRNet [26]† HRNet-W32 512x512 68.8 88.8 75.7 64.4 75.0 73.5 UDP HigherHRNet-W32 512x512 69.1 89.1 75.8 64.4 75.5 73.8 HigherHRNet [26]† HRNet-W48 640x640 70.4 89.7 77.4 66.4 75.7 75.2 +UDP HRNet-W48 640x640 70.3 90.1 76.7 66.6 75.3 75.1 HigherHRNet [26] HigherHRNet-W48 640x640 70.5 89.3 77.2 66.6 75.8 - +UDP HigherHRNet-W48 640x640 70.5 89.4 77.0 66.8 75.4 75.1 Top-down methods Mask-RCNN [53] ResNet-50-FPN [62] - 63.1 87.3 68.7 57.8 71.4 - Integral Pose Regression [63] ResNet-101 [64] 256x256 67.8 88.2 74.8 63.9 74.0 - SCN [56] Hourglass [40] - 70.5 88.0 76.9 66.0 77.0 - CPN [27] ResNet-Inception 384x288 72.1 91.4 80.0 68.7 77.2 78.5 RMPE [65] PyraNet [66] 320x256 72.3 89.2 79.1 68.0 78.6 - CFN [67] - - 72.6 86.1 69.7 78.3 64.1 - CPN(ensemble) [27] ResNet-Inception 384x288 73.0 91.7 80.9 69.5 78.1 79.0 Posefix [68] ResNet-152 384x288 73.6 90.8 81.0 70.3 79.8 79.0 CSANet [69] ResNet-152 384x288 74.5 91.7 82.1 71.2 80.2 80.7 MSPN [28] MSPN [28] 384x288 76.1 93.4 83.8 72.3 81.5 81.6 SimpleBaseline [27] ResNet-50 256x192 70.2 90.9 78.3 67.1 75.9 75.8 +UDP ResNet-50 256x192 71.7 (+1.5) 91.1 79.6 68.6 77.5 77.2 SimpleBaseline [27] ResNet-50 384x288 71.3 91.0 78.5 67.3 77.9 76.6 +UDP ResNet-50 384x288 72.5 (+1.2) 91.1 79.7 68.8 79.1 77.9 SimpleBaseline [27] ResNet-152 256x192 71.9 91.4 80.1 68.9 77.4 77.5 +UDP ResNet-152 256x192 72.9 (+1.0) 91.6 80.9 70.0 78.5 78.4 SimpleBaseline [27] ResNet-152 384x288 73.8 91.7 81.2 70.3 80.0 79.1 +UDP ResNet-152 384x288 74.7 (+0.9) 91.8 82.1 71.5 80.8 80.0 HRNet [25] HRNet-W32 256x192 73.5 92.2 82.0 70.4 79.0 79.0 +UDP HRNet-W32 256x192 75.2 (+1.7) 92.4 82.9 72.0 80.8 80.4 HRNet [25] HRNet-W32 384x288 74.9 92.5 82.8 71.3 80.9 80.1 +UDP HRNet-W32 384x288 76.1 (+1.2) 92.5 83.5 72.8 82.0 81.3 HRNet [25] HRNet-W48 256x192 74.3 92.4 82.6 71.2 79.6 79.7 +UDP HRNet-W48 256x192 75.7 (+1.4) 92.4 83.3 72.5 81.4 80.9 HRNet [25] HRNet-W48 384x288 75.5 92.5 83.3 71.9 81.5 80.5 +UDP HRNet-W48 384x288 76.5 (+1.0) 92.7 84.0 73.0 82.4 81.6","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"poseplugin","slug":"poseplugin","permalink":"http://example.com/tags/poseplugin/"}]},{"title":"Distribution-Aware Coordinate Representation for Human Pose Estimation","slug":"Paper-Notes-Pose-Estimation-20210113-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation","date":"2021-01-13T08:32:34.000Z","updated":"2021-01-18T04:29:24.203Z","comments":true,"path":"2021/01/13/Paper-Notes-Pose-Estimation-20210113-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/","link":"","permalink":"http://example.com/2021/01/13/Paper-Notes-Pose-Estimation-20210113-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/","excerpt":"Distribution-Aware Coordinate Representation for Human Pose Estimation 这篇文章从 关节坐标表征 方向研究，发现传统方法中的 coordinate encoding （关键点转化为高斯热力图）过程存在量化误差； coordinate decoding（网络输出热力图转化为关键点坐标）过程对 shift 操作的依赖很高。作者提出 Distribution-Aware coordinate Representation of Keypoints (DARK) 方法，在 关节坐标表征（coordinate Representation）层面提升现有方法的效果。 DARK 方法可以不改变现有算法的条件下，无缝提升算法效果。","text":"Distribution-Aware Coordinate Representation for Human Pose Estimation 这篇文章从 关节坐标表征 方向研究，发现传统方法中的 coordinate encoding （关键点转化为高斯热力图）过程存在量化误差； coordinate decoding（网络输出热力图转化为关键点坐标）过程对 shift 操作的依赖很高。作者提出 Distribution-Aware coordinate Representation of Keypoints (DARK) 方法，在 关节坐标表征（coordinate Representation）层面提升现有方法的效果。 DARK 方法可以不改变现有算法的条件下，无缝提升算法效果。 1. Method Coordinate Decoding现有方法： 坐标极值点，向第二极值点偏移1/4像素。DARK： 假设网络输出服从高斯分布，通过求解一阶导数零点计算中心点，根据论文公式9，中心点坐标可以表示为极值点坐标及其附近梯度值的函数。 Heatmap distribution modulation： 网络输出的 heatmap 通常不够平滑，文章提出 Heatmap distribution modulation 的方法，使用一个 Gaussian kernel 在原始特征图上进行卷积操作，Modulated heatmap 更有利于 Coordinate Decoding Coordinate Encoding现有方法：将 keypoint 坐标量化，然后以坐标点为中心生成高斯分布的 heatmapDARK： 取消量化操作，计算 heatmap 时，heatmap 中心点的坐标以浮点数表示。 2. Result COCO test-dev Method Backbone Input size #Params GFLOPs AP AP50 AP75 AP M AP L AR G-RMI[23] ResNet-101 353 × 257 42.6M 57.0 64.9 85.5 71.3 62.3 70.0 69.7 IPR [27] ResNet-101 256 × 256 45.1M 11.0 67.8 88.2 74.8 63.9 74.0 - CPN [6] ResNet-Inception 384 × 288 - - 72.1 91.4 80.0 68.7 77.2 78.5 RMPE [11] PyraNet 320 × 256 28.1M 26.7 72.3 89.2 79.1 68.0 78.6 - CFN [13] - - - - 72.6 86.1 69.7 78.3 64.1 - CPN (ensemble) [6] ResNet-Inception 384 × 288 - - 73.0 91.7 80.9 69.5 78.1 79.0 SimpleBaseline[33] ResNet-152 384 × 288 68.6M 35.6 73.7 91.9 81.1 70.3 80.0 79.0 HRNet[25] HRNet-W32 384 × 288 28.5M 16.0 74.9 92.5 82.8 71.3 80.9 80.1 HRNet[25] HRNet-W48 384 × 288 63.6M 32.9 75.5 92.5 83.3 71.9 81.5 80.5 DARK HRNet-W32 128 × 96 28.5M 1.8 70.0 90.9 78.5 67.4 75.0 75.9 DARK HRNet-W48 384 × 288 63.6M 32.9 76.2 92.5 83.6 72.5 82.4 81.1 G-RMI (extra data) ResNet-101 353 × 257 42.6M 57.0 68.5 87.1 75.5 65.8 73.3 73.3 HRNet (extra data) HRNet-W48 384 × 288 63.6M 32.9 77.0 92.7 84.5 73.4 83.1 82.0 DARK (extra data) HRNet-W48 384 × 288 63.6M 32.9 77.4 92.6 84.6 73.6 83.7 82.3 MPII Method Head Sho. Elb. Wri. Hip Kne. Ank. Mean HRN32 97.1 95.9 90.3 86.5 89.1 87.1 83.3 90.3 DARK 97.2 95.9 91.2 86.7 89.7 86.7 84.0 90.6","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"poseplugin","slug":"poseplugin","permalink":"http://example.com/tags/poseplugin/"}]},{"title":"PoseFix: Model-agnostic General Human Pose Refinement Network","slug":"Paper-Notes-Pose-Estimation-20210113-PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network","date":"2021-01-13T05:54:18.000Z","updated":"2021-01-18T04:29:18.819Z","comments":true,"path":"2021/01/13/Paper-Notes-Pose-Estimation-20210113-PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network/","link":"","permalink":"http://example.com/2021/01/13/Paper-Notes-Pose-Estimation-20210113-PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network/","excerpt":"PoseFix: Model-agnostic General Human Pose Refinement Network 现有的 Pose Estimation 方法的 error 具有相似的分布，收集这些 error statistics 作为先验知识，结合标注数据，可以合成 Pose 数据。利用合成 Pose 数据训练网络，就可以让网络学习 error statistics，进而 refine 其他网络的 Pose 结果。 PoseFix 可对任意模型进行 refine，实验结果表明对多种模型都有提升效果。","text":"PoseFix: Model-agnostic General Human Pose Refinement Network 现有的 Pose Estimation 方法的 error 具有相似的分布，收集这些 error statistics 作为先验知识，结合标注数据，可以合成 Pose 数据。利用合成 Pose 数据训练网络，就可以让网络学习 error statistics，进而 refine 其他网络的 Pose 结果。 PoseFix 可对任意模型进行 refine，实验结果表明对多种模型都有提升效果。 1. Method Synthesizing poses for trainingPose 关键点一共有 5 中类型： Good： 预测结果完全正确 Jitter： 预测结果与 GT 有小距离偏差 Inversion： 预测结果将对称关键点预测错误 Swap： 预测结果将关键点标注到旁边的人体目标上 Miss： 预测结果与 GT 有大距离偏差根据之前的研究，Pose Estimation 中这 5 种类型关键点服从一定的分布，依据不同类型关键点出现的频率，随机合成 Pose，然后输入 PoseFix 训练。 Architecture and learning of PoseFixPoseFix 将 原始图像 和 合成Pose 同时输入网络，网络输出 refined pose。PoseFix 采用和 simplebaseline 相同的骨干网络。PoseFix 利用 Coarse-to-fine 的模式进行预测，网络输入的 合成Pose 为粗粒度结果，网络输出为 精细化结果。refined pose 被两个 Loss 约束。 L(H): 分别在 x方向 和 y方向 上计算 交叉熵 L(C): 利用 softmax 函数，计算出对应的 Pose 坐标 C，然后计算 C 与 GT 的 L1 距离。L(H) 约束网络只产生一个关键点，L(C) 约束网络输出的关键点坐标更精确。 2. Result COCO test-dev Methods AP AP.50 AP.75 APM APL AR AR.50 AR.75 ARM ARL AE [18] 56.6 81.7 62.1 48.1 69.4 62.5 84.9 67.2 52.2 76.5 + PoseFix (Ours) 63.9 83.6 70.0 56.9 73.7 69.1 86.6 74.2 61.1 79.9 PAFs [4] 61.7 84.9 67.4 57.1 68.1 66.5 87.2 71.7 60.5 74.6 + PoseFix (Ours) 66.7 85.7 72.9 62.9 72.3 71.3 88.0 76.7 66.3 78.1 Mask R-CNN (ResNet-50) [9] 62.9 87.1 68.9 57.6 71.3 69.7 91.3 75.1 63.9 77.6 + PoseFix (Ours) 67.2 88.0 73.5 62.5 75.1 74.0 92.2 79.6 68.8 81.1 Mask R-CNN (ResNet-101) 63.4 87.5 69.4 57.8 72.0 70.2 91.8 75.6 64.3 78.2 + PoseFix (Ours) 67.5 88.4 73.8 62.6 75.5 74.3 92.6 79.9 69.1 81.4 Mask R-CNN (ResNeXt-101-64) 64.9 88.6 71.0 59.6 73.3 71.4 92.4 76.8 65.9 78.9 + PoseFix (Ours) 68.7 89.3 75.2 64.1 76.4 75.2 93.1 80.9 70.3 81.9 Mask R-CNN (ResNeXt-101-32) 64.9 88.4 70.9 59.5 73.2 71.3 92.2 76.7 65.8 78.9 + PoseFix (Ours) 68.5 88.9 75.0 64.0 76.2 75.0 92.9 80.7 70.1 81.8 IntegralPose 66.3 87.6 72.9 62.7 72.7 73.2 91.8 79.1 68.3 79.8 + PoseFix (Ours) 69.5 88.3 75.9 65.7 76.1 75.9 92.4 81.8 71.1 82.5 CPN (ResNet-50) [6] 68.6 89.6 76.7 65.3 74.6 75.6 93.7 82.6 70.8 82.0 + PoseFix (Ours) 71.8 89.8 78.9 68.3 78.1 78.2 93.9 84.3 73.5 84.6 CPN (ResNet-101) 69.6 89.9 77.6 66.3 75.6 76.6 93.9 83.5 72.0 82.9 + PoseFix (Ours) 72.6 90.2 79.7 69.0 78.9 78.9 94.1 85.0 74.2 85.1 Simple (ResNet-50) [28] 69.4 90.1 77.4 66.2 75.5 75.1 93.9 82.4 70.8 81.0 + PoseFix (Ours) 72.5 90.5 79.6 68.9 79.0 78.0 94.1 84.4 73.4 84.1 Simple (ResNet-101) 70.5 90.7 78.8 67.5 76.3 76.2 94.3 83.7 72.1 81.9 + PoseFix (Ours) 73.3 90.8 80.7 69.8 79.8 78.7 94.4 85.3 74.3 84.8 Simple (ResNet-152) 71.1 90.7 79.4 68.0 76.9 76.8 94.4 84.3 72.6 82.4 + PoseFix (Ours) 73.6 90.8 81.0 70.3 79.8 79.0 94.4 85.7 74.8 84.9","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"poseplugin","slug":"poseplugin","permalink":"http://example.com/tags/poseplugin/"}]},{"title":"Rethinking on Multi-Stage Networks for Human Pose Estimation","slug":"Paper-Notes-Pose-Estimation-20210112-Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation","date":"2021-01-12T13:33:24.000Z","updated":"2021-01-18T04:38:31.709Z","comments":true,"path":"2021/01/12/Paper-Notes-Pose-Estimation-20210112-Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation/","link":"","permalink":"http://example.com/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation/","excerpt":"Rethinking on Multi-Stage Networks for Human Pose Estimation 作者将现有的 Pose Estimation 方法分为 single-stage 和 multi-stage 两类。并且认为现有的 multi-stage 方法的不足之处在于模型设计缺陷。文章在经典的 stack hourglass 模型的基础上，提出三个方向的改进：1) 单阶段模型设计，2) 跨阶段特征聚合，3) coarse-to-fine 监督学习。 本文设计的 Multi Stage Pose Network 在 COCO test-dev 上取得 78.1% 的准确率，在 MPII test 上取得 92.6% 的准确率。","text":"Rethinking on Multi-Stage Networks for Human Pose Estimation 作者将现有的 Pose Estimation 方法分为 single-stage 和 multi-stage 两类。并且认为现有的 multi-stage 方法的不足之处在于模型设计缺陷。文章在经典的 stack hourglass 模型的基础上，提出三个方向的改进：1) 单阶段模型设计，2) 跨阶段特征聚合，3) coarse-to-fine 监督学习。 本文设计的 Multi Stage Pose Network 在 COCO test-dev 上取得 78.1% 的准确率，在 MPII test 上取得 92.6% 的准确率。 1. Introduction 单阶段模型从图像分类领域迁移而来，一般基于单个骨干网络。 多阶段模型由若干阶段组成，每个阶段是一个轻量级的网络，网络包含上采样和下采样通路。 根据现有工作。单阶段模型和多阶段模型的优劣无法区分。 improvements 多阶段模型中，每个阶段的 module 设计不够好。 提出跨阶段的特征聚合方法，加强 information flow。 采用由粗到细的监督学习方法。 2. Method Analysis of a Single-Stage ModuleHourglass 设计中，网络下采样和上采样过程中，channel 数一致为常数，导致大量信息丢失。现有改进通常会在 下采样和上采样过程中增加 channel 数，下采样过程 [256, 386, 512, 768]，上采样过程 [768, 512, 386, 256]。MSPN 提出，上采样过程中不需要增加 channel 数，维持较少的 channel，即下采样过程 [256, 386, 512, 768]，上采样过程 [256, 256, 256, 256]。 Method Res-50 Res-101 Res-152 Res-254 AP 71.5 73.1 73.6 74.0 FLOPs(G) 4.4 7.5 11.2 18.0 单阶段模型的上限大约为 74.0。 Stages Hourglass Stages MSPN FLOPs(G) AP FLOPs(G) AP 1 3.9 65.4 1 4.4 71.5 2 6.2 70.9 2 9.6 74.5 4 10.6 71.3 3 14.7 75.2 8 19.5 71.6 4 19.9 75.9 MSPN 设计的 Single-Stage Module，模型上限能达到 75.9 Method Res-50 2×Res-18 L-XCP 4× S-XCP AP 71.5 71.6 73.7 74.7 FLOPs 4.4G 4.0G 6.1G 5.7G FLOPs 相近的情况下，多阶段模型的效果好于单阶段模型。 Cross Stage Feature Aggregation将 stage(t-1) 的 downsampling 和 upsampling units 特征图，通过 1x1 卷积，连接到 stage(t) 的 downsampling unit，形成 residual 结构，实现 Cross Stage Feature Aggregation。 Coarse-to-fine SupervisionMSPN 在靠前的阶段，heatmap 使用较大的高斯核，在靠后的阶段，heatmap 使用较小的高斯核，用这种方法构造 GT，监督网络学习。 BaseNet CTF CSFA Hourglass MSPN √ 71.3 73.3 √ √ 72.5 74.2 √ √ √ 73.0 74.5 加入 Coarse-to-fine Supervision(CTF) 和 Cross Stage Feature Aggregation(CSFA) 模型的精度提高。 3. Result COCO test-dev Method Backbone Input Size AP AP50 AP75 APM APL AR AR50 AR75 ARM ARL CMU Pose [5] - - 61.8 84.9 67.5 57.1 68.2 66.5 87.2 71.8 60.6 74.6 Mask R-CNN [16] Res-50-FPN - 63.1 87.3 68.7 57.8 71.4 - - - - - G-RMI [31] Res-152 353×257 64.9 85.5 71.3 62.3 70.0 69.7 88.7 75.5 64.4 77.1 AE [28] - 512×512 65.5 86.8 72.3 60.6 72.6 70.2 89.5 76.0 64.6 78.1 CPN [9] Res-Inception 384×288 72.1 91.4 80.0 68.7 77.2 78.5 95.1 85.3 74.2 84.3 Simple Base [46] Res-152 384×288 73.7 91.9 81.1 70.3 80.0 79.0 - - - - HRNet [39] HRNet-W48 384×288 75.5 92.5 83.3 71.9 81.5 80.5 - - - - Ours (MSPN) 4×Res-50 384×288 76.1 93.4 83.8 72.3 81.5 81.6 96.3 88.1 77.5 87.1 CPN+ [9] Res-Inception 384×288 73.0 91.7 80.9 69.5 78.1 79.0 95.1 85.9 74.8 84.6 Simple Base+* [46] Res-152 384×288 76.5 92.4 84.0 73.0 82.7 81.5 95.8 88.2 77.4 87.2 HRNet* [39] HRNet-W48 384×288 77.0 92.7 84.5 73.4 83.1 82.0 - - - - Ours (MSPN*) 4×Res-50 384×288 77.1 93.8 84.6 73.4 82.3 82.3 96.5 88.9 78.4 87.7 Ours (MSPN+*) 4×Res-50 384×288 78.1 94.1 85.9 74.5 83.3 83.1 96.7 89.8 79.3 88.2 COCO test-challenge Method Backbone Input Size AP AP50 AP75 APM APL AR AR50 AR75 ARM ARL Mask R-CNN* [16] ResX-101-FPN - 68.9 89.2 75.2 63.7 76.8 75.4 93.2 81.2 70.2 82.6 G-RMI* [31] Res-152 353×257 69.1 85.9 75.2 66.0 74.5 75.1 90.7 80.7 69.7 82.4 CPN+ [9] Res-Inception 384×288 72.1 90.5 78.9 67.9 78.1 78.7 94.7 84.8 74.3 84.7 Sea Monsters+* - - 74.1 90.6 80.4 68.5 82.1 79.5 94.4 85.1 74.1 86.8 Simple Base+* [46] Res-152 384×288 74.5 90.9 80.8 69.5 82.9 80.5 95.1 86.3 75.3 87.5 Ours (MSPN+*) 4×Res-50 384×288 76.4 92.9 82.6 71.4 83.2 82.2 96.0 87.7 77.5 88.6","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"}]},{"title":"Pose Estimation resources","slug":"Paper-Notes-Pose-Estimation-20210112-Pose-Estimation-resources","date":"2021-01-12T07:19:47.000Z","updated":"2021-01-18T04:40:18.054Z","comments":true,"path":"2021/01/12/Paper-Notes-Pose-Estimation-20210112-Pose-Estimation-resources/","link":"","permalink":"http://example.com/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Pose-Estimation-resources/","excerpt":"Awsome Pose Estimation1. Pose Estimation Papers awesome-human-pose-estimation: a github repo which collects update-to-date Pose Estimation Papers.","text":"Awsome Pose Estimation1. Pose Estimation Papers awesome-human-pose-estimation: a github repo which collects update-to-date Pose Estimation Papers. 2. Pose Estimation SOTA Paper With Code: Summary of SOTA over MPII, COCO-val, COCO-test_dev, PoseTrack datasets. 3. Pose Estimation Codebase MMPose: an open-source toolbox for pose estimation based on PyTorch. MMPose Document 4. Pose Estimation benchmark COCO test-challenge Method backbone input-size AP remark bottomup topdown G-RMI - 69.1 extra data CPN ResNet-Inception 72.1 ensemble DARK HRNet-W48 384x288 76.4 extra data &amp; ensemble COCO test-dev Method backbone input-size AP remark bottomup OpenPose - 61.8 AE StackHourglass 65.0 multi scale AE StackHourglass 65.5 refine HigherHRNet HRNet-W32 512x512 66.4 HigherHRNet HRNet-W48 640x640 68.4 HigherHRNet HRNet-W48 640x640 70.5 multi scale topdown CPN ResNet-Inception 72.1 CPN ResNet-Inception 73.0 ensemble RMPE PyraNet 320x256 72.3 SimpleBaseline ResNet-152 384x288 73.7 HRNet-W32 HRNet-W32 384x288 74.9 HRNet-W48 HRNet-W48 384x288 75.5 HRNet-W48 HRNet-W48 384x288 77.0 extra data EvoPose2D-L 512x384 75.7 MSPN 4×Res-50 384x288 76.1 MSPN 4×Res-50 384x288 77.1 extra data MSPN 4×Res-50 384x288 78.1 extra data &amp; ensemble DARK HRNet-W48 384x288 76.2 75.5(+0.7) DARK HRNet-W48 384x288 77.4 77.0(+0.4) &amp; extra data DARK HRNet-W48 384x288 78.9 extra data &amp; ensemble UDP HRNet-W48 384x288 76.5 75.5(+1.0) PoseFix HRNet-W48 384x288 76.7 75.5(+1.2) PoseFix EvoPose2D-L 512x384 76.8 75.7(+1.1) COCO-val Method backbone input-size AP remark bottomup OpenPose - 58.4 OpenPose - 61.0 CPM refine HigherHRNet HRNet-W32 512x512 67.1 HigherHRNet HRNet-W32 640x640 68.5 HigherHRNet HRNet-W48 640x640 69.9 UDP HigherHRNet-W32 512x512 67.8 67.1(+0.7) UDP HigherHRNet-W48 640x640 69.9 69.9(+0.0) topdown CPN ResNet-Inception 72.7 CPN ResNet-Inception 74.5 ensemble MSPN 4×Res-50 384x288 76.4 extra data &amp; ensemble HRNet-W32 HRNet-W32 384x288 75.8 HRNet-W48 HRNet-W48 384x288 76.3 EvoPose2D-L 512x384 76.6 DARK HRNet-W32 384x288 76.6 75.8(+0.8) PoseFix HRNet-W48 384x288 77.3 76.3(+1.0) UDP HRNet-W48 384x288 77.8 77.1(+0.7) ??? MPII test Method backbone input-size AP remark bottomup topdown Stack Hourglass 90.9 SimpleBaseline ResNet-152 91.5 HRNet-W32 HRNet-W32 92.3 MPII multi-person test Method backbone input-size AP remark bottomup OpenPose - 72.5 OpenPose - 75.6 multi-scale AE StackHourglass 77.5 topdown RMPE PyraNet 320x256 82.1","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"benchmark","slug":"benchmark","permalink":"http://example.com/tags/benchmark/"}]},{"title":"EvoPose2D-Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution","slug":"Paper-Notes-Pose-Estimation-20210112-EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution","date":"2021-01-12T07:15:37.000Z","updated":"2021-01-18T04:44:18.150Z","comments":true,"path":"2021/01/12/Paper-Notes-Pose-Estimation-20210112-EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution/","link":"","permalink":"http://example.com/2021/01/12/Paper-Notes-Pose-Estimation-20210112-EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution/","excerpt":"EvoPose2D-Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution 本文使用 神经网络进化（neuroevolution） 的方法搜索最优的 2D Human Pose Estimation 模型。文章提出 weight transfer scheme 来加速神经网络进化的速度。 通过神经网络进化生成的模型，可以比现有的 SOTA 具有更优的效果。其中最优的模型 EvoPose2D-L 相比于 HRNet-W48 只有 1/2.0 的 operations， 1/4.3 的 parameters。 EvoPose2D-L 在 COCO test-dev 数据集上准确率为 75.7%","text":"EvoPose2D-Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution 本文使用 神经网络进化（neuroevolution） 的方法搜索最优的 2D Human Pose Estimation 模型。文章提出 weight transfer scheme 来加速神经网络进化的速度。 通过神经网络进化生成的模型，可以比现有的 SOTA 具有更优的效果。其中最优的模型 EvoPose2D-L 相比于 HRNet-W48 只有 1/2.0 的 operations， 1/4.3 的 parameters。 EvoPose2D-L 在 COCO test-dev 数据集上准确率为 75.7% 1. Method Weight transfer文章提出 Weight transfer 来加速神经网络进化的过程。为了保证突变（mutated）后的 child 网络能够快速收敛，很自然的想法就是使用 parent 的参数对其进行初始化。作者根据 child 和 parent 的 kernel-size 和 channels 大小，定义了四种情况，具有 transfer 的方法参见论文公式。 Search space作者根据网络的各种超参，定义了 10e14大小的搜索空间，具有定义方法见论文描述。 Fitness神经网络进化中，需要考虑 performance 和 effciency 的平衡，文章使用 Pareto optimizations 的优化方法，最小化适应函数 fitness function。函数表达式见论文。 Evolutionary strategy神经网络进化的过程类似生物进化的流程。从 0 号祖先开始，生成若干个 child，然后找到 m 个 fit 比较好的 child，成为新的祖先，完成一次进化，重复多轮进化，直到 适应函数 收敛时，手动结束网络进化流程。 Large-batch training为了使神经网路进化的进程尽量快，文章使用了 large-batchsize 训练，训练过程中参照之前研究的 large-batchsize 训练方法，保证模型可以收敛。 Compound scaling同样根据现有的研究结果，网络的 resolution, channels, depth 同步的缩放可以使神经网络进化过程更加高效，作者根据搜索空间中的 resolution 直接计算出 channels 和 depth，实现 Compound scaling。 2. Result COCO val Method Backbone Pretrain Input size Params (M) FLOPs (G) AP AP50 AP75 APM APL AR CPN [9] ResNet-50 Y 256 × 192 27.0 6.20 68.6 − − − − − SimpleBaseline [49] ResNet-50 Y 256 × 192 34.0 5.21† 70.4 88.6 78.3 67.1 77.2 76.3 SimpleBaseline [49] ResNet-101 Y 256 × 192 53.0 8.84† 71.4 89.3 79.3 68.1 78.1 77.1 SimpleBaseline [49] ResNet-152 Y 256 × 192 68.6 12.5† 72.0 89.3 79.8 68.7 78.9 77.8 HRNet-W32 [40] - N 256 × 192 28.5 7.65† 73.4 89.5 80.7 70.2 80.1 78.9 HRNet-W32 [40] - Y 256 × 192 28.5 7.65† 74.4 90.5 81.9 70.8 81.0 79.8 HRNet-W48 [40] - Y 256 × 192 63.6 15.7† 75.1 90.6 82.2 71.5 81.8 80.4 MSPN [25] 4xResNet-50 Y 256 × 192 120 19.9 75.9 − − − − − SimpleBaseline [49] ResNet-152 Y 384 × 288 68.6 28.1† 74.3 89.6 81.1 70.5 79.7 79.7 HRNet-W32 [40] - Y 384 × 288 28.5 16.0† 75.8 90.6 82.7 71.9 82.8 81.0 HRNet-W48 [40] - Y 384 × 288 63.6 35.3† 76.3 90.8 82.9 72.3 83.4 81.2 HRNet-W48 + PF [33] - Y 384 × 288 63.6 35.3† 77.3 90.9 83.5 73.5 84.4 82.0 SimpleBaseline ResNet-50 N 256 × 192 34.1 5.21 70.6 89.0 78.4 66.9 77.1 77.3 SimpleBaseline ResNet-50 Y 256 × 192 34.1 5.21 71.0 89.2 78.5 67.4 77.4 78.0 HRNet-W32 - N 256 × 192 28.6 7.65 73.6 89.9 80.5 70.1 80.0 80.0 EvoPose2D-XS - N 256 × 192 2.53 0.47 67.7 87.8 75.8 64.5 73.7 74.7 EvoPose2D-XS - WT 256 × 192 2.53 0.47 68.0 87.9 76.1 64.5 74.3 75.0 EvoPose2D-S - N 256 × 192 2.53 1.07 69.8 88.6 77.3 66.3 76.2 76.4 EvoPose2D-S - WT 256 × 192 2.53 1.07 70.2 88.9 77.8 66.5 76.8 76.9 EvoPose2D-M - N 384 × 288 7.34 5.59 75.1 90.2 81.9 71.5 81.7 81.0 EvoPose2D-L - N 512 × 384 14.7 17.7 76.6 90.5 83.0 72.7 83.4 82.3 EvoPose2D-L + PF - N 512 × 384 14.7 17.7 77.5 90.9 83.6 74.0 84.2 82.5 COCO test-dev Method Backbone Pretrain Input size Params (M) FLOPs (G) AP AP50 AP75 APM APL AR CPN [9] Res-Inception Y 384 × 288 - - 72.1 91.4 80.0 68.7 77.2 78.5 SimpleBaseline [49] ResNet-152 Y 384 × 288 68.6 35.6 73.7 91.9 81.1 70.3 80.0 79.0 HRNet-W48 [40] - Y 384 × 288 63.6 32.9 75.5 92.5 83.3 71.9 81.5 80.5 HRNet-W48 + PF [33] - Y 384 × 288 63.6 32.9 76.7 92.6 84.1 73.1 82.6 81.5 EvoPose2D-L - N 512 × 384 14.7 17.7 75.7 91.9 83.1 72.2 81.5 81.7 EvoPose2D-L + PF - N 512 × 384 14.7 17.7 76.8 92.5 84.3 73.5 82.5 81.7","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"}]},{"title":"Deep High-Resolution Representation Learning for Human Pose Estimation","slug":"Paper-Notes-Pose-Estimation-20210112-Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation","date":"2021-01-12T05:55:08.000Z","updated":"2021-01-18T04:44:20.436Z","comments":true,"path":"2021/01/12/Paper-Notes-Pose-Estimation-20210112-Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/","link":"","permalink":"http://example.com/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/","excerpt":"Deep High-Resolution Representation Learning for Human Pose Estimation 本文提出一种新型的网络结构 HRNet，与以往的网络结构不同，HRNet 中 high-to-low resolution subnetworks 是平行排列的，不同分辨率的特征通过 downsample 和 upsample 的形式相互融合。 HRNet 在 COCO test-dev 数据集上取得 75.5% 的准确率，在加入额外训练数据后准确率为 77.0%","text":"Deep High-Resolution Representation Learning for Human Pose Estimation 本文提出一种新型的网络结构 HRNet，与以往的网络结构不同，HRNet 中 high-to-low resolution subnetworks 是平行排列的，不同分辨率的特征通过 downsample 和 upsample 的形式相互融合。 HRNet 在 COCO test-dev 数据集上取得 75.5% 的准确率，在加入额外训练数据后准确率为 77.0% 1. Method Sequential multi-resolution subnetworks现有网络的 backbone 大多为 Sequential 结构，每个 subnetwork 被称为一个 stage，相邻的 stage 通过 downsample 连接，网络的分辨率逐层减半。 1N11 → N22 → N33 → N44 Parallel multi-resolution subnetworksHRNet 第一个 stage 为 high-resolution subnetwork，然后以并行的方法，逐渐添加 high-to-low resolution subnetworks。因此，后续添加的 stage 既包含之前 stage 的 high resolution 信息，又包含 low resolution 信息。 1234N11 → N21 → N31 → N41 N22 → N32 → N42 N33 → N43 N44 Repeated multi-scale fusionHRNet 中加入 exchange units 来交换不同 subnetworks 的信息。downsample 过程通过 3x3 的卷积实现，卷积的 stride 为 2，图像分辨率下降一半；upsample 过程分为两步，第一步是 nearest neighbor sampling，第二步是 1x1 卷积，用于改变通道数。 Heatmap estimation作者在最后一层 exchange unit 输出的 特征图上，预测最终的 heatmap。 2. Result COCO val Method Backbone Pretrain Input size #Params GFLOPs AP AP50 AP75 APM APL AR 8-stage Hourglass [40] 8-stage Hourglass N 256 × 192 25.1M 14.3 66.9 − − − − − CPN [11] ResNet-50 Y 256 × 192 27.0M 6.20 68.6 − − − − − CPN + OHKM [11] ResNet-50 Y 256 × 192 27.0M 6.20 69.4 − − − − − SimpleBaseline [72] ResNet-50 Y 256 × 192 34.0M 8.90 70.4 88.6 78.3 67.1 77.2 76.3 SimpleBaseline [72] ResNet-101 Y 256 × 192 53.0M 12.4 71.4 89.3 79.3 68.1 78.1 77.1 SimpleBaseline [72] ResNet-152 Y 256 × 192 68.6M 15.7 72.0 89.3 79.8 68.7 78.9 77.8 HRNet-W32 HRNet-W32 N 256 × 192 28.5M 7.10 73.4 89.5 80.7 70.2 80.1 78.9 HRNet-W32 HRNet-W32 Y 256 × 192 28.5M 7.10 74.4 90.5 81.9 70.8 81.0 79.8 HRNet-W48 HRNet-W48 Y 256 × 192 63.6M 14.6 75.1 90.6 82.2 71.5 81.8 80.4 SimpleBaseline [72] ResNet-152 Y 384 × 288 68.6M 35.6 74.3 89.6 81.1 70.5 79.7 79.7 HRNet-W32 HRNet-W32 Y 384 × 288 28.5M 16.0 75.8 90.6 82.7 71.9 82.8 81.0 HRNet-W48 HRNet-W48 Y 384 × 288 63.6M 32.9 76.3 90.8 82.9 72.3 83.4 81.2 COCO test-dev Method Backbone Input size #Params GFLOPs AP AP50 AP75 APM APL AR Mask-RCNN [21] ResNet-50-FPN 63.1 87.3 68.7 57.8 71.4 G-RMI [47] ResNet-101 353 × 257 42.6M 57.0 64.9 85.5 71.3 62.3 70.0 69.7 Integral Pose Regression [60] ResNet-101 256 × 256 45.0M 11.0 67.8 88.2 74.8 63.9 74.0 G-RMI + extra data [47] ResNet-101 353 × 257 42.6M 57.0 68.5 87.1 75.5 65.8 73.3 73.3 CPN [11] ResNet-Inception 384 × 288 72.1 91.4 80.0 68.7 77.2 78.5 RMPE [17] PyraNet [77] 320 × 256 28.1M 26.7 72.3 89.2 79.1 68.0 78.6 CFN [25] 72.6 86.1 69.7 78.3 64.1 CPN (ensemble) [11] ResNet-Inception 384 × 288 73.0 91.7 80.9 69.5 78.1 79.0 SimpleBaseline [72] ResNet-152 384 × 288 68.6M 35.6 73.7 91.9 81.1 70.3 80.0 79.0 HRNet-W32 HRNet-W32 384 × 288 28.5M 16.0 74.9 92.5 82.8 71.3 80.9 80.1 HRNet-W48 HRNet-W48 384 × 288 63.6M 32.9 75.5 92.5 83.3 71.9 81.5 80.5 HRNet-W48 + extra data HRNet-W48 384 × 288 63.6M 32.9 77.0 92.7 84.5 73.4 83.1 82.0","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"}]},{"title":"Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation","slug":"Paper-Notes-Pose-Estimation-20210112-Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation","date":"2021-01-12T00:41:07.000Z","updated":"2021-01-18T04:45:39.620Z","comments":true,"path":"2021/01/12/Paper-Notes-Pose-Estimation-20210112-Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation/","link":"","permalink":"http://example.com/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation/","excerpt":"Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation 本文以多人姿态估计中的尺度差异为研究点，提出 HigherHRNet 结构，生成不同尺度的特征图，同时在多尺度的特征图上进行监督学习，实现自底向上的多人姿态估计。 文章提出的方法在 COCO-test-dev 数据集上取得 70.5% 的 mAP","text":"Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation 本文以多人姿态估计中的尺度差异为研究点，提出 HigherHRNet 结构，生成不同尺度的特征图，同时在多尺度的特征图上进行监督学习，实现自底向上的多人姿态估计。 文章提出的方法在 COCO-test-dev 数据集上取得 70.5% 的 mAP 1. Method HigherHRNet 网络结构HRNet 生成原图 1/4 大小的特征图，然后经过一层卷积生成 K 通道的 heatmap。以此为基础，HigherHRNet 网络结构进行下面一系列改动： multi-resolution supervision (MRS) 将 1/4特征图 经过 deconv 模块，生成 1/2特征图； 1/2特征图 进过一层卷积得到 K 通道 1/2heatmap； 同时在 1/4heatmap 和 1/2heatmap 两个特征图上进行监督学习； 预测时使用 1/2heatmap 进行预测。 feature concat步骤1 中的 1/4特征图 先与 1/4heatmap concat，再进行 deconv 操作。 heatmap aggregation步骤4 时，同时使用 1/4heatmap 和 1/2heatmap 进行预测。 extra res. blocks步骤2 中，1/2特征图 经过 4 个 residual block 之后生成 1/2heatmap COCO-val ablation 实验结果| Network | w/ MRS | feature concat. | w/ heatmap aggregation | extra res. blocks | AP | APM | APL || - | - | - | - | - | - | - | - || HRNet | | | | | 64.4 | 57.1 | 75.6 || HigherHRNet | y | | | | 66.0 | 60.7 | 74.2 || HigherHRNet | y | y | | | 66.3 | 60.8 | 74.0 || HigherHRNet | y | y | y | | 66.9 | 61.0 | 75.7 || HigherHRNet | y | y | y | y | 67.1 | 61.5 | 76.1 | Grouping文章使用了 associative embedding 实现 grouping，将 tags 相近的关键点连接成单人姿态。 2. Result COCO test-dev 实验结果 Method AP AP50 AP75 APM APL AR OpenPose∗ [3] 61.8 84.9 67.5 57.1 68.2 66.5 Hourglass∗+ [30] 65.5 86.8 72.3 60.6 72.6 70.2 PifPaf [22] 66.7 - - 62.4 72.9 - SPM [32] 66.9 88.5 72.9 62.6 73.1 - PersonLab+ [33] 68.7 89.0 75.4 64.1 75.5 75.4 HigherHRNet-W48+ 70.5 89.3 77.2 66.6 75.8 74.9","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"bottomup","slug":"bottomup","permalink":"http://example.com/tags/bottomup/"}]},{"title":"Associative Embedding  End-to-End Learning for Joint Detection and Grouping","slug":"Paper-Notes-Pose-Estimation-20210111-Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping","date":"2021-01-11T09:05:40.000Z","updated":"2021-01-18T04:45:58.157Z","comments":true,"path":"2021/01/11/Paper-Notes-Pose-Estimation-20210111-Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/","link":"","permalink":"http://example.com/2021/01/11/Paper-Notes-Pose-Estimation-20210111-Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/","excerpt":"Associative Embedding End-to-End Learning for Joint Detection and Grouping 这篇文章提出利用 Associative Embedding 实现 关键点分组 的方法，作者认为计算机视觉中的很多任务可以看做 Detection + Grouping 的操作，首先检测出关键点，然后给关键点打上 tag ，根据 tag 对关键点进行分组。作者用这种思路实现了一种多人姿态估计的算法。 文章提出的方法在 COCO-test-dev 数据集上取得 65.5% 的 mAP，在 MPII 数据集上取得 77.5 的 mAP","text":"Associative Embedding End-to-End Learning for Joint Detection and Grouping 这篇文章提出利用 Associative Embedding 实现 关键点分组 的方法，作者认为计算机视觉中的很多任务可以看做 Detection + Grouping 的操作，首先检测出关键点，然后给关键点打上 tag ，根据 tag 对关键点进行分组。作者用这种思路实现了一种多人姿态估计的算法。 文章提出的方法在 COCO-test-dev 数据集上取得 65.5% 的 mAP，在 MPII 数据集上取得 77.5 的 mAP 1. Method Network Architecture作者使用 StackHourglass 的网络结构，在最终预测时，同时预测 detection heatmap 和 associative embedding，associative embedding 是一个长度为 1 的向量。预测得到关键点的最强响应之后，将 associative embedding 相近的关键点连接起来，就得到了最终的多人姿态估计结果。作者对 StackHourglass 进行了一点小改动，提高了 resolution 部分的 特征图通道数，为了提高网络的参数量，使其能够学习到 associative embedding 信息。 Detection and GroupingDetection 部分，本文采用和 Hourglass 相同的方法，使用 Heatmap 进行监督，利用 MSE Loss 学习关节点的位置。Grouping 部分，作者定义 grouping loss，定义图像中 N 个 Person， K 个 Keypoint，缩小相同 Person 关键点的 embedding 距离，增大不同 Person 关键点的 embedding 距离。具体定义见论文公式。 Multiscale Evaluation作者使用 Multiscale Evaluation 的方式进行测试。假设使用 4 种 scale 的图像作为输入，得到 4 种 heatmap 和 embedding，那么 新的 heatmap由 4 个 heatmap 进行平均得到，embedding vector 由 4 个 embedding concat 而成，成为长度为 4 的 vector。COCO-test-dev 实验结果| | AP | AP50 | AP75 | APM | APL ||- |- |- |- |- |- || single scale | 0.566 | 0.818 | 0.618 | 0.498 | 0.670 || single scale + refine | 0.628 | 0.846 | 0.692 | 0.575 | 0.706 || multi scale | 0.650 | 0.867 | 0.713 | 0.597 | 0.725 || multi scale + refine | 0.655 | 0.868 | 0.723 | 0.606 | 0.726 | 2. Result COCO-test-dev method AP OpenPose 0.618 Mask RCNN 0.627 G-RMI 0.649 AE 0.655 COCO-test-std method AP OpenPose 0.611 G-RMI 0.643 AE 0.663 MPII method AP OpenPose 0.756 AlphaPose 0.767 AE 0.775","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"bottomup","slug":"bottomup","permalink":"http://example.com/tags/bottomup/"}]},{"title":"Simple Baselines for Human Pose Estimation and Tracking","slug":"Paper-Notes-Pose-Estimation-20210107-Simple-Baselines-for-Human-Pose-Estimation-and-Tracking","date":"2021-01-07T03:34:40.000Z","updated":"2021-01-18T04:44:26.450Z","comments":true,"path":"2021/01/07/Paper-Notes-Pose-Estimation-20210107-Simple-Baselines-for-Human-Pose-Estimation-and-Tracking/","link":"","permalink":"http://example.com/2021/01/07/Paper-Notes-Pose-Estimation-20210107-Simple-Baselines-for-Human-Pose-Estimation-and-Tracking/","excerpt":"Simple Baselines for Human Pose Estimation and Tracking1. Introduction: 提出姿态估计 和 姿态跟踪 的一个简单 baseline","text":"Simple Baselines for Human Pose Estimation and Tracking1. Introduction: 提出姿态估计 和 姿态跟踪 的一个简单 baseline 2. Pose Estimation Using A Deconvolution Head Network作者提出一个简单的网络结构作为 Baseline，网络结构如下： 参考Fig1 作者对比了之前的两种方法 Stacked hourglass 和 CPN，两者都是对特征图进行上采样，生成 heatmap，而作者提出的网络结构，直接通过三层反卷积生成了heatmap，结构十分简单。 2.1 Pose Tracking Based on Optical Flow作者对 ICCV’17 PoseTrack Challenge 的方法（Detect-and-Track）进行改进，提出 多人姿态跟踪 的 Baseline。作者的改进点有一下两个： Joint Propagation using Optical Flow利用前一帧的关键点预测 J(k) 已经两帧的光流 F(k, k+1)，投影出当前帧的关键点 J(k+1)，计算关键点的包围框并扩大一定比例，投影出当前帧的包围框。这样可以解决由于 模糊、遮挡 造成的误检 Flow-based Pose SimilarityPose Track 的时候需要计算两帧之间的 similarity，作者认为，用 包围框的 IOU 作为 metric 不能处理运动太快的情况；直接用 OKS 作为 metric 不能处理人物动作幅度比较大的情况。因此，作者首先利用光流投影出上一帧的关键点在当前帧的位置，然后再计算 OKS。 2.2 Flow-based Pose Tracking Algorithm 参考Fig2 3. Experiments3.1 Pose Estimation on COCO Train： pretrain on ImageNet 将 GT 框扩展成 4:3 crop 出 GT 框并 resize 成固定大小 256x192 数据增强： scale（0.7~1.3）， rotation （-40~40），flip Test： faster-rcnn 检测 origin + flip quarter offset from highest response to the second highest response Method Backbone Input Size OHKM AP 8-stage Hourglass - 256x192 n 66.9 8-stage Hourglass - 256x256 n 67.1 CPN ResNet-50 256x192 n 68.6 CPN ResNet-50 384x288 n 70.6 CPN ResNet-50 256x192 y 69.4 CPN ResNet-50 384x288 y 71.6 Ours ResNet-50 256x192 n 70.4 Ours ResNet-50 384x288 n 72.2 Ours ResNet-101 256x192 n 71.4 Ours ResNet-152 256x192 n 72.0 3.2 Pose Estimation and Tracking on PoseTrack Train: pretrain on COCO 根据 keypoint 画出 bbox，外扩 15% 数据增强 Test： 检测框（ R-FCN、 FPN-DCN ） 光流估计 （ FlowNet2S ) Propagation 丢弃低置信度框（&lt;0.5） 丢弃低置信度关键点 （&lt;0.4） Method Backbone Detector Joint Propagation Similarity Metric mAP MOTA a6 ResNet-50 R-FCN y SMulti_flow 70.3 62.2 b1 ResNet-50 FPN-DCN n SBbox 69.3 59.8 b2 ResNet-50 FPN-DCN n SPose 69.3 59.7 b3 ResNet-50 FPN-DCN y SBbox 72.4 62.1 b4 ResNet-50 FPN-DCN y SPose 72.4 61.8 b5 ResNet-50 FPN-DCN y SFlow 72.4 62.4 b6 ResNet-50 FPN-DCN y SMulti_Flow 72.4 62.9 c6 ResNet-152 FPN-DCN y SMulti_Flow 76.7 65.4","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"}]},{"title":"RMPE  Regional Multi-Person Pose Estimation","slug":"Paper-Notes-Pose-Estimation-20210107-RMPE-Regional-Multi-Person-Pose-Estimation","date":"2021-01-07T03:34:28.000Z","updated":"2021-01-18T04:44:27.980Z","comments":true,"path":"2021/01/07/Paper-Notes-Pose-Estimation-20210107-RMPE-Regional-Multi-Person-Pose-Estimation/","link":"","permalink":"http://example.com/2021/01/07/Paper-Notes-Pose-Estimation-20210107-RMPE-Regional-Multi-Person-Pose-Estimation/","excerpt":"RMPE Regional Multi-Person Pose Estimation1. Introduction top-down 的 muti-person PE 方法有两个缺点：a 有的框(IOU&gt;0.5)实际上框的不准，b 有一些冗余框 提出 Regional Muti-person estimation 框架。 提出 symmetric spatial transformer network (SSTN), 对 SPPE 方法进行改进，在不准确的 bbox 上仍然能够准确地预测姿态。 提出 parametric pose NMS (PP-NMS), 解决冗余框的问题 提出 pose-guided human proposal generator (PGPG), 对训练数据进行数据增强","text":"RMPE Regional Multi-Person Pose Estimation1. Introduction top-down 的 muti-person PE 方法有两个缺点：a 有的框(IOU&gt;0.5)实际上框的不准，b 有一些冗余框 提出 Regional Muti-person estimation 框架。 提出 symmetric spatial transformer network (SSTN), 对 SPPE 方法进行改进，在不准确的 bbox 上仍然能够准确地预测姿态。 提出 parametric pose NMS (PP-NMS), 解决冗余框的问题 提出 pose-guided human proposal generator (PGPG), 对训练数据进行数据增强 2. Related Work Single Person PE： tree models random forrest models CNN based models Muti person PE: part based framework 一大堆没听过的算法 缺点是只利用了局部区域的信息，part 的检测效果可能不好 two-step framework 又是一大堆没听过的算法 本文的框架基于 two-step framework，解决人体 bbox 检测不准的问题。 3. Regional Multi-person Pose Estimationfig3 STN &amp; SDTN （SSTN）：作者认为，目标检测得到的框很有可能是不准确的，这是制约 top-down 方法效果的一个关键点。因此，作者将 detection 的结果输入 SPPE 模块之前，会先经过一个 STN 模块，得到 PE 结果后，再经过一个对称的 SDTN 模块。STN 的目标是从 detection 结果中 warp 出更精确的框，STDN 的目标则相反。STN 和 STDN 都包含 3 个参数，通过推到可以得到两者的关系（参考论文）。 Parallel SPPE：为了使 STN 更准确地定位到 detection proposal 的 domainate person，作者还加入了一个 Parallel SPPE，这个模块是一个固定参数的 SPPE 模块，输入由 STN 生成的 grid 之后，使用预训练好的 SPPE 对 grid 中的 person 进行预测，然后与 groundtruth 进行比较，这里的 groundtruth 是对原始 groundtruth 进行 center-located 操作之后得到的。 我的理解是，假设图片中有 A,B 两个人，目标检测得到一个不太准的框 bbox(A)，Parallel SPPE 的 groundtruth 就是以 A 的 pose groundtruth 为中心的 图片（SPPE 的 gt 就是直接将 gt 坐标转化到 bbox(A) 里面图片），如果 STN 生成的 grid 离 A 很远，Parallel SPPE 的误差就会非常大。 Parametric Pose NMS：为了消除冗余，作者提出了对 Pose 进行 NMS 的方法，NMS 的 metric 是 两个 Pose 的距离，作者提出了两种度量 Pose distance 的方法，NMS 的时候会取两种距离的加权和。 H_sim公式参照论文，这个距离很好理解，就是两个 pose 各个关键点的 L2 距离 K_sim公式参照论文，这个距离的含义是，如果两个 Pose 的某个关键点重合（重合的概念被定义为 一个关键点处于另一个关键点为中心的矩形区域中），就会计算两个关键点置信度的乘积，因此，重叠的关键点越多，并且置信度越高的情况下，计算的结果越大。 这个地方有个很奇怪的点。按照公式，越相似的 Pose 计算出来的 distance 会越大，但 NMS 公式里面却是距离小才会被抑制，不知道哪里看错了。 Poseguided Proposals Generator：为了让 STN+SPPE 模块适应多样的 bbox proposal 输入，作者提出了一种数据增广的方案。作者会统计 detection 的 bbox 和 gt 的偏移量的分布。训练的时候，根据偏移量的分布，随机对 detection 的结果进行 shift。由于不同的 Pose 对应的偏移量分布可能各不相同，作者定义了 4 个 原子姿态，首先对图片的 Pose 用 kmeans 进行聚类，每种原子姿态会对应不同的偏移量分布。","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"}]},{"title":"Cascaded Pyramid Network for Multi-Person Pose Estimation","slug":"Paper-Notes-Pose-Estimation-20210107-Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation","date":"2021-01-07T03:32:27.000Z","updated":"2021-01-18T04:41:31.072Z","comments":true,"path":"2021/01/07/Paper-Notes-Pose-Estimation-20210107-Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation/","link":"","permalink":"http://example.com/2021/01/07/Paper-Notes-Pose-Estimation-20210107-Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation/","excerpt":"Cascaded Pyramid Network for Multi-Person Pose Estimation1. Introduction: 提出 cascaded pyramid network（CPN），包含 global pyramid network 和 pyramid refined network 探讨 多人目标检测中各种因素的影响 COCO 上 test-dev AP： 73.0; test-challenge AP：72.1","text":"Cascaded Pyramid Network for Multi-Person Pose Estimation1. Introduction: 提出 cascaded pyramid network（CPN），包含 global pyramid network 和 pyramid refined network 探讨 多人目标检测中各种因素的影响 COCO 上 test-dev AP： 73.0; test-challenge AP：72.1 2. Related work: muti-pose: bottom-up: DeepCut, OpenPose top-down: Mask-RCNN single pose: CPM hourglass … detection: FPN mask-RCNN 3. Method network structure:Fig-1 Detector:这篇文章使用的 detetor 配置如下， FPN(res101) + ROIAlign + softNMS. AP = 0.411, AP(Person) = 0.533 GlobalNetbasemodel 提取不同 stage 的特征，底层的特征直接用插值的方法 upsample，然后通过 1*1 卷积将通道统一为 256，对特征做 element-wise sum 得到 feature pyramid，金字塔上每一层特征通过一个 1*1 conv + 3*3*17 conv 得到 globalnet 的输出（ heatmap * 17） RefineNet使用 bottleneck 结构对特征进行 upsample，特征图越小，通过的 bootleneck 越多，upsample 倍率越大，最后所有特征都具有相同的大小。将 4 个 stage 的特征在 channel wise concat 起来。 ohkm根据 loss 判断出 hard keypoint，然后只对部分关键点的 loss 进行反向传播。 4. Experiment Cropping Strategybbox --(extend)--&gt; 256:192 --(crop)--&gt; block --(resize)--&gt; 256:192 Data Augmentationrandom fliprandom rotationrandom scale Test:Apply gussian filter to heatmapAlso test flipped image最高的响应 R1，第二高的响应为 R2，最终结果 = R1+0.25*R2Rescore: S(pose) = S(bbox) * avg(S(key-point)) 5. ResultAblation experiment AP FLOPs Params ResNet-50 + dilation(res4-5) 66.5 17.71G 92M GlobalNet only 66.6 3.90G 94M CPN w/o ohkm 68.6 6.20G 102M CPN 69.4 6.20G 102M GlobalNet + Concat 68.5 5.87G - GlobalNet + 1 bottleneck + Concat 69.2 6.92G - CPN (384 × 288) 71.6 13.9G - 在 refineNet 中使用不同的 layer AP FLOPs C2 68.3 5.02G C2-3 68.4 5.50G C2-4 69.1 5.88G C2-5 69.4 6.20G","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"}]},{"title":"Python 编写 C 扩展","slug":"Python-20210107-Python-编写-C-扩展","date":"2021-01-07T03:07:56.000Z","updated":"2021-01-07T03:09:16.519Z","comments":true,"path":"2021/01/07/Python-20210107-Python-编写-C-扩展/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-Python-%E7%BC%96%E5%86%99-C-%E6%89%A9%E5%B1%95/","excerpt":"Python 编写 C/C++ 扩展Python 的特性使得其在编写大型项目的时候并不占优势，目前主流的项目代码还是用C/C++或Java编写的，为了让我们写的Python代码能够嵌入到项目中，一般有两种方式，1）让Python操作数据库，相当于通过读数据库完成传参，写数据库实现结果返回；2）使用Python嵌入式编程，操作数据库的方式相对简单，但是存在一些问题，例如，主程序需要监听Python代码的返回值、数据库的权限需要暴露给模块、每一个模块都需要变为一个单独的服务常驻进程，相比之下，嵌入式编程的方式更加优雅，只需要将Python代码编写成 DLL/.so 文件提供给主程序，主程序直接调用就可以，这种过程的一个副产物就是可以保护源代码。下面记录一次在Windows上使用Python嵌入式编程的实战过程，主要包含 工具准备，创建DLL，调用DLL，变量类型优化，依赖解决五个部分","text":"Python 编写 C/C++ 扩展Python 的特性使得其在编写大型项目的时候并不占优势，目前主流的项目代码还是用C/C++或Java编写的，为了让我们写的Python代码能够嵌入到项目中，一般有两种方式，1）让Python操作数据库，相当于通过读数据库完成传参，写数据库实现结果返回；2）使用Python嵌入式编程，操作数据库的方式相对简单，但是存在一些问题，例如，主程序需要监听Python代码的返回值、数据库的权限需要暴露给模块、每一个模块都需要变为一个单独的服务常驻进程，相比之下，嵌入式编程的方式更加优雅，只需要将Python代码编写成 DLL/.so 文件提供给主程序，主程序直接调用就可以，这种过程的一个副产物就是可以保护源代码。下面记录一次在Windows上使用Python嵌入式编程的实战过程，主要包含 工具准备，创建DLL，调用DLL，变量类型优化，依赖解决五个部分 工具准备1. MinGW网上的很多教程使用 VS 编写 DLL，鉴于 VS 安装过程太麻烦，并且还不能跨平台，这里选择的编译器是 GCC，直接在 MinGW-W64 官网上下载安装程序就可以了（不要在 minGW 官网上下载，在线安装过程会不断 fail），安装完了之后需要做两件事：121. 将 MinGW 的 bin 目录加入环境变量2. 在 bin 目录下建立软链接，make -&gt; mingw32-make安装成功之后可以在命令行使用gcc, make这两个命令 2. Cython这个很简单，只需要 pip install cython 就安装好了，安装成功可以在命令行使用 cython命令 创建 DLL1. Python 源文件这次封装的 Python 代码源文件如下12345678910111213141516171819202122# Y_interpoldate.pyimport pandas as pdimport numpy as npfrom scipy.interpolate import griddataimport osos.environ[&#x27;PROJ_LIB&#x27;] = r&#x27;D:\\Softwares\\Anaconda3\\pkgs\\proj4-4.9.3-hcf24537_7\\Library\\share&#x27;from mpl_toolkits.basemap import Basemapdef load_data(csv_name): return pd.read_csv(csv_name, header=None, names=[&#x27;lons&#x27;,&#x27;lats&#x27;,&#x27;values_&#x27;])def interpolate(): data = load_data(&#x27;HG.csv&#x27;) basemap = Basemap(projection=&#x27;ortho&#x27;,lat_0=30,lon_0=120) x, y = basemap(data.lons.values, data.lats.values) xi = np.linspace(min(x)*0.8, max(x)*1.2, 300) yi = np.linspace(min(y)*0.8, max(y)*1.2, 300) X, Y = np.meshgrid(xi, yi) Z = griddata( (x,y), data.values_, (X,Y), method=&#x27;linear&#x27; ) print(X.shape, Y.shape, Z.shape)需要封装的函数是 interpolate，为了简单起见，函数没有设置入参和返回值，在动态变量类型优化部分会深入讨论 2. Cython 编译第一步用 Cython 编译源文件，Cython 编译的源文件是 .pyx 格式，Cython中定义导出函数的格式如下：12345cdef public PyObject* func(PyObject* argv, ...)# cdef： 是 Cython 定义函数的关键字； # public： 代表函数是要被导出的； # void： 指定函数的返回类型。 因此将源文件需要导出的函数进行下面的改动并另存为 .pyx 文件123# _Y_interpolate.pyxcdef public void _interpolate(): ...使用如下命令编译 .pyx文件生成 .c, .h 文件1cython _Y_interpolate.pyx在生成的头文件中可以找到两个函数：1234__PYX_EXTERN_C void _interpolate(void);PyMODINIT_FUNC init_Y_interpolate(void); //for py2PyMODINIT_FUNC PyInit__Y_interpolate(void); //for py3注意这里另存的 .pyx 文件名和 函数名 前面都添加了一个 _，在之后会解释。 2.5 生成 exe 文件生成 _Y_interpolate.c 和 _Y_interpolate.h 文件后，就可以直接利用这两个源文件编写 C 程序了123456789101112// cppmain.c#include &quot;Python.h&quot;#include &quot;_Y_interpolate.h&quot; int main(int argc, char *argv[])&#123; Py_Initialize(); PyInit__Y_interpolate(); _interpolate(); Py_Finalize(); return 0;&#125;编译命令1234gcc _Y_interpolate.c cppmain.c -m64 -mthreads -Wall -O3 \\-I. -ID:\\Softwares\\Anaconda3\\include \\-LD:\\Softwares\\Anaconda3\\Lib -LD:\\Softwares\\Anaconda3\\ -lpython36 -o cppmain可以看见直接调用的程序还比较麻烦，需要使用 Py_Initialize() 和 Py_Finalize() 包裹代码，而且使用函数之前还需要一次初始化(注意这里的初始化函数 python2 和 python3 有区别，例子是 py3 的写法)。下面看看 DLL 封装是怎么做的吧。 3 生成 DLL 文件编写下面的 DLL 主程序代码1234567891011// Y_interpolate.h#ifndef Y_interpolate_H#define Y_interpolate_H#ifdef BUILD_DLL __declspec(dllexport) void interpolate(void);#else __declspec(dllimport) void interpolate(void);#endif#endif1234567891011121314151617181920212223//Y_interplate.c#include &lt;Python.h&gt;#include &lt;Windows.h&gt;#include &quot;Y_interpolate.h&quot;// 定义DLL导出函数 interpolate()，如果没有这一步，同样可以生成 dll，但是符号表中找不到 _interpolate()__declspec(dllexport) void __stdcall interpolate() &#123; return _interpolate();&#125;//将 Python 初始化的代码封装到 DLL 的主函数中BOOL WINAPI DllMain(HINSTANCE hinstDLL,DWORD fdwReason,LPVOID lpReserved) &#123; switch( fdwReason ) &#123; case DLL_PROCESS_ATTACH: Py_Initialize(); PyInit_Y_interpolate(); break; case DLL_PROCESS_DETACH: Py_Finalize(); break; &#125; return TRUE;&#125;编译命令：12345gcc _Y_interpolate.c Y_interpolate.c Y_interpolate.h \\-DBUILD_DLL -shared -m64 -Wall -O3 \\-I. -ID:\\Softwares\\Anaconda3\\include \\-LD:\\Softwares\\Anaconda3\\Lib -LD:\\Softwares\\Anaconda3\\ \\-lpython36 -o Y_interpolate.dll -Wl,--output-def,Y_interpolate.def,--out-implib,Y_interpolate.a可以看见，Y_interplate.c 中的代码就是将 Cython 生成的 _Y_interplate.c 进行了一次封装，第一，将函数接口封装成 DLL导出函数；第二，将 Python 初始化相关的代码封装到DLL的加载主函数中（这就是之前 Cython 生成 .c 文件时最好修改一下文件名和函数接口的原因）。编译的时候 -Wl 选项后面的内容指定编译器同时生成了 def 文件和 .a 文件，这两个文件一般是用来存放代码中的函数表的，实际上，目前调用的时候并不需要这两个文件 调用 DLL生成好 DLL 后，就可以编写 C 程序来调用了，看一下调用的代码12345678//dllcall.c#include &quot;Y_interpolate.h&quot;int main()&#123; interpolate(); return 0;&#125;编译命令：1gcc dllcall.c -m64 -mthreads -Wall -O3 -I. -L. -lY_interpolate -o dllcall不需要解释，无论是程序代码编写还是编译命令都非常优雅简洁，没有任何冗余。只需要一个 .h 文件和 dll 文件，就可以让其他程序调用 interpolate 这个接口（实际上，即使没有头文件，程序也可以编译成功，只是会出现 warning）。 变量类型优化实际项目中，混合编程大多数都是需要传递参数的，由于这一部分比较复杂，因此在上面的例子中导出函数入参和返回值都设计为 void，下面详细讲解 Python 混合编程的时候怎样传参。众所周知，Python 中的变量是动态变量，不需要显式声明就可以直接使用，而 C/C++ 中的变量都是静态变量，因此传参的过程必然需要转换，转换例子可以参看 Python 官方的例子1234567891011121314151617// 1.1 转换入参 C-&gt;PyObject，构造包含3个元素的数组PyObject* args = PyTuple_New(3);PyObject* arg1 = Py_BuildValue(&quot;i&quot;, 100); // 整数参数PyObject* arg2 = Py_BuildValue(&quot;f&quot;, 3.14); // 浮点数参数PyObject* arg3 = Py_BuildValue(&quot;s&quot;, &quot;hello&quot;); // 字符串参数PyTuple_SetItem(args, 0, arg1);PyTuple_SetItem(args, 1, arg2);PyTuple_SetItem(args, 2, arg3);// 1.2 与上面等价PyObject* args = Py_BuildValue(&quot;(ifs)&quot;, 100, 3.14, &quot;hello&quot;);// 2 调用函数PyObject* pRet = PyObject_CallObject(pv, args);// 3. 转换返回值 PyObject-&gt;C，类似函数还有 PyFloat_AsDouble,PyString_AsString,PyArg_ParseTuple 等long res = PyInt_AsLong(pRet);例程中的转换分别使用了 Py_BuildValue，PyInt_AsLong两个接口，这都是 Python.h 中定义的函数，用这些操作去构造数组无疑是繁琐且效率低下的，那么可不可以避免这些操作呢。答案是可以的，我们可以在 Cython 编译 pyx文件的时候告诉它入参和返回值的类型，这样调用方就可以像调用C语言的函数一样调用导出的函数了，由于使用了静态类型，这样做的一个副产物就是可以提高Python代码的效率。123456789101112131415161718192021222324252627# _Y_interpolate.pyxcdef public void _interpolate(float in_arr[][3], float out_arr[][3], int in_size, int out_size): cdef np.npy_intp in_dims[2] cdef np.npy_intp out_dims[2] in_dims = &#123;in_size,3&#125; py_in_arr = np.PyArray_SimpleNewFromData(2, in_dims, np.NPY_FLOAT, &lt;void*&gt; in_arr) out_dims = &#123;out_size,3&#125; py_out_arr = np.PyArray_SimpleNewFromData(2, out_dims, np.NPY_FLOAT, &lt;void*&gt; out_arr) py_interpolate(py_in_arr, py_out_arr, out_size)def py_interpolate( np.ndarray[float, ndim=2, mode=&quot;c&quot;] in_arr not None, np.ndarray[float, ndim=2, mode=&quot;c&quot;] out_arr not None, int out_size): data = pd.DataFrame(in_arr, columns=[&#x27;lons&#x27;,&#x27;lats&#x27;,&#x27;values_&#x27;]) _min, _max = data.values_.min(), data.values_.max() data[&#x27;norm_values_&#x27;] = data.values_.apply(lambda x: x-_min/_max) basemap = Basemap(projection=&#x27;ortho&#x27;,lat_0=30,lon_0=120) x, y = basemap(data.lons.values, data.lats.values) xi = np.linspace(min(x)*0.8, max(x)*1.2, 300) yi = np.linspace(min(y)*0.8, max(y)*1.2, 300) X, Y = np.meshgrid(xi, yi) Z = griddata( (x,y), data.norm_values_, (X,Y), method=&#x27;linear&#x27;) res = np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1) res = np.delete(res, np.where(np.isnan(res))[0], axis=0) out_arr[...] = res[:out_size,:]这里我们使用了 numpy 的 PyArray_SimpleNewFromData() 接口将 C 语言的数组转换为 numpy.ndarray, 相反的操作可以通过 PyArray_DATA()返回数组指针，但是由于二维数组的指针的特殊性，本例没有选择使用指针作为返回值。利用 cython 编译 pyx 文件后，生成 .c,.h文件，同样可以直接使用源文件或者编译成动态链接库，这里给出通过源文件使用接口的示例1234567891011121314151617181920212223#include &quot;Python.h&quot;#include &quot;_Y_interpolate.h&quot;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;malloc.h&gt;#include &lt;stdlib.h&gt;#include &lt;math.h&gt;void read_csv(float arr[][3], int line_cnt);int main(int argc, char *argv[])&#123; int in_size = 48000, out_size = 48000; float in_arr[in_size][3], out_arr[out_size][3]; read_csv(in_arr, in_size); Py_Initialize(); PyInit__Y_interpolate(); _interpolate(in_arr, out_arr, in_size, out_size); Py_Finalize(); printf(&quot;%f, %f, %f&quot;, out_arr[1][0], out_arr[1][1], out_arr[1][2]); return 0;&#125;编译命令为：12345gcc _Y_interpolate.c cppmain.c -m64 -mthreads -Wall -O3 \\-I. -ID:\\Softwares\\Anaconda3\\include -ID:\\Softwares\\Anaconda3\\lib\\site-packages\\numpy\\core\\include \\-LD:\\Softwares\\Anaconda3\\ -LD:\\Softwares\\Anaconda3\\Lib \\-lpython36 -o examples\\cppmain# 注意需要引用 numpy 的头文件至此，在 Cython 的帮助下，我们终于完成了一个 Python 代码的封装，熟悉这个步骤，就可以在实际项目中很轻松地利用 Python 进行开发了，利用运行效率换取开发效率。 依赖解决0. 依赖问题试试运行按照上面步骤生成的 exe 文件，可能会出现提示 找不到 Y_interpolate.dll，将生成的 Y_interpolate.dll 文件复制到当前文件夹下，继续报错 找不到 python36.dll，同样将 python36.dll 复制到当前文件夹下。同时发布这三个文件，程序可以启动，但是并不一定能运行，如果电脑上的 Anaconda 被卸载，程序启动后会提示错误12Fatal Python error: Py_Initialize: unable to load the file system codecModuleNotFoundError: No module named &#x27;encodings&#x27;显然这是 Python 的各种包引发的问题，那么怎么把程序中用到的 Python 包都打包到新的环境中呢？ 0.5 用户安装依赖最简单解决依赖的方法就是在用户的计算机上安装 Python，然后通过 pip 安装需要的依赖，这种方法只需要建立虚拟环境，然后使用 pip freeze, conda env export 等命令就可以完成。这种方法虽然简单，但是对于用户来说也更容易出错，有没有什么办法可以让开发者把所有依赖都打包都一个文件夹或打包成一个文件的方法呢。下面介绍两种可行的方法。 1. PyinstallerPyinstaller 是一个将 Python 代码编译成 EXE 文件的工具，类似的工具还有 cx_freeze,py2exe, 这些工具生成 exe 时，会同时将 exe 所需要的依赖打包到一个文件夹中，将这个文件夹与生成的 DLL 一起发布，就可以解决依赖问题。 尝试到一半，打包的体积有点儿大，网上有很多相关讨论，时间问题没有继续试验 2. 虚拟环境这个方法与 0.5 中介绍的方法有些相似，只是不需要用户自己安装了。12345# 首先创建虚拟环境conda create -n dependency python=3.6conda activate dependencypip install numpy pandas scipy matplotlib ...安装完毕后，用户需要的依赖就全部都在123D:\\Softwares\\Anaconda3\\envs\\dependency\\DLLs\\D:\\Softwares\\Anaconda3\\envs\\dependency\\Lib\\D:\\Softwares\\Anaconda3\\envs\\dependency\\Lib\\site-packages\\这个三个目录里面了,运行时候将 PYTHONPATH 指向这三个子目录 或者 set PYTHONHOME=D:\\Softwares\\Anaconda3\\envs\\dependency 就可以成功运行程序了。因此我们可以在 C 程序中加入如下代码1234567wchar_t* py_home = Py_GetPythonHome();printf(&quot;%S&quot;, py_home);if(py_home==NULL)&#123; Py_SetPythonHome(L&quot;./dependency&quot;);&#125;Py_Initialize();......这样就可以将 Python 所需要的依赖打包给用户，用户用 PYTHONHOME 环境变量或者 .pth 文件指定依赖的位置，或者直接放在 exe 所在目录即可。（注意路径中不要出现中文字符） 扩展阅读 pyinstaller 打包 pandas cython 编译 pyd boost-python swig protocol buffer","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"动态链接库","slug":"动态链接库","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"混合编程","slug":"混合编程","permalink":"http://example.com/tags/%E6%B7%B7%E5%90%88%E7%BC%96%E7%A8%8B/"}]},{"title":"argparser 模块","slug":"Python-20210107-argparser-模块","date":"2021-01-07T03:05:37.000Z","updated":"2021-01-07T03:07:00.629Z","comments":true,"path":"2021/01/07/Python-20210107-argparser-模块/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-argparser-%E6%A8%A1%E5%9D%97/","excerpt":"Command line parseargpaser 是 Python 官方推荐的 命令行参数解析器，习惯使用它可以极大地方便 Python程序的交互。","text":"Command line parseargpaser 是 Python 官方推荐的 命令行参数解析器，习惯使用它可以极大地方便 Python程序的交互。 Example1234567import argparseargs = argparse.ArgumentParser(descroption=&#x27;Argpase example&#x27;)args.add_argument(&#x27;first&#x27;, type=str, help=&#x27;first argument&#x27;)args = args.parse_args()print(args.fisrt) 必选参数1args.add_argument(&#x27;first&#x27;, type=str, help=&#x27;first argument&#x27;) 可选参数1args.add_argument(&#x27;--optional&#x27;, type=str, help=&#x27;first argument&#x27;) action1args.add_argument(&#x27;--action_1&#x27;, action=&#x27;store_true&#x27;, help=&#x27;action_1 is True&#x27;)","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"argparser","slug":"argparser","permalink":"http://example.com/tags/argparser/"}]},{"title":"Python 多线程","slug":"Python-20210107-Python-多线程","date":"2021-01-07T03:03:36.000Z","updated":"2021-01-07T03:04:14.742Z","comments":true,"path":"2021/01/07/Python-20210107-Python-多线程/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-Python-%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"0. Intro多线程/多进程 在执行 IO密集型 或者 CPU密集型 的任务时，能够大大提高效率。Python 实现 多线程/多进程 的方式有很多，这里列举几个我见过的。 thread/threading/Queue ： Python 官方库，接口比较简单 multiprocessing concurrent ：对 Threading 进一步封装 joblib","text":"0. Intro多线程/多进程 在执行 IO密集型 或者 CPU密集型 的任务时，能够大大提高效率。Python 实现 多线程/多进程 的方式有很多，这里列举几个我见过的。 thread/threading/Queue ： Python 官方库，接口比较简单 multiprocessing concurrent ：对 Threading 进一步封装 joblib 多线程是一个看起来很复杂，但是实际学习使用很简单的一个东西，所以当你有并发运算的需求的时候，一定要勤于动手，使用多线程/多进程去实现，这样会节省很多时间。关于多线程与多进程的选择，一般的原则是 IO 密集型任务使用多线程， CPU密集型任务使用多进程，Python3 中好像还有 协程 的概念，属于比线程更加细粒度的并发，暂时没有研究过。这篇文章主要记录 concurrent 库的简单使用。 Python2 中需要安装 future 库，Python3 的官方库中已经包含。 concurrentconcurrent 的接口非常简单：12345678910111213141516171819202122232425262728def print_job(obj): print(obj) return obj from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executorimport time# 多线程s = time.time()with ThreadPoolExecutor(max_workers=8) as pool: results = list( pool.map(print_job, range(100000)) )t1 = (time.time()-s)# 多进程s = time.time()with ProcessPoolExecutor(max_workers=8) as pool: results = list( pool.map(print_job, range(100000)) )t2 = (time.time()-s)# 循环s = time.time()for i in range(100000): print_job(i)t3 = (time.time()-s)print(t1, t2, t3)8.888239145278937.6266052722930912.69059157371521由于举了一个 非IO密集型，非 CPU 密集型 的例子，所以结果非常尴尬。总之，Python 的并发编程接口就是这样的。","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Python 中的 configure 设计","slug":"Python-20210107-Python-中的-configure-设计","date":"2021-01-07T03:02:06.000Z","updated":"2021-01-18T04:47:29.454Z","comments":true,"path":"2021/01/07/Python-20210107-Python-中的-configure-设计/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-Python-%E4%B8%AD%E7%9A%84-configure-%E8%AE%BE%E8%AE%A1/","excerpt":"Build configure file for you project作为程序员，配置文件的作用和必要性无需赘言，很多优秀的开源项目也会配备一个 config 文件，下面介绍一些常见的 config 方法","text":"Build configure file for you project作为程序员，配置文件的作用和必要性无需赘言，很多优秀的开源项目也会配备一个 config 文件，下面介绍一些常见的 config 方法 Python 脚本Python 脚本是一种简单的配置方法。这种方法适用于一些自己写的小工程，一来它安全性不高，二来违背了配置与代码解耦的原则。 1234567891011121314151617181920############################################# databaseconfig.py#############################################!/usr/bin/env pythonimport preprocessingmysql = &#123;&#x27;host&#x27;: &#x27;localhost&#x27;, &#x27;user&#x27;: &#x27;root&#x27;, &#x27;passwd&#x27;: &#x27;my secret password&#x27;, &#x27;db&#x27;: &#x27;write-math&#x27;&#125;preprocessing_queue = [preprocessing.scale_and_center, preprocessing.dot_reduction, preprocessing.connect_lines]use_anonymous = True############################################# main.py#############################################!/usr/bin/env pythonimport databaseconfig as cfgconnect(cfg.mysql[&#x27;host&#x27;], cfg.mysql[&#x27;user&#x27;], cfg.mysql[&#x27;password&#x27;]) YAML 文件 yaml 文件也是一种很流行的配置方法，相比于使用 Python 脚本，部署的时候， yaml 文件可以做到与代码解耦，因此更友好。相比于 json 文件，yaml 文件更易于阅读修改，除非有特殊需求，一般情况使用 yaml 文件配置工程是很好的选择。 1234567891011121314151617181920212223242526272829####################### databaseconfig.yml######################mysql: host: localhost user: root passwd: my secret password db: write-mathother: preprocessing_queue: - preprocessing.scale_and_center - preprocessing.dot_reduction - preprocessing.connect_lines use_anonymous: yes####################### main.py######################import yamlwith open(&quot;config.yml&quot;, &#x27;r&#x27;) as ymlfile: cfg = yaml.load(ymlfile)for section in cfg: print(section)print(cfg[&#x27;mysql&#x27;])print(cfg[&#x27;other&#x27;]) yaml 文件里面还可以指定 item 的数据类型数据类型对应表 1234video_size: !!python/tuple [1280, 720] # 注意后面是中括号stride: 300history: 10mask_n: 10 configparserconfigparser 是 Python 的一个模块。","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}]},{"title":"namedtuple 模块","slug":"Python-20210107-namedtuple-模块","date":"2021-01-07T02:57:47.000Z","updated":"2021-01-07T02:58:26.516Z","comments":true,"path":"2021/01/07/Python-20210107-namedtuple-模块/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-namedtuple-%E6%A8%A1%E5%9D%97/","excerpt":"Namedtuple: A useful data structure for explicit codingnamedtuple 是 collections 包中的一个数据结构，可以很方便的存储从 csv, 数据库 中读取到的数据，这种数据的特点是，数据分为多条记录，每一条记录包含若干字段。","text":"Namedtuple: A useful data structure for explicit codingnamedtuple 是 collections 包中的一个数据结构，可以很方便的存储从 csv, 数据库 中读取到的数据，这种数据的特点是，数据分为多条记录，每一条记录包含若干字段。 下面以一个班级学生的成绩表为例讲解一下，成绩表中包含 学号，姓名，成绩 三个字段 list/dict 存储如果对 Python 的各种数据类型接触不多，很容易会选择使用 list 或者 dict 去存储。12345678910111213141516171819202122232425262728293031323334353637raw_data = [ [ 01, &#x27;jack&#x27;, &#x27;80&#x27;], [ 02, &#x27;mike&#x27;, &#x27;82&#x27;], [ 03, &#x27;mary&#x27;, &#x27;96&#x27;],]list_sheet = []for record in raw_data: list_sheet.append(record)# 这种方法的缺点是，每一条记录的各个字段具有什么含义并不清晰# 列表嵌字典dict_sheet = []dict_record = &#123; &#x27;stu_id&#x27;: None, &#x27;name&#x27;: None, &#x27;score&#x27;: None&#125;for record in raw_data: _id, _name, _score = record dict_record[&#x27;stu_id&#x27;] = _id dict_record[&#x27;name&#x27;] = _name dict_record[&#x27;score&#x27;] = _score dict_sheet.append(dict_record) # 字典嵌列表 dict_sheet2 = &#123; &#x27;stu_id&#x27;: [], &#x27;name&#x27;: [], &#x27;score&#x27;: [],&#125;for record in raw_data: _id, _name, _score = record dict_sheet2[&#x27;stu_id&#x27;].append(_id) dict_sheet2[&#x27;name&#x27;].append(_name) dict_sheet2[&#x27;score&#x27;].append(_score)# 这两种方法代码写起来比较麻烦，而且如果字典的 value 是 tuple/list 类型理解起来会有一些困难 class 方法熟练使用 C/C++ 的人，比较容易想到用 class 来存储数据12345678910111213def Record(): stu_id = None name = None score = None def __init__(self, _id, _name, _score): self.stu_id = _id self.name = _name self.score = _score sheet = []for record in raw_data: sheet.append(Record(**record))# 这种方法需要定义出来一个 class，代码同样显得比较麻烦 namedtuple实际上，这种场景最合适的数据结构就是 namedtuple，namedtuple 可以理解为用最简单的代码去定义一个 Record Class1234567891011from collections import namedtupleRecord = namedtuple(&#x27;Record&#x27;, [&#x27;stu_id&#x27;, &#x27;name&#x27;, &#x27;score&#x27;])sheet = []for record in raw_data: sheet.append(Record._make(record)) for r in sheet: print(r.stu_id, r.name, r.score)# 相比之下，这种方法显得非常 explicit，习惯使用这种数据结构，处理 csv 数据的时候可以大大提高代码可读性。","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}]},{"title":"Python 中的 import 机制","slug":"Python-20210107-Python-中的-import-机制","date":"2021-01-07T02:56:12.000Z","updated":"2021-01-07T02:58:44.711Z","comments":true,"path":"2021/01/07/Python-20210107-Python-中的-import-机制/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-Python-%E4%B8%AD%E7%9A%84-import-%E6%9C%BA%E5%88%B6/","excerpt":"module import 12+ A/ - a.py import m from m import * 1- m.py # empty 1234 &#x27;&#x27;&#x27;最简单的 import 情况&#x27;&#x27;&#x27;","text":"module import 12+ A/ - a.py import m from m import * 1- m.py # empty 1234 &#x27;&#x27;&#x27;最简单的 import 情况&#x27;&#x27;&#x27; package import 12+ A/ - a.py import m from m import * import C from C import * 1- m.py import C from C import * 12+ C/ - __init__.py # empty 1234567 &#x27;&#x27;&#x27;package 的 import Q - ImportError: No module named C A : python2 中，package 文件夹下必须有 __init__.py&#x27;&#x27;&#x27; relative import 123456################################################################################## &#x27;import 关键字的 相对导入&#x27;+ X/ + A/ - __init__.py - a.py import m from m import * import C from C import * 1- m.py import C from C import * 123456789101112131415161718&#x27;&#x27;&#x27;在 A 的 父级目录执行 python -c &quot;import A.a&quot; a.py:1 --&gt; import m Q - ModuleNotFoundError: No module named &#x27;m&#x27; A : import &lt;pkg/module&gt;， pkg/module 必须在 PYTHONAPTH 路径下，所以这里有两种解决方案， 一种是将目录 A 的路径加入 PYTHONPATH；一种是改成 import A.m。 很明显后一种改法固定了调用脚本与 packageA 的相对路径关系，因此只有在 A 作为 X 的一个 subpackage 才可以这么写；而 PYTHONPATH 又跟代码所在的路径相关，因此也不太实用。 所以这种情况下就需要用 from &lt;&gt; import &lt;&gt; 语法，这种语法支持 relative import&#x27;&#x27;&#x27;################################################################################### &#x27;from ... import ... 的相对导入&#x27;+ X/ + A/ - __init__.py - a.py # import A.m from m import * # import A.C from C import * 1- m.py # import A.C from C import * 123456789101112131415161718192021 + C/&#x27;&#x27;&#x27;修正好 import m/C 的错误之后，继续执行 python -c &quot;import A.a&quot; a.py:1 --&gt; import A.m m.py:2 --&gt; from C import * Q - ModuleNotFoundError: No module named &#x27;m&#x27; A : 这里可以通过改 PYPATHPATH 消除这个错误； 如果不改环境变量的话，from ... import ... 语法支持相对导入，改成 from .C import * # 这里的 .C 是相对于 __package__ 变量的, 这一句等价于下面 from __package__.C # 由于执行的语句是 import A.m， 所以这里的 __package__=&#x27;A&#x27;，所以等价于下面 from A.C import *&#x27;&#x27;&#x27;################################################################################## &#x27;from ... import ... 的导入(2)&#x27;+ X/ + A/ - __init__.py - a.py # import A.m from .m import * # import A.C from .C import * 1- m.py # import A.C from .C import * 1234 + C/+ B/ - __init__.py - b.py from ..A import * 123456789101112131415161718&#x27;&#x27;&#x27;将 A 中的导入全部都变成相对导入之后，试一下在 B 里面导入 package A，执行 python -c &quot;import B.b&quot; --- b.py:1 --&gt; from ..A import * Q - ValueError: attempted relative import beyond top-level package A : 这是由于 b.py 中的 import 语句使用了上一级的 package，而调用语句是 import B.b，只有一层 package， 要避免这个错误，需要将 X 作为一个 package 运行： python -c &quot;import X.B.b&quot;&#x27;&#x27;&#x27;################################################################################### &#x27;module 的执行&#x27;+ X/ + A/ - __init__.py - a.py # import A.m from .m import * # import A.C from .C import * 1- m.py # import A.C from .C import * 1234 + C/+ B/ - __init__.py - b.py from ..A import * 12345678910111213141516&#x27;&#x27;&#x27;先总结一下，当你实现一个模块 a.py 后，如果你想在其他路径下写代码调用这个模块，就会用到形如 import A.a 这样的语句，但是这种情况下，a.py 当中的 import 语句可能会报错。因为 import 语句只支持绝对导入，所以这种情况要用 from .. import ..，并且将其改成相对导入的形式。但是这种情况下，你再回到文件夹 A，却无法执行 python a.py 了 --- a.py:2 --&gt; from .m import * Q - ModuleNotFoundError: No module named &#x27;__main__.m&#x27;; &#x27;__main__&#x27; is not a package A : 执行 import A.a 的时候，a 作为一个 module，from 语句从 __package__ 变量相对的路径下去 import 执行 python A/a.py 的时候， a 作为一个 脚本，__package__=None，from 语句从 __name__ 的相对路径下去 import 这时正确的做法是将 a.py 作为一个 module 执行。 python [-i] -m A.a # 用了这么久才知道 python 有个 -i 选项 !!!!!!!!&#x27;&#x27;&#x27;","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}]},{"title":"Python tips","slug":"Python-20210107-Python-tips","date":"2021-01-07T02:52:07.000Z","updated":"2021-01-07T02:55:31.630Z","comments":true,"path":"2021/01/07/Python-20210107-Python-tips/","link":"","permalink":"http://example.com/2021/01/07/Python-20210107-Python-tips/","excerpt":"收集一些短小的 Python 代码段","text":"收集一些短小的 Python 代码段 123456# 查看磁盘占用import osvfs = os.statvfs(&#x27;/&#x27;)vfstotal = vfs.f_blocks * statvfs.f_bsizeused = vfs.f_bsize * (vfs.f_blocks - vfs.f_bfree) 123# 输出保留 2 位小数a = 3.1415926print( round(a,2) ) 123456# 查看文件类型improt filetypef = &#x27;1.txt&#x27;kind = filetype.guess(f)print(kind.mime)","categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}]},{"title":"Linux .so 文件编译","slug":"C-20210107-Linux-so-文件编译","date":"2021-01-07T02:46:50.000Z","updated":"2021-01-07T02:50:40.647Z","comments":true,"path":"2021/01/07/C-20210107-Linux-so-文件编译/","link":"","permalink":"http://example.com/2021/01/07/C-20210107-Linux-so-%E6%96%87%E4%BB%B6%E7%BC%96%E8%AF%91/","excerpt":"http://www.cnblogs.com/tzhangofseu/archive/2011/11/15/2249585.html","text":"http://www.cnblogs.com/tzhangofseu/archive/2011/11/15/2249585.html .so生成 在 soDemo.cpp文件中实现函数1234#include&lt;iostrem&gt;void fun()&#123; std::cout&lt;&lt;&quot;hello&quot;&lt;&lt;std::endl;&#125; 使用 g++ 编译动态链接库1g++ soDemo.cpp -shared -fPIC -o libsoDemo.so .so使用 在 socallDemo.cpp文件中直接调用库函数123456#include&lt;iostream&gt;void fun();int main()&#123; fun(); retrun 0;&#125; 使用 g++ 编译程序，在命令行参数中指定 .so文件123g++ socallDemo.cpp -L. -lsoDemo -o socallDemo# -L. 指定 .so 文件路径为当前目录# -lsoDemo 指定 .so 文件的名称为 libsoDemo.so 运行程序的时候需要让 loader 能够找到 .so文件http://blog.csdn.net/sahusoft/article/details/7388617方法1. 修改 /etc/ld.so.conf 文件，在其中加入 .so文件所在路径方法2. 修改环境变量 LD_LIBRARY_PATHexport LD_LIBRARY_PATH= $path_to_lib:$LD_LIBRARY_PATH","categories":[{"name":"C++","slug":"C","permalink":"http://example.com/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"动态链接库","slug":"动态链接库","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93/"}]},{"title":"DLL 编译","slug":"C-20210107-DLL-编译","date":"2021-01-07T02:46:13.000Z","updated":"2021-01-07T02:51:03.554Z","comments":true,"path":"2021/01/07/C-20210107-DLL-编译/","link":"","permalink":"http://example.com/2021/01/07/C-20210107-DLL-%E7%BC%96%E8%AF%91/","excerpt":"http://www.jellythink.com/archives/111http://www.cnblogs.com/fangyukuan/archive/2010/06/20/1761464.html","text":"http://www.jellythink.com/archives/111http://www.cnblogs.com/fangyukuan/archive/2010/06/20/1761464.html dll生成 使用关键字声明导出函数 新建 win32 项目，项目名为 dllDemo 在 dllDemo.cpp 中编写代码 导出函数的代码用关键字 __declspec(dllexport) 声明12345extern &quot;C&quot; __declspec(dllexport) void SayHello()&#123; std::cout&lt;&lt;&quot;This function is exported from .dll&quot;&lt;&lt;std::endl; ::MessageBoxW(NULL, L&quot;Hello&quot;, L&quot;fangyukuan&quot;, MB_OK);&#125; 编译 win32 项目 使用.def文件声明导出函数 新建 win32 项目，项目名为 dllDemo 在 dllDemo.cpp 中编写代码 导出函数不再需要关键字声明12345void SayHello()&#123; std::cout&lt;&lt;&quot;This function is exported from .dll&quot;&lt;&lt;std::endl; ::MessageBoxW(NULL, L&quot;2_DLLDemo::Hello&quot;, L&quot;fangyukuan&quot;, MB_OK);&#125; 新建 .def 文件123LIBRARY &quot;dllDemo&quot;EXPORTS SayHello 编译 win32 项目 dll使用 隐式链接隐式链接在项目配置中指定导入函数（即 .dll 文件中的导出函数），代码中只需要声明导入函数就可以调用 建立 win32控制台项目 声明并调用导入函数123456extern &quot;C&quot; __declspec(dllimport) void SayHello(void);int _tmain(int argc, _TCHAR* argv[])&#123; SayHello(); return 0;&#125; 在工程配置中指定 .lib文件的路径方法1. 属性-&gt;链接器-&gt;常规-&gt;附加库目录：指向.lib文件的路径; 属性-&gt;链接器-&gt;输入-&gt;附加依赖项：.lib文件的文件名方法2. 在代码中指定导入库文件.lib1#pragma comment(lib, &quot;dllDemo.lib&quot;) 编译运行 显式链接显式链接在代码中加载.dll文件显示链接的时候不需要提供 .lib文件；但是，如果 .dll中存在依赖项，并且依赖项没有被导入，.dll会加载失败 1234567891011121314typedef void (*SayHello)();int _tmain(int argc, _TCHAR* argv[])&#123; HMODULE hDll = LoadLibrary(&quot;dllDemo.dll&quot;); if (hDll != NULL) &#123; SayHello sayhello_proc = (SayHello)GetProcAddress(hDll, &quot;SayHello&quot;); if (sayhello_proc != NULL) &#123; sayhello_proc(); &#125; FreeLibrary(hDll); &#125; &#125; 查看 dll 导出函数：dumpbin -exports &lt;*.dll&gt;","categories":[{"name":"C++","slug":"C","permalink":"http://example.com/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"动态链接库","slug":"动态链接库","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93/"}]},{"title":"C3D User Guide","slug":"Caffe-20210107-C3D-User-Guide","date":"2021-01-07T02:42:22.000Z","updated":"2021-01-07T02:43:58.043Z","comments":true,"path":"2021/01/07/Caffe-20210107-C3D-User-Guide/","link":"","permalink":"http://example.com/2021/01/07/Caffe-20210107-C3D-User-Guide/","excerpt":"C3D User GuideDu Tran (Last modified Mar 20, 2017) C3D-v1.1 is released with new models (Mar 01, 2017). ● No documentation for v1.1 yet, but some examples for feature extraction, training, and fine-tuning are provided.","text":"C3D User GuideDu Tran (Last modified Mar 20, 2017) C3D-v1.1 is released with new models (Mar 01, 2017). ● No documentation for v1.1 yet, but some examples for feature extraction, training, and fine-tuning are provided. The below guide was written for C3D-v1.0I. C3D Feature ExtractionIf you have installed C3D successfully (same as install caffe and its dependences), following the following steps: Download pre-trained model and save it to YOUR_C3D_HOME/examples/c3d_feature_extraction Change directory to YOUR_C3D_HOME/examples/c3d_feature_extraction Run: sh c3d_sport1m_feature_extraction_frm.sh or sh c3d_sport1m_feature_extraction_video.sh If you can run the examples successfully, then you should find the extracted features in the output folders. If you run to “out of memory” error, then you should consider to reduce the batch-size (see section I.B) If you can run feature extraction with frames successfully, but fail with the video inputs. The cause may come from video codecs. Make sure you had compiled your OpenCV and Ffmpeg with shared-flags are on. make sure that ‘shuffle: false’ in your data layer when you use C3D as a feature extractor. This help us to keep the correspondences between the input clips and the output features. I.A Extract C3D features for your own videos or framesPrepare your input files C3D allows you to use video inputs as sequences of frames or video files. In the case of video files (.mp4, .avi, .mov), make sure that your machine has codecs, opencv, and ffmpeg installed properly. In the case of using frames, C3D assumes that each video is a folder with frames which are numbered starting from 1 to N (number of frames). The frame names are formatted as “video_folder/%06d.jpg”. Note that: frame numbers starting from 1 (e.g. 1..N) for using frame as inputs, and starting from 0 (e.g. [0..N-1]) for using video as inputs. Prepare your setting files There are two setting files you need to prepare: input-list and output prefix. In the provided example, they are: input_list_frm.txt, input_list_video.txt, and output_list_prefix.txt in YOUR_C3D_HOME/examples/c3d_feature_extraction/prototxt The input list file is a text file where each line contain information for a clip that you are inputting into C3D for extracting features. Each line has the following format: where is only used for training, testing, or fine-tuning, but NOT for extracting features, thus can be ignored (in the provided example, they are filled with 0s). For , we have two cases. For the setting with video file inputs, is the full path and filename of the video (e.g. input/avi/v_ApplyEyeMakeup_g01_c01.avi). For the setting with frame inputs, is the full path to the folder containing frames of the video (e.g. input/frm/v_ApplyEyeMakeup_g01_c01/). Finally, the is used to specify the starting frame of the clip. We note that C3D extract feature of 16-frame-long clips. For example, if starting frame is 1, then you are extracting features for the clip (from the video specified by ) from frame 1 to 16. If starting frame is 17, then the clip of interest is from frame 17 to 32. Note that in the provided examples, we have sampled clips from videos with step size (or stride) of 16 frames. You can use different sampling step-size: e.g. as dense as every 1 frame or sparser e.g. every 32 frames. The output prefix file is used to specify the locations for extracting features to be saved. Each line is formatted as Each line in the prefix file is corresponded to a line in the input list file (in the same order, e.g. line 1 in prefix file is the output prefix for the clip of line 1 in the list file). C3D will save features are output_prefix.[feature_name] (e.g. prefix.fc6). It is recommend that for each video, you should create an output folder and the prefix lines are formatted as sprintf(“output_folder/%06d”, starting_frame). That means each clip has its starting frame as identifier and file extensions are used for different features. Remember to create output folders, as C3D does not create them. Extract C3D features Assume that you have prepared your setting files, then you need to modify the prototxt file to point to the input list file. In the prototxt file, looks for line: source: “prototxt/input_list_frm.txt” Also remember set the use_image: true if you use images as inputs or false if use videos as inputs. Use extract_image_features tool to extract features. The arguments used by this tools is follow: extract_image_features.bin … In which: : is prototxt file (provided in example) which points to your input list file. : is the C3D pre-trained model that you downloaded. : GPU ID you would like to run (starting from 0), if this is set to -1, then it will use CPU. : your mini batch size. Default is 50, but you can modify this number, depend on your GPU memory. : Number of mini-batches you want to extract features. For examples, if you have 100 clips to extract features and you are using mini-batch size of 50, then this parameter should be set to 2. However, if you have 101 clips to be extracted features, then this number should be set to 3. : Your output prefix file. : You can list as many feature names as possible as long as they are in the names of the output blobs of the network (see prototxt file for all layers, but they look like fc6-1, fc7-1, fc8-1, pool5, conv5b, prob,…). You can find the following command line provided in the example as below: GLOG_logtosterr=1 ../../build/tools/extract_image_features.bin prototxt/c3d_sport1m_feature_extractor_frm.prototxt conv3d_deepnetA_sport1m_iter_1900000 0 50 1 prototxt/output_list_prefix.txt fc7-1 fc6-1 prob I.B Extract C3D features with smaller or larger batch-size In case you have more or less memory, you can adjust the mini-batch size (larger or smaller than 50). To do that, you need to change this parameter in the prototxt file of the network (find line e.g. batch_size: 50). And you also need to input the newly-adjust parameters of and in the command line. After extracted C3D features, you can use the provided MATLAB script (read_binary_blob.m ) to read the features for further analysis. II. Train 3D ConvNetA. Compute volume mean from listThis tool allows you to compute volume mean for you own dataset which can be useful for both training from scratch or fine-tuning C3D on your own dataset. Usage:GLOG_logtostderr=1 compute_volume_mean_from_list input_chunk_list length height width sampling_rate output_file [dropping rate] Arguments:input_chunk_list: the same as the list file used in feature extractionlength: the length of the clip used in training (e.g. 16)height, width: size of frame e.g. 128, 171sampling_rate: this is used to adjust the frame rate in you clip (e.g. clip length=16, sampling=1, then your clip is a 16-consecutive frame video chunk. Or if clip length=16, while sampling rate=2, then you clip is 32-frame long clips, but you sample 1 of every 2 frames)output_file: the output mean file.dropping_rate: In case you dataset is too large (e.g. 1M), you may want to compute the mean from a subset of your clips. Setting this to n, meaning the dropping rate is 1:n, choose 1 sample among every n clips for computing mean. If you prefer to use mean_value, instead of volume_mean file, then you can set this mean_value field in your data layer. This is equivalent to the volume mean with all values are set to mean_value. B. Train your own network from scratchAssume you have your input_data_list, your train/test prototxt and your solver prototxt, you can use train_net to train the network. C. An example of training from scratch on UCF101 Change directory to YOUR_C3D_HOME/examples/c3d_train_ucf101/ run sh create_volume_mean.sh to compute the volume mean file run sh train_ucf101.sh to train, expect a couple days to finish run sh test_ucf101.sh to test, expect about 15’ to complete and you should have ~45% accuracy (this is clip accuracy) III. Fine-tune C3DAssume you have download the C3D pre-trained model. You can try the fine-tuning example, by: Change directory to YOUR_C3D_HOME/examples/c3d_finetuning Run: sh ucf101_finetuning.sh When fine-tuning is done, you can test your fine-tuned model by running: sh ucf101_testing.sh [Added 05/10/2016] In case you don’t have time to fine-tune C3D on UCF101 yourself, here we provide the C3D model fine-tuned on UCF101: https://www.dropbox.com/s/mkc9q7g4wnqnmcv/c3d_ucf101_finetune_whole_iter_20000 Simply download this model to YOUR_C3D_HOME/examples/c3d_finetuning and run sh ucf101_testing.sh (assume that you have made sure your test_01.lst file points to your UCF101 frames). This will give an accuracy of 80.19% (clip accuracy). NOTE: this model is fine-tuned on UCF101 “train split 1”, thus it is only valid to test on “test split 1”. FAQs● Do we have MATLAB or Python wrappers for extracting C3D features? Unfortunately, we don’t have them yet.● Can I use C3D on a CPU?This version of C3D is built on an old caffe branch, so there is no ‘CPU_ONLY’ mode inMakefile. But you can do that by the following: Compile C3D as normal (it requires CUDA driver to compile, but if you don’t have GPU, you still can run on a CPU). To train using CPU, you can modify solver file to solver_mode: CPU (see here https://github.com/facebook/C3D/blob/master/examples/c3d_train_ucf101/conv3d_ucf101_solver.prototxt#L19) To test using CPU, in the command line use CPU instead of GPU (https://github.com/facebook/C3D/blob/master/examples/c3d_train_ucf101/test_ucf101.sh) and no GPU_ID is needed. To extract features with CPU, use GPU_ID = -1, instead of 0 as in the example here (https://github.com/facebook/C3D/blob/master/examples/c3d_feature_extraction/c3d_sport1m_feature_extraction_frm.sh) ● Email me your questions? (trandu -at- fb.com) or post on github. This is more preferred because I sometime miss some emails. Github keeps tracks much better.","categories":[{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/categories/Caffe/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/tags/Caffe/"},{"name":"Action Recgnition","slug":"Action-Recgnition","permalink":"http://example.com/tags/Action-Recgnition/"}]},{"title":"查看 caffemodel 内容","slug":"Caffe-20210107-查看-caffemodel-内容","date":"2021-01-07T02:37:29.000Z","updated":"2021-01-07T02:39:26.057Z","comments":true,"path":"2021/01/07/Caffe-20210107-查看-caffemodel-内容/","link":"","permalink":"http://example.com/2021/01/07/Caffe-20210107-%E6%9F%A5%E7%9C%8B-caffemodel-%E5%86%85%E5%AE%B9/","excerpt":"","text":"使用 Python 接口，查看 .caffemodel 文件内容训练好一个网络后，使用 Python 接口进行部署的时候，需要两个文件 trian.prototxt 和 xx.caffemodel。初始化网络的时候，首先会通过 train.prototxt 生成网络的拓扑结构，我们可以在 netscope 预览网络拓扑结构；网络搭建好了之后，Caffe 会从预训练模型中载入参数，对网络参数进行初始化，这个时候有三种情况： prototxt 中出现的层，caffemodel 中有对应的层：这是最常见的情况，网络参数会使用 caffemodel 中存储的参数进行初始化。如果两者的结构参数不一样，就会抛出异常（shape mismatch）。比较典型的有：conv层/pool层 的 kernel-size，channel 不一样；fc层 的输入/输出 维度不一样。 prototxt 中出现的层，caffemodel 中没有对应的层：这种情况下，网络参数会使用 prototxt 中指定的 filler 方法进行初始化，等同于这一层 train from scratch；使用 ImageNet 预训练模型训练其他图片分类模型，修改最后一个 fc层 的名字，就是这种场景。 caffemodel 中出现的层，prototxt 中没有对应的层：这种情况下，caffemodel 中的参数就不会使用，在 Caffe 中会打印出 Ignore layer ** 这样的 log 信息。 举例来说：使用 下面的 pretrain.prototxt 训练出来 caffemodel，然后初始化 train.prototxt 的参数时：layer-A 会被 ignore；layer-B 会使用 pretrain-model 的参数；layer-C 会使用 高斯分布初始化 123456789# pretrain.prototxtlayer&#123; name: layer-A # param in this layer will be ignored when training train.prototxt ...&#125;layer&#123; name: layer-B # param in this layer will be used to initalize training prototxt ...&#125; 123456789101112# train.prototxtlayer&#123; name: layer-B ...&#125;layer&#123; name: layer-C # this layer does not have cordnate layer in pretrained model, will be initialize by weight-filler weiget-filler:&#123; type: guassian std: 0.1 &#125;&#125; 下面言归正传，prototxt 是文本文件，很容易看到 待训练模型 的网络结构，那么，如何看到预训练模型的网络结构呢？直接上代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import caffe.proto.caffe_pb2 as caffe_pb2model_f = &#x27;pretrain.caffemodel&#x27;model = caffe_pb2.NetParameter()with open(model_f, &#x27;rb&#x27;) as f: string = f.read() model.ParseFromString(string) # this instruction will cost much timelayers = model.layerlayers[0]&#x27;&#x27;&#x27;name: &quot;input-data&quot;type: &quot;Python&quot;top: &quot;data&quot;top: &quot;im_info&quot;top: &quot;gt_boxes&quot;phase: TRAINpython_param &#123;module: &quot;roi_data_layer.layer&quot;layer: &quot;RoIDataLayer&quot;param_str: &quot;\\&#x27;num_classes\\&#x27;: 3&quot;&#125;&#x27;&#x27;&#x27;for layer in layers: print(layer.name)&#x27;&#x27;&#x27;input-datadata_input-data_0_splitim_info_input-data_1_splitgt_boxes_input-data_2_splitconv1_1relu1_1conv1_2relu1_2pool1conv2_1relu2_1conv2_2relu2_2pool2conv3_1relu3_1conv3_2relu3_2conv3_3relu3_3pool3conv4_1relu4_1conv4_2relu4_2conv4_3relu4_3pool4conv5_1relu5_1conv5_2relu5_2conv5_3relu5_3conv5_3_relu5_3_0_splitrpn_conv/3x3rpn_relu/3x3rpn/output_rpn_relu/3x3_0_splitrpn_cls_scorerpn_cls_score_rpn_cls_score_0_splitrpn_bbox_predrpn_bbox_pred_rpn_bbox_pred_0_splitrpn_cls_score_reshaperpn_cls_score_reshape_rpn_cls_score_reshape_0_splitrpn-datarpn_loss_clsrpn_loss_bboxrpn_cls_probrpn_cls_prob_reshapeproposalroi-dataroi_pool5fc6relu6drop6fc7relu7drop7fc7_drop7_0_splitcls_score6bbox_pred6loss_clsloss_bbox&#x27;&#x27;&#x27; 使用 Python 接口，修改 caffemodel 的内容在上面的接口中，我们可以看到，读取 caffemodel 的过程实际上是 load 一串二进制方式存储的字符串，相对地，我们也可以将 字符串序列化成为二进制文件，成为一个 caffemodel，实际上，使用 Caffe 训练模型的时候，就是通过这样一个函数保存模型的，当保存路径不存在或没有权限的时候，这个接口还会抛出异常，我们调用这个函数的 Python 接口，就可以修改预训练 caffemodel 中的参数了。","categories":[],"tags":[]},{"title":"使用 Python 接口制作 lmdb","slug":"Caffe-20210107-使用-Python-接口制作-lmdb","date":"2021-01-07T02:33:48.000Z","updated":"2021-01-07T02:36:10.508Z","comments":true,"path":"2021/01/07/Caffe-20210107-使用-Python-接口制作-lmdb/","link":"","permalink":"http://example.com/2021/01/07/Caffe-20210107-%E4%BD%BF%E7%94%A8-Python-%E6%8E%A5%E5%8F%A3%E5%88%B6%E4%BD%9C-lmdb/","excerpt":"使用 lmdb 的 Python 接口制作数据集Caffe 输入格式：一般来说，使用 Caffe 训练的过程，网络结构的第一层是都用来处理输入数据的，输入数据的形式大多数为图片（NLP 工作中，输入的一般是文本的特征，不过这种情况不太常见）。Caffe 的数据输入层有以下几种：","text":"使用 lmdb 的 Python 接口制作数据集Caffe 输入格式：一般来说，使用 Caffe 训练的过程，网络结构的第一层是都用来处理输入数据的，输入数据的形式大多数为图片（NLP 工作中，输入的一般是文本的特征，不过这种情况不太常见）。Caffe 的数据输入层有以下几种： 12345678910111213141516root@p3d-container:/bigdata/dxx/pseudo-3d-residual-networks/caffe/src/caffe/layers# ll *data_layer.cpp-rw-r--r-- 1 root root 4118 Mar 22 09:25 base_data_layer.cpp-rw-r--r-- 1 root root 4313 Mar 22 09:25 data_layer.cpp-rw-r--r-- 1 root root 4826 Mar 22 09:25 dummy_data_layer.cpp-rw-r--r-- 1 root root 6136 Mar 22 09:25 hdf5_data_layer.cpp-rw-r--r-- 1 root root 6946 Mar 22 09:25 image_data_layer.cpp-rw-r--r-- 1 root root 4414 Mar 22 09:25 memory_data_layer.cpp-rw-r--r-- 1 root root 17544 Mar 22 09:25 window_data_layer.cpp################################################################## 其中，base_data_layer.cpp 定义了数据层的基类；## data_layer.cpp 支持 **lmdb/leveldb** 格式的输入## hdf5_data_layer.cpp 支持 **hdf5** 格式的输入## image_data_layer.cpp 支持 **图片** 直接输入## 其他的三个 layer 我不太了解，感兴趣可以查阅相关资料################################################################ 我们平时最常见的情况就是使用 lmdb/leveldb 作为输入，Caffe 的 mnist example 就是使用的这两个格式。因为这样比直接使用 image 进行输入要更快一些（原因可能是因为 lmdb/leveldb 支持 prefetch 操作，可以节省 IO 时间），其中 lmdb 比 leveldb 更快，体积更小。目前 Caffe 最常见的输入就是 lmdb 格式。那么怎么制作 lmdb 呢？ Caffe 提供了一个 convert_imageset 程序，按照指定的格式整理好 image 和 label，直接调用命令可以将图片转换为 lmdb。具体操作参考官网，这里不详细介绍。 lmdb 的 Python 接口简单的情况下，我们可以直接利用官方提供的工具，将图片数据转换为 lmdb，然后利用 data_layer 的 transform_param 对数据进行简单的预处理或者数据增强操作。但是某些特殊情况下，transform_param 可能无法实现我们想要的预处理。这种情况下，我们可以修改 data_layer 的源码，实现想要的功能，当然这样就比较麻烦了，更为简单的方法就是，我们可以在制作 lmdb 之前先实现想要的预处理操作，然后将图片制作成 lmdb，这就是下面要介绍的内容。同理，如果想用这种方法实现比较复杂的数据增强功能也是可以的，不过，这种情况下，lmdb 占用的空间会相应的增加。我这次的任务是让 Caffe 实现视频输入功能，因此需要将 16 个连续视频帧 stack 起来，制作成一个 clip，后续的网络会对 clip 进行卷积操作。12345678910111213141516171819202122232425262728293031323334353637383940414243444546import lmdbimport numpy as npimport cv2import caffefrom caffe.proto import caffe_pb2#basic setting# 这个设置用来存放lmdb数据的目录lmdb_file = &#x27;lmdb_data&#x27;batch_size = 256# create the lmdb file# map_size指的是数据库的最大容量，根据需求设置lmdb_env = lmdb.open(lmdb_file, map_size=int(1e12))lmdb_txn = lmdb_env.begin(write=True)# 因为caffe中经常采用datum这种数据结构存储数据datum = caffe_pb2.Datum()item_id = -1for x in range(1000): item_id += 1 #prepare the data and label #data = np.ones((3,64,64), np.uint8) * (item_id%128 + 64) #CxHxW array, uint8 or float # pic_path设置成图像目录, 0表示读入灰度图 data = cv2.imread(pic_path, 0) # label 设置图像的label就行 label = item_id%128 + 64 # save in datum datum = caffe.io.array_to_datum(data, label) keystr = &#x27;&#123;:0&gt;8d&#125;&#x27;.format(item_id) lmdb_txn.put( keystr, datum.SerializeToString() ) # write batch if(item_id + 1) % batch_size == 0: lmdb_txn.commit() lmdb_txn = lmdb_env.begin(write=True) print (item_id + 1)# write last batchif (item_id+1) % batch_size != 0: lmdb_txn.commit() print &#x27;last batch&#x27; print (item_id + 1)","categories":[{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/categories/Caffe/"}],"tags":[{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/tags/Caffe/"},{"name":"LMDB","slug":"LMDB","permalink":"http://example.com/tags/LMDB/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}]},{"title":"Py-faster-rcnn","slug":"Caffe-20210107-Py-faster-rcnn","date":"2021-01-07T02:25:06.000Z","updated":"2021-01-07T02:32:12.913Z","comments":true,"path":"2021/01/07/Caffe-20210107-Py-faster-rcnn/","link":"","permalink":"http://example.com/2021/01/07/Caffe-20210107-Py-faster-rcnn/","excerpt":"使用 faster-rcnn 检测游戏中的血条https://github.com/rbgirshick/py-faster-rcnn这一周使用 rbg 大神的 py-faster-rcnn 训练王者荣耀视频中的人物血条，因为之前已经有不少经验，以为可以直接了当得到结果，但是事实却并不那么如意，现在将实验过程简单总结一下。","text":"使用 faster-rcnn 检测游戏中的血条https://github.com/rbgirshick/py-faster-rcnn这一周使用 rbg 大神的 py-faster-rcnn 训练王者荣耀视频中的人物血条，因为之前已经有不少经验，以为可以直接了当得到结果，但是事实却并不那么如意，现在将实验过程简单总结一下。 环境准备 实验环境：实验的环境配置是 Python-2.7, OpenCV-2.4.9, Caffe-1.0, cudnn-v6 安装依赖项：faster-rcnn 使用的是 Caffe 框架，首先需要安装 Caffe 的依赖，另外还需要一些 Python 环境，通过 pip 即可安装。1pip install cpython easydict 编译 faster-rcnn 从 github pull 代码 1git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git 更新 Caffehttp://blog.csdn.net/rzjmpb/article/details/52373012github 中的代码使用的 Caffe 版本很古老，因此不支持 cudnnv5，为了使用 cudnn，可以升级 Caffe 或者安装低版本的 cudnn，这里选择升级 Caffe.12345cd py-faster-rcnn/caffe-fast-rcnn/git remote add caffe https://github.com/BVLC/caffe.git # git config --list git fetch caffegit merge -X theirs caffe/master merge 之后需要注释掉 python_layer.hpp 中的一行代码12$ vi caffe-fast-rcnn/include/caffe/layers/python_layer.hpp +29// self_.attr(&quot;phase&quot;) = static_cast&lt;int&gt;(this-&gt;phase_); // &lt;================ 配置 Makefile.config注意打开 WITH_PYTHON_LAYER=1 编译Caffe 和 faster-rcnn 12345cd py-faster-rcnn/caffe-fater-rcnn/make -jmake pycaffecd py-faster-rcnn/lib/make Caffe 默认使用的是 Python2, OpenCV2; faster-rcnn 使用系统环境变量中的 Python 解释器。一定要注意两者保持一致，特别是由于不可抗原因必须使用 Python3 的场景，确保 Caffe 和 faster-rcnn 的编译环境保持一致（faster-rcnn 中有大量的 Python2 接口不被 Python3兼容，使用 Python3 需要做很多修改，或者在 Github 上寻找其他 fork 分支） 数据准备一般按照 pascal_voc 的格式准备数据，需要准备三个文件夹 my_data/VOC2007/Annotations/Annotation 文件夹下面是 XML 文件，具体格式如下：1234567891011121314151617181920212223242526272829303132&lt;annotation&gt; &lt;folder&gt;VOC2007&lt;/folder&gt; &lt;filename&gt;img.jpg&lt;/filename&gt; &lt;size&gt; &lt;width&gt;1280&lt;/width&gt; &lt;height&gt;720&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;object&gt; &lt;bndbox&gt; &lt;xmin&gt;548&lt;/xmin&gt; &lt;xmax&gt;711&lt;/xmax&gt; &lt;ymin&gt;255&lt;/ymin&gt; &lt;ymax&gt;287&lt;/ymax&gt; &lt;/bndbox&gt; &lt;name&gt;health_blue&lt;/name&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;/object&gt; &lt;object&gt; &lt;bndbox&gt; &lt;xmin&gt;675&lt;/xmin&gt; &lt;xmax&gt;842&lt;/xmax&gt; &lt;ymin&gt;373&lt;/ymin&gt; &lt;ymax&gt;407&lt;/ymax&gt; &lt;/bndbox&gt; &lt;name&gt;health_red&lt;/name&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;/object&gt; &lt;segmented&gt;0&lt;/segmented&gt;&lt;/annotation&gt; my_data/VOC2007/ImageSets/ImageSets/Main 文件夹下面是 TXT 文件，总共有 4 个，分别是 trainval.txt, train.txt, val.txt, test.txt， 文件中每一行代表一张图片； my_data/VOC2007/JPEGImages/JPEGImages 文件夹下面保存的是图片。 这里有几个小细节：（1）图片的文件名后缀是 .jpg；（2）ImageSets 文件夹下面的 txt 是没有后缀的；（3）xml 中矩形框字段的 tag 是 bndbox，不是常用的 bbox；（4）cls 的名字最好全部使用小写英文字母和下划线，因为 faster-rcnn 会将字符全部转换为小写；（5）注意检查 xml 中是否每一张图片都含有 object 字段，没有 object 字段的 xml 进入网络训练不会报错，但是对训练结果会有影响。 制作完数据集后，在 data/ 目录下，建立软连接，指向 my_data/ 文件夹12cd py-faster-rcnn/data/ln -s my_data/ VOCdevkit2007 更改网络设置faster-rcnn 的网络结构由两部分组成，一部分在 models/ 目录下，这里的主要包含 prototxt， 用来初始化 Solver 和 Network；另一部分在 lib/ 目录下，这里包含 rpn, roi_layer, nms 等模块，以及用来 读取和解析 xml 文件的代码。 models/py-faster-rcnn 提供了两种训练方法，一种是 alt-opt 训练，一种是 end2end 训练，关于两种训练的差别可以参考 Github 上的 readme，另外我们还可以选择使用三种不同的 Backbone Network： {VGG16, VGG-M, ZF}，这里我们选择性能最好的 VGG16，训练方式为 end2end 训练。对应目录为 models/pascal_voc/VGG16/faster_rcnn_end2end/— 修改 train.prototxt1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283$ vi models/pascal_voc/VGG16/faster_rcnn_end2end/train.prototxt# input_data 层（第 11 行）， 修改 num_classes 为 cls+1layer &#123; name: &#x27;input-data&#x27; type: &#x27;Python&#x27; top: &#x27;data&#x27; top: &#x27;im_info&#x27; top: &#x27;gt_boxes&#x27; python_param &#123; module: &#x27;roi_data_layer.layer&#x27; layer: &#x27;RoIDataLayer&#x27; # param_str: &quot;&#x27;num_classes&#x27;: 21&quot; param_str: &quot;&#x27;num_classes&#x27;: 3&quot; # &lt;================ &#125;&#125;# roi_data 层（第 530 行）， 修改 num_classes 为 cls+1layer &#123; name: &#x27;roi-data&#x27; type: &#x27;Python&#x27; bottom: &#x27;rpn_rois&#x27; bottom: &#x27;gt_boxes&#x27; top: &#x27;rois&#x27; top: &#x27;labels&#x27; top: &#x27;bbox_targets&#x27; top: &#x27;bbox_inside_weights&#x27; top: &#x27;bbox_outside_weights&#x27; python_param &#123; module: &#x27;rpn.proposal_target_layer&#x27; layer: &#x27;ProposalTargetLayer&#x27; # param_str: &quot;&#x27;num_classes&#x27;: 21&quot; param_str: &quot;&#x27;num_classes&#x27;: 3&quot; # &lt;================ &#125;&#125;# cls_score 层（第 620 行）， 修改 num_output 为 cls+1layer &#123; name: &quot;cls_score&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;cls_score&quot; param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; inner_product_param &#123; # num_output: 21 num_output: 3 # &lt;================ weight_filler &#123; type: &quot;gaussian&quot; std: 0.01 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125;&#125;# box_pred 层（第 643 行）， 修改 num_output 为 4*(cls+1)layer &#123; name: &quot;bbox_pred&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;bbox_pred&quot; param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; inner_product_param &#123; # num_output: 84 num_output: 12 # &lt;================ weight_filler &#123; type: &quot;gaussian&quot; std: 0.001 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125;&#125; — 修改 test.prototxt123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ vi models/pascal_voc/VGG16/faster_rcnn_end2end/test.prototxt# cls_score 层（第 567 行）， 修改 num_output 为 cls+1layer &#123; name: &quot;cls_score&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;cls_score&quot; param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; inner_product_param &#123; # num_output: 21 num_output: 3 # &lt;================ weight_filler &#123; type: &quot;gaussian&quot; std: 0.01 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125;&#125;# box_pred 层（第 592 行）， 修改 num_output 为 4*(cls+1)layer &#123; name: &quot;bbox_pred&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;bbox_pred&quot; param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; inner_product_param &#123; # num_output: 84 num_output: 12 # &lt;================ weight_filler &#123; type: &quot;gaussian&quot; std: 0.001 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125;&#125; — 修改 solver.prototxt修改 solver 中的参数，使之适合训练自己的数据；faster-rcnn 的另外大多数网络参数在 lib/fast-rcnn/config.py 中，附有详细的注释，可以参考注释进行修改。 lib/ — 修改 pascal_voc.py 由于自己训练数据的类别数与 pascal_voc 不同，因此需要稍微修改一下 12345678910vi lib/datasets/pascal_voc.py# 修改 self._classes 为自己的类别：# self._classes = (&#x27;__background__&#x27;, # always index 0# &#x27;aeroplane&#x27;, &#x27;bicycle&#x27;, &#x27;bird&#x27;, &#x27;boat&#x27;,# &#x27;bottle&#x27;, &#x27;bus&#x27;, &#x27;car&#x27;, &#x27;cat&#x27;, &#x27;chair&#x27;,# &#x27;cow&#x27;, &#x27;diningtable&#x27;, &#x27;dog&#x27;, &#x27;horse&#x27;,# &#x27;motorbike&#x27;, &#x27;person&#x27;, &#x27;pottedplant&#x27;,# &#x27;sheep&#x27;, &#x27;sofa&#x27;, &#x27;train&#x27;, &#x27;tvmonitor&#x27;)self._classes = (&#x27;__background__&#x27;, # always index 0 &#x27;health_blue&#x27;, &#x27;health_red&#x27;) 开始训练 建立测试结果的文件夹： 1mkdir -p data/VOCdevkit2007/results/VOC2007/Main/ 训练完成后，会进行一次测试，如果不手动新建文件夹，测试的时候会报错说找不到这个目录。 直接使用 faster-rcnn 的 脚本进行训练： 1./experiments/scripts/faster_rcnn_end2end.sh 0 VGG16 pascal_voc 最大迭代次数在 ./experiments/scripts/faster_rcnn_end2end.sh 中进行设置； 训练的 log 存储在 experiments/logs/ 中， 也可以自己重定向出来。 测试模型 faster-rcnn 提供的训练脚本，在使用训练集验证集训练完成后，会在测试集上进行测试，输出模型的 AP 和 AR； 如果想使用单张图片进行测试，并且看到可视化效果，可以参考 tools/demo.py， demo.py 使用 matplotlib 进行可视化，对脚本稍加修改，可以将测试结果保存成图片存储下来。 修改脚本代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#!/usr/bin/env python# --------------------------------------------------------# Faster R-CNN# Copyright (c) 2015 Microsoft# Licensed under The MIT License [see LICENSE for details]# Written by Ross Girshick# --------------------------------------------------------&quot;&quot;&quot;Demo script showing detections in sample images.See README.md for installation instructions before running.&quot;&quot;&quot;import _init_pathsfrom fast_rcnn.config import cfgfrom fast_rcnn.test import im_detectfrom fast_rcnn.nms_wrapper import nmsfrom utils.timer import Timerimport matplotlib.pyplot as pltimport numpy as npimport scipy.io as sioimport caffe, os, sys, cv2import argparseimport globCLASSES=(&#x27;__background__&#x27;,&#x27;health_blue&#x27;,&#x27;health_red&#x27;)def vis_detections(im, class_name, dets, thresh=0.5): inds = np.where(dets[:, -1] &gt;= thresh)[0] boxes, scores = ([], []) for i in inds: bbox = dets[i, :4] score = dets[i, -1] boxes.append( dets[i, :4] ) scores.append(score) print(bbox, score) return boxes, scoresdef demo(net, im, filename, out_dir): timer = Timer() timer.tic() scores, boxes = im_detect(net, im) timer.toc() print(&#x27;-----------------------------------------------------&#x27;) print ((&#x27;Detection &#123;&#125; took &#123;:.3f&#125;s for &#123;:d&#125; object proposals&#x27;).format(filename, timer.total_time, boxes.shape[0])) CONF_THRESH = 0.8 NMS_THRESH = 0.1 for cls_ind, cls in enumerate(CLASSES[1:]): cls_ind += 1 # because we skipped background cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)] cls_scores = scores[:, cls_ind] dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32) keep = nms(dets, NMS_THRESH) dets = dets[keep, :] _boxes, _scores = vis_detections(im, cls, dets, thresh=CONF_THRESH) for box, score in zip( _boxes, _scores): display_str = cls + &#x27; &#x27; + str(score) cv2.rectangle(im, tuple(box[:2]), tuple(box[2:]), (255,0,0), 2) cv2.putText(im, display_str, tuple(box[:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2) cv2.imwrite( os.path.join( out_dir, filename), im)def parse_args(): &quot;&quot;&quot;Parse input arguments.&quot;&quot;&quot; parser = argparse.ArgumentParser(description=&#x27;Faster R-CNN demo&#x27;) parser.add_argument(&#x27;model_name&#x27;, help=&#x27;model name&#x27;, type=str) parser.add_argument(&#x27;--gpu&#x27;, dest=&#x27;gpu_id&#x27;, help=&#x27;GPU device id to use [0]&#x27;, default=1, type=int) parser.add_argument(&#x27;--cpu&#x27;, dest=&#x27;cpu_mode&#x27;, help=&#x27;Use CPU mode (overrides --gpu)&#x27;, action=&#x27;store_true&#x27;) parser.add_argument(&#x27;--data_dir&#x27;, dest=&#x27;data_dir&#x27;, help=&#x27;data path&#x27;,default=&#x27;/bigdata/dxx/py-faster-rcnn/demo/&#x27;, type=str) parser.add_argument(&#x27;--video_name&#x27;, dest=&#x27;video_name&#x27;, help=&#x27;video name&#x27;,default=&#x27;test.mp4&#x27;, type=str) parser.add_argument(&#x27;--image_name&#x27;, dest=&#x27;image_name&#x27;, help=&#x27;image name&#x27;,default=&#x27;test.mp4&#x27;, type=str) parser.add_argument(&#x27;--model_dir&#x27;, dest=&#x27;model_dir&#x27;, help=&#x27;model path&#x27;,default=&#x27;/bigdata/dxx/py-faster-rcnn/models/&#x27;, type=str) args = parser.parse_args() return argsif __name__ == &#x27;__main__&#x27;: cfg.TEST.HAS_RPN = True # Use RPN for proposals args = parse_args() out_dir = os.path.join(args.data_dir, args.model_name) if not os.path.exists(out_dir): os.mkdir(out_dir) os.system(&#x27;&#123;&#125; &#123;&#125; &#123;&#125;&#x27;.format(&#x27;chmod&#x27;, &#x27;777&#x27;, out_dir)) print(&#x27;output images to &#123;&#125;&#x27;.format(out_dir)) prototxt = &#x27;models/pascal_voc/VGG16/faster_rcnn_end2end/test.prototxt&#x27; caffemodel = os.path.join(args.model_dir, args.model_name, &#x27;vgg16_faster_rcnn_final.caffemodel&#x27;) if args.cpu_mode: caffe.set_mode_cpu() else: caffe.set_mode_gpu() caffe.set_device(args.gpu_id) cfg.GPU_ID = args.gpu_id net = caffe.Net(prototxt, caffemodel, caffe.TEST) print (&#x27;\\n\\nLoaded network &#123;:s&#125;&#x27;.format(caffemodel)) image_f = os.path.join(args.data_dir, &#x27;image&#x27;, args.image_name, &#x27;*.jpg&#x27;) for im_f in sorted(glob.glob(image_f)): im = cv2.imread(im_f) demo(net, im, os.path.basename(im_f), out_dir) &#x27;&#x27;&#x27; video_f = os.path.join(args.data_dir, &#x27;video&#x27;, args.video_name) video_cap = cv2.VideoCapture(video_f) frame_cnt = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT)) error_frame = max(20, frame_cnt / 50) for i, frame_index in enumerate(range(0, frame_cnt, 10)): video_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index) status, frame = video_cap.read() if not status: error_frame -= 1 if error_frame &gt; 0: continue video_cap.release() raise Exception(&#x27;Bad video file&#x27;) demo(net, frame, frame_index+&#x27;.jpg&#x27;, out_dir) &#x27;&#x27;&#x27; 测试图片的路径格式如下： 1234567891011121314+ /bigdata/dxx/py-faster-rcnn/ + models/ + &lt;model_name&gt;/ - vgg16_faster_rcnn_final.caffemodel + demo/ + video/ - test.mp4 + image/ + test.mp4/ - 001.jpg - 002.jpg + &lt;model_name&gt;/ # detection results - 001.jpg - 002.jpg python tools/demo.py &lt;model_name&gt; 这次使用了迭代 5000 次的模型进行测试，在两个视频上进行了简单的测试，在测试的 100 张图片上，完全没有出现 漏检或错检，准确率达到 1.0 错误列表 注意事项：1. 编译过程中，每次更新环境，最好重新编译 Caffe、pycaffe、faster-rcnn/lib2. 运行过程中，每次重新开始训练/测试，最好删除掉 data/cache/ 和 data/VOCdevkit2007/annotationcache/ AttributeError: can&#39;t set attribute 这是由于 git merge 之后没有修改 caffe-fast-rcnn/include/caffe/layers/python_layer.hpp 导致的。 AttributeError: &#39;module&#39; object has no attribute &#39;text_format&#39; 这个和 protobuf 的版本有关 12$ vi lib/fast-rcnn/train.py+ import google.protobuf.text_format TypeError: slice indices must be integers or None or have an __index__ method 123456789101112131415# $ vi lib/rpn/proposal_target_layer.pydef _get_bbox_regression_labels(bbox_target_data, num_classes): clss = bbox_target_data[:, 0] bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32) bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32) inds = np.where(clss &gt; 0)[0] for ind in inds: ind = int(ind) &lt;================= cls = clss[ind] start = int(4 * cls) &lt;================= end = int(start + 4) &lt;================= bbox_targets[ind, start:end] = bbox_target_data[ind, 1:] bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS return bbox_targets, bbox_inside_weights TypeError: &#39;numpy.float64&#39; object cannot be interpreted as an index https://github.com/rbgirshick/py-faster-rcnn/issues/481 numpy1.12 版本以后，不再支持 1.0，2.0 这样的浮点数作为索引，因此需要安装低版本的numpy 1pip install -U numpy==1.11.0 另外还可以找到所有出问题的地方，使用 astype() 函数进行类型转换 1234567891011$ vi lib/roi_data_layer/minibatch.py +26fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION *rois_per_image).astype(np.int)$ vi lib/datasets/ds_utils.py +12hashes = np.round(boxes * scale).dot(v).astype(np.int)$ vi lib/fast_rcnn/test.py line +129hashes = np.round(blobs[&#x27;rois&#x27;] * cfg.DEDUP_BOXES).dot(v).astype(np.int)$ vi lib/rpn/proposal_target_layer.py +60fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image).astype(np.int)","categories":[{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/categories/Caffe/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/tags/Caffe/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://example.com/tags/Object-Detection/"},{"name":"Faster RCNN","slug":"Faster-RCNN","permalink":"http://example.com/tags/Faster-RCNN/"}]},{"title":"openpose_train","slug":"Caffe-20210107-openpose-train","date":"2021-01-07T02:22:03.000Z","updated":"2021-01-07T02:23:33.837Z","comments":true,"path":"2021/01/07/Caffe-20210107-openpose-train/","link":"","permalink":"http://example.com/2021/01/07/Caffe-20210107-openpose-train/","excerpt":"https://github.com/CMU-Perceptual-Computing-Lab/openposehttps://github.com/CMU-Perceptual-Computing-Lab/openpose_train","text":"https://github.com/CMU-Perceptual-Computing-Lab/openposehttps://github.com/CMU-Perceptual-Computing-Lab/openpose_train 1. 拉取代码1git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose_train.git2. 下载数据集2.1. cocoapi12345678910mkdir -p openpose_train/dataset/mkdir -p openpose_train/dataset/COCO/cd openpose_train/dataset/COCO/git clone --recurse https://github.com/gineshidalgo99/cocoapi.git# 安装 mingw, matlab PCT# 编译 gasonMex, maskApiMexcd cocoapi/MatlabAPI/mex(&#x27;CXXFLAGS=$CXXFLAGS -std=c++11 -Wall&#x27;,&#x27;-largeArrayDims&#x27;,&#x27;private/gasonMex.cpp&#x27;,&#x27;../common/gason.cpp&#x27;,&#x27;-I../common/&#x27;,&#x27;-outdir&#x27;,&#x27;private&#x27;);mex(&#x27;COMPFLAGS=\\$CFLAGS -Wall -std=c++11&#x27;,&#x27;-largeArrayDims&#x27;,&#x27;private/maskApiMex.c&#x27;,&#x27;../common/maskApi.c&#x27;,&#x27;-I../common/&#x27;,&#x27;-outdir&#x27;,&#x27;private&#x27;); 2.2. coco数据集12345678910# annotations_trainval2017, image_info_test2017 --&gt; dataset/cocoapi/annotations/# train2017, val2017, test2017 --&gt; dataset/cocoapi/images/http://images.cocodataset.org/zips/train2017.ziphttp://images.cocodataset.org/zips/val2017.ziphttp://images.cocodataset.org/zips/test2017.ziphttp://images.cocodataset.org/annotations/annotations_trainval2017.ziphttp://images.cocodataset.org/annotations/image_info_test2017.ziphttp://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip 2.3. 生成LMDB1234567# 生成 body keypoints LMDB# 依次执行 a1_coco_jsonToNegativesJson, a2_coco_jsonToMat, a3_coco_matToMasks, a4_coco_matToRefinedJsonpython c_generateLmdbs.py# 生成 foot keypoints LMDB# 依次执行 a2_coco_jsonToMat, a4_coco_matToRefinedJsonpython c_generateLmdbs.py3. 模型训练3.1. 拉取 OpenPose Caffe 和 预训练模型1234567cd /openpose_train/git clone --recurse github.com/CMU-Perceptual-Computing-Lab/openpose_caffe_train.gitcd /openpose_train/dataset/mkdir vggwget -c http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodelwget -c https://gist.githubusercontent.com/ksimonyan/3785162f95cd2d5fee77/raw/bb2b4fe0a9bb0669211cf3d0bc949dfdda173e9e/VGG_ILSVRC_19_layers_deploy.prototxt 3.2. 生成 prototxt12cd /openpose_train/trainingpython d_setLayers.py 3.3. 训练12cd /openpose_train/training_result/pose/./train_pose.sh 04.测试4.1. 速度测试12345# Test using LMDB:/openpose_train/openpose_caffe_train/build/tools/caffe time -gpu 0 -model pose_training.prototxt# Test using input layer: 368*368/openpose_train/openpose_caffe_train/build/tools/caffe time -gpu 0 -model pose_deploy.prototxt -phase TEST | batchsize | 1 | 8 |16 | **deploy**| |--- |--- |--- |--- |--- | |time(ms) |19 |103 |199 |17.3 | 4.2. Evaluate1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768cd /openpose./scripts/tests/pose_accuracy_coco_val.sh # https://github.com/gineshidalgo99/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb --&gt; evaluate.pypython scripts/tests/evaluate.pyBody25 baseline: 5000samples/119s = 42FPS Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.523 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.763 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.568 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.466 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.604 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.576 Average Recall (AR) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.790 Average Recall (AR) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.613 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.483 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.709COCO baseline: Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.490 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.742 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.520 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.426 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.588 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.544 Average Recall (AR) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.766 Average Recall (AR) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.571 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.442 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.688body23: 5000samples/120s = 41FPS Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.433 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.692 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.450 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.360 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.535 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.484 Average Recall (AR) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.719 Average Recall (AR) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.502 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.378 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.632 90 degree Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.324 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.602 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.304 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.259 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.416 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.376 Average Recall (AR) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.633 Average Recall (AR) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.366 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.283 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.505 60 degree Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.345 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.630 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.327 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.290 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.425 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.399 Average Recall (AR) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.662 Average Recall (AR) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.392 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.313 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.520","categories":[{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/categories/Caffe/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/tags/Caffe/"},{"name":"OpenPose","slug":"OpenPose","permalink":"http://example.com/tags/OpenPose/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"}]},{"title":"tensorflow object-detecct-api","slug":"TensorFlow-20210107-tensorflow-object-detecct-api","date":"2021-01-07T02:19:14.000Z","updated":"2021-01-18T04:44:04.738Z","comments":true,"path":"2021/01/07/TensorFlow-20210107-tensorflow-object-detecct-api/","link":"","permalink":"http://example.com/2021/01/07/TensorFlow-20210107-tensorflow-object-detecct-api/","excerpt":"0. 安装1234567891011apt-get install protobuf-compilerpip install pillowpip install lxmlpip install jupyterpip install matplotlib# From tensorflow/models/research/protoc object_detection/protos/*.proto --python_out=.# From tensorflow/models/research/export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim","text":"0. 安装1234567891011apt-get install protobuf-compilerpip install pillowpip install lxmlpip install jupyterpip install matplotlib# From tensorflow/models/research/protoc object_detection/protos/*.proto --python_out=.# From tensorflow/models/research/export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim 1. 训练命令https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md12345# From the tensorflow/models/research/ directorypython object_detection/train.py \\ --logtostderr \\ --pipeline_config_path=$&#123;PATH_TO_YOUR_PIPELINE_CONFIG&#125; \\ --train_dir=$&#123;PATH_TO_TRAIN_DIR&#125;[pipeline_config.pbtxt]123456789101112131415161718192021222324&#96;&#96;&#96;PATH_TO_TRAIN_DIR&#96;&#96;&#96;：训练模型保存位置---## 2. PIPELINE_CONFIG&lt;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;models&#x2F;blob&#x2F;master&#x2F;research&#x2F;object_detection&#x2F;g3doc&#x2F;configuring_jobs.md&gt;&#96;&#96;&#96;protomodel &#123;(... Add model config here...)&#125;train_config : &#123;(... Add train_config here...)&#125;train_input_reader: &#123;(... Add train_input configuration here...)&#125;eval_config: &#123;&#125;eval_input_reader: &#123;(... Add eval_input configuration here...)&#125;pipeline_config 文件分为以下五个部分 model: object_detection/samples/model_configs train_config: 12345678910111213141516171819202122232425262728293031batch_size: 1optimizer &#123;momentum_optimizer: &#123;learning_rate: &#123; manual_step_learning_rate &#123; initial_learning_rate: 0.0002 schedule &#123; step: 0 learning_rate: .0002 &#125; schedule &#123; step: 900000 learning_rate: .00002 &#125; schedule &#123; step: 1200000 learning_rate: .000002 &#125; &#125;&#125;momentum_optimizer_value: 0.9&#125;use_moving_average: false&#125;fine_tune_checkpoint: &quot;/usr/home/username/tmp/model.ckpt-#####&quot;from_detection_checkpoint: truegradient_clipping_by_norm: 10.0data_augmentation_options &#123; random_horizontal_flip &#123; &#125;&#125; train_input_reader:12345tf_record_input_reader &#123;input_path&#123; &quot;/usr/home/username/data/train.record&quot;&#125;label_map_path: &quot;/usr/home/username/data/label_map.pbtxt&quot; 3. TF-recordhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md 1234567891011# From tensorflow/models/research/wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tartar -xvf VOCtrainval_11-May-2012.tarpython object_detection/create_pascal_tf_record.py \\ --label_map_path=object_detection/data/pascal_label_map.pbtxt \\ --data_dir=VOCdevkit --year=VOC2012 --set=train \\ --output_path=pascal_train.recordpython object_detection/create_pascal_tf_record.py \\ --label_map_path=object_detection/data/pascal_label_map.pbtxt \\ --data_dir=VOCdevkit --year=VOC2012 --set=val \\ --output_path=pascal_val.record4. export trained modelhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md123456# From tensorflow/models/research/python object_detection/export_inference_graph.py \\ --input_type image_tensor \\ --pipeline_config_path object_detection/dxx_models/inception_resnet_v2/pipeline_config.pbtxt \\ #$&#123;PIPELINE_CONFIG_PATH&#125; i --trained_checkpoint_prefix object_detection/dxx_models/inception_resnet_v2/train/model.ckpt-381545 \\ #$&#123;MODEL_PATH&#125; --output_directory object_detection/dxx_models/inception_resnet_v2/train/output_inference_graph.pb2","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://example.com/categories/TensorFlow/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://example.com/tags/TensorFlow/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://example.com/tags/Object-Detection/"}]},{"title":"Non-local Neural Network","slug":"Caffe2-20210107-Non-local-Neural-Network","date":"2021-01-07T01:47:01.000Z","updated":"2021-01-07T01:56:32.100Z","comments":true,"path":"2021/01/07/Caffe2-20210107-Non-local-Neural-Network/","link":"","permalink":"http://example.com/2021/01/07/Caffe2-20210107-Non-local-Neural-Network/","excerpt":"Facebook 的 Non-local 开源了，上周简单试验了一下怎么用。记录一下步骤https://github.com/facebookresearch/video-nonlocal-net","text":"Facebook 的 Non-local 开源了，上周简单试验了一下怎么用。记录一下步骤https://github.com/facebookresearch/video-nonlocal-net Caffe2 Installhttps://github.com/facebookresearch/video-nonlocal-net/blob/master/INSTALL.md 依赖库12345678910111213141516171819202122232425git clone --recrusive http://github.com/caffe2/caffe2.gitapt-get updateapt-get install -y --no-install-recommends \\ build-essential \\ cmake \\ git \\ libgoogle-glog-dev \\ libgtest-dev \\ libiomp-dev \\ libleveldb-dev \\ liblmdb-dev \\ libopencv-dev \\ libopenmpi-dev \\ libsnappy-dev \\ libprotobuf-dev \\ openmpi-bin \\ openmpi-doc \\ protobuf-compiler \\ python-dev \\ python-pip pip install \\ future \\ numpy \\ protobuf 中间可能需要安装 setuptools123456apt install wgetwget http://pypi.python.org/packages/source/s/setuptools/setuptools-0.6c11.tar.gztar -xvf setuptools-0.6c11.tar.gzcd setuptools-0.6c11python setup.py buildpython setup.py install Clone Non-local 代码1234cd caffe2git clone --recursive https://github.com/facebookresearch/video-nonlocal-net.gitrm -rf caffe2/video/cp -r video-nonlocal-net/caffe2_customized_ops/video/ caffe2/video/ Make 123# use_ffmpeg ONmake -jpython caffe2_setup.py install # (not sure how it works) 配置环境变量1export PYTHONPATH=&lt;caffe2-dir&gt;/video-nonlocal-net/lib:$PYTHONPATH Data Preparationhttps://github.com/facebookresearch/video-nonlocal-net/blob/master/DATASET.md 我是用 UCF-101 数据集进行 finetune 的，主要参照了官方的链接制作 LMDB 图片准备首先下载解压 UCF-101 数据集到 /data 目录下12root@dxx# ls /data/UCF-101/ ucfTrainTestlist/ 生成 list 文件官方文档里面使用 gen_py_list.py 生成 trainlist.txt这里我们直接用 ucf101 提供的 trainlist01.txt我们还需要做另外两个工作 （1）ucf101 的标签是从 1 开始，需要改成 0； （2）ucf101 中的 testlist01.txt 里面没有标签信息，需要自己加上12345678910111213141516171819202122232425262728293031323334cd &lt;caffe2-dir&gt;/video-nonlocal-net/process_data/cp -r kinetics/ ucf101/cd ucf101/rm -f *.txtcp /data/ucfTrainTestlist/classInd.txt .cp /data/ucfTrainTestlist/trainlist01.txt .cp /data/ucfTrainTestlist/testlist01.txt .python gen_py_list_football.py&#x27;&#x27;&#x27;&gt;&gt;&gt;pythond = &#123;&#125;for line in open(&#x27;classInd.txt&#x27;): ind, label = line.strip().split() d[label] = ind with open(&#x27;trainlist.txt&#x27;, &#x27;w&#x27;) as f: for line in open(&#x27;trainlist01.txt&#x27;): img, ind = line.strip().split() if ind == &#x27;101&#x27;: ind = &#x27;0&#x27; f.write(&#x27;/data/UCF-101/&#x27; + img + &#x27; &#x27; + ind + &#x27;\\n&#x27;)with open(&#x27;vallist.txt&#x27;, &#x27;w&#x27;) as f: for line in open(&#x27;testlist01.txt&#x27;): img = line.strip() label = img.split(&#x27;/&#x27;)[0] ind = d[label] if ind == &#x27;101&#x27;: ind = &#x27;0&#x27; f.write(&#x27;/data/UCF-101/&#x27; + img + &#x27; &#x27; + ind + &#x27;\\n&#x27;)&#x27;&#x27;&#x27; resize 数据集为了加快训练时候的 IO 速度，我们可以提前把数据集 resize 好，官方直接写好了一个脚本12345678910# 修改 downscale_video_joblib.py:21,22YOUR_DATASET_FOLDER/train/ --&gt; /data/UCF-101/YOUR_DATASET_FOLDER/train_256/ --&gt; /data/UCF-101_s256/ # 注意不能少了路径最后的 &#x27;/&#x27;pip install joblib pandasmkdir -p /data/UCF-101_s256/python downscale_video_joblib.py# 修改 downscale_video_joblib.py:20trainlist.txt --&gt; vallist.txtpython downscale_video_joblib.py 生成 LMDB12345678sed -i &#x27;s/UCF-101/UCF-101_s256/g&#x27; trainlist.txtsed -i &#x27;s/UCF-101/UCF-101_s256/g&#x27; vallist.txtpython shuffle_list_rep.py trainlist_shuffle_rep.txtpip install lmdbbash run_createdb.shbash run_createdb_test.shbash run_createdb_test_multicrop.sh Trainhttps://github.com/facebookresearch/video-nonlocal-net/blob/master/README.md 训练过程可以有很多种设置，这里我选择的是 run_i3d_nlnet_400k.sh 这个脚本，具体区别看官方说明。123456789101112131415161718192021cd &lt;caffe2-dir&gt;/video-nonlocal-net/scripts/pip install networkxapt install python-yaml# 设置一下 yaml 文件: ../configs/DBG_kinetics_resnet_8gpu_c2d_nonlocal_400k.yaml# GPU_NUMS# MODEL.NUM_CLASSES# TRAIN.BATCH_SIZE# TEST.BATCH_SIZE# TEST.DATASET_SIZE# 修改 solver 超参数# 修改 run_i3d_nlnet_400k.sh# 6: TRAIN.PARAMS_FILE ../data/pretrained_model/i3d_nonlocal_8x8_IN_pretrain_400k_clean.pkl \\# 14: FILENAME_GT ../process_data/ucf101/vallist.txt \\##chmod 777 run_i3d_nlnet_400k.sh./run_i3d_nlnet_400k.sh 按照上面的步骤基本可以开始训练 ucf101了，这里的预训练模型有两种选择，一种是在 Kinetics 上训练时选择的 ImageNet 预训练模型，脚本里面名字是 ../data/pretrained_model/r50_pretrain_c2_model_iter450450_clean.pkl; 一种是在 Kinetics 上训练好的模型 i3d_nonlocal_8x8_IN_pretrain_400k。我选择的是后者，但是刚开始训练的时候就会认为已经进行到了 400k 次迭代，所以直接结束了训练。我最后在 ../tools/train_net_video.py:131 加上 start_model_iter = 0训练了起来（小伙伴可以尝试其他更好的解决办法）。一旦网络保存过一次 checkpoint，下次训练就会从 checkpoint 里选择最新的模型继续训练，这个时候就可以删掉 start_model_iter = 0 了。 Test视频分类这一块的模型测试，通常都会有两层意思，一个是对测试视频进行上述的数据准备操作，对于 C3D (caffe版) 来说，是生成 lst 文件；对于 Non-local I3D (caffe2版) 来说，是生成 lmdb 文件，这两个文件的作用都是定义测试 clip 在 video/image-squence 中对应的帧号，网络读取数据准备的结果文件后，会组织数据送入后面的卷积层。（当然，数据准备过程只与输入层的实现相关，与网络结构没有关系），这一部分工作与训练几乎没有区别，并且可以方便进行测试，因此很容易跑通。另一个意思就是对视频输入进行分类测试，也就是落地部署，一般的开源项目只为了让人复现实验结果，并不会实现这一部分。视频输入 与 文本/lmdb 输入具有不小的 gap，为了能够直接在视频上测试，需要对网络的输入模块进行改动，下面是 caffe2 版 Non-local I3D 部署的步骤。 Input layer modificationhttps://github.com/clover978/video-nonlocal-net/blob/master/tools/deploy_net_video_local.py Image propocesscaffe2_customized_ops/video/customized_video_input_op.h:200-221 summarize 了 Non-local I3D 的 video_input_op 对视频帧进行的预处理过程。更改后的网络结构需要实现对应的功能。1234567891011121314151617181920212223242526272829303132333435# customized_video_input_op.h:206, scaledef _scale(frame, size=(320, 256)): return cv2.resize(frame, size)# customized_video_input_op.h:208,210, cropdef _crop(frame, crop_size=224, type=&#x27;random&#x27;): # type: &#123;&#x27;random&#x27;, &#x27;center&#x27;&#125; h, w = frame.shape[:2] if type == &#x27;random&#x27;: x = random.randint(crop_size//2, w-crop_size//2-1) y = random.randint(crop_size//2, h-crop_size//2-1) elif type == &#x27;center&#x27;: x = w//2 y = h//2 else: logger.warning(&#x27;unknow type, use center crop instead&#x27;) x = w//2 y = h//2 return frame[y-crop_size//2:y+crop_size//2, x-crop_size//2:x+crop_size//2, :]# customized_video_input_op.h:211,212, sampling# @sa: deploy_net_video_local:collect_clip()# customized_video_input_op.h:213, normalizedef _normalize(clip, mean=128, std=1): # just do it over clip return (clip-mean)/std # customized_video_input_op.h:219, channel_swapdef _channel_swap(frame, use_bgr=True): if use_bgr: return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) else: return frame Q&amp;A 训练过程中，会周期性地进行 test，我在 test 的时候报错 1234Traceback: File &quot;lib/utils/misc.py:200&quot;: used_gpu_memory = out_dict[&#x27;Used GPU Memory&#x27;]KeyError: Used GPU Memory 查了半天不知道正确的 key 是什么，直接注释掉了 lib/utils/metrics.py:343 1# json_stats[&#x27;used_gpu_memory&#x27;] = misc.get_gpu_stats() 关于训练时使用 i3d_nonlocal_8x8_IN_pretrain_400k 会直接结束训练的问题，作者提供了一个新的脚本，重置了 lr, iteration, momentum 等信息。 1234cd &lt;caffe2-dir&gt;/video-nonlocal-net/process_data/convert_models/# 修改 modify_blob_rm.py: 5,6 行input_model_file --&gt; ../../data/pretrained_model/i3d_nonlocal_8x8_IN_pretrain_400k.pkloutput_model_file --&gt; ../../data/pretrianed_model/i3d_nonlocal_8x8_IN_pretrain_400k_clean.pkl finetune 的时候，log 中显示的进度是根据 Kinetic 数据集的规模计算，有很大的出入，具体的设置在 &lt;caffe2-dir&gt;/video-nonlocal-net/lib/core/config.py:78, 配置 yaml 文件可以修改，添加 TRAIN.DATASET_SIZE = 9537 训练是输入的是视频，具体按照怎样的规则得到 clip ？github issue 中有两个是与这个问题相关的，可以参考 #12 #15简单来说，第一步会 decode 出输入视频的所有帧，此时如果遇到太短的视频会抛弃掉，从源代码看，最小是 32K 字节。==&gt; video/customized_video_decoder.cc:79第二步从解码出来的帧中，随机选择 clip_length 长度的图片进行训练。 ==&gt; video/customized_video_io.cc:674,687 训练和测试的时候视频经过怎样的预处理？#16#10","categories":[{"name":"Caffe2","slug":"Caffe2","permalink":"http://example.com/categories/Caffe2/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Action Recgnition","slug":"Action-Recgnition","permalink":"http://example.com/tags/Action-Recgnition/"},{"name":"Caffe2","slug":"Caffe2","permalink":"http://example.com/tags/Caffe2/"}]},{"title":"TSN-Pytorch","slug":"Pytorch-20210107-TSN-Pytorch","date":"2021-01-07T01:23:06.000Z","updated":"2021-01-18T04:44:13.940Z","comments":true,"path":"2021/01/07/Pytorch-20210107-TSN-Pytorch/","link":"","permalink":"http://example.com/2021/01/07/Pytorch-20210107-TSN-Pytorch/","excerpt":"https://github.com/yjxiong/tsn-pytorch 0. IntroTSN 是动作识别领域里面一个很老工作，也是一个很重要的工作。作者开源了代码，是用 Caffe 实现的，但是由于 Pytorch 在学界的流行，作者又重新实现了一个 Pytorch 版本的。代码写的很优雅，基本是可以直接用的，这个简单记录一下使用步骤。TSN 的训练模式有 3 种, (1) RGB, (2) Flow, (3) RGB+Flow, 这里只介绍第一种下文需要用到的代码片段都已经上传到 fork 的 repo 中 https://github.com/clover978/tsn-pytorch","text":"https://github.com/yjxiong/tsn-pytorch 0. IntroTSN 是动作识别领域里面一个很老工作，也是一个很重要的工作。作者开源了代码，是用 Caffe 实现的，但是由于 Pytorch 在学界的流行，作者又重新实现了一个 Pytorch 版本的。代码写的很优雅，基本是可以直接用的，这个简单记录一下使用步骤。TSN 的训练模式有 3 种, (1) RGB, (2) Flow, (3) RGB+Flow, 这里只介绍第一种下文需要用到的代码片段都已经上传到 fork 的 repo 中 https://github.com/clover978/tsn-pytorch 1. Data-Prepare 抽帧：首先生成视频文件列表： 12cd /home/dxx/UCF101/UCF101/find . -type f &gt; trainvallist.txt 然后抽帧：12cd &lt;tsn-path&gt;python tools/extract_frames.py 生成 lst 文件：tsn-pytorch 用文本文件的形式作为输入，读取数据集。文本文件的格式如下：12# 视频帧路径 视频总帧数 视频标签&lt;image_dir_path&gt; &lt;frames_cnt&gt; &lt;label_index&gt; 准备 classInd.txt 文件，用下面的脚本生成训练需要的文本文件：1python gen_tsn_list.py 2. Train ucf101 训练：UCF101数据集 可以直接使用如下的命令训练： 123456789# tools/train_tsn.shCUDA_VISIBLE_DEVICES=0,1,2,3 \\ python -u main.py ucf101 RGB train_tsn.txt val_tsn.txt \\ --arch BNInception \\ --num_segments 5 --gd 20 \\ --lr 0.001 --lr_steps 70 130 --epochs 180 \\ -b 140 -j 8 --dropout 0.5 --snapshot_pref models/meitu_bninception \\ |&amp; tee log.txt 自定义数据集训练：自定义的数据集，可以使用同样的命令训练，训练之前只需要修改 main.py, 将 num_class 替换为自己数据集的类别数123# main.py:25if args.dataset == &#x27;ucf101&#x27;: num_class = &lt;datasets classes number&gt; 使用 resume 参数从中间开始训练1--resume models/tsn-pytorch/meitu_bninception_rgb_1_checkpoint.pth.tar","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Action Recgnition","slug":"Action-Recgnition","permalink":"http://example.com/tags/Action-Recgnition/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"}]},{"title":"Deep-high-resolution-net.pytorch","slug":"Pytorch-20210107-Deep-high-resolution-net-pytorch","date":"2021-01-07T01:15:54.000Z","updated":"2021-01-07T01:19:30.929Z","comments":true,"path":"2021/01/07/Pytorch-20210107-Deep-high-resolution-net-pytorch/","link":"","permalink":"http://example.com/2021/01/07/Pytorch-20210107-Deep-high-resolution-net-pytorch/","excerpt":"https://github.com/leoxiaobin/deep-high-resolution-net.pytorch","text":"https://github.com/leoxiaobin/deep-high-resolution-net.pytorch 1. 配置 HRNET 环境12345678910111213141516171819202122232425262728293031323334353637383940414243FROM nvidia/cuda:11.1-cudnn8-devel-ubuntu18.04LABEL auther=clover978# Basic toolchainRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ git \\ ffmpeg \\ python3-pip \\ python3-dev \\ libsm6 \\ libxext6 \\ &amp;&amp; cd /usr/bin \\ &amp;&amp; ln -s /usr/bin/python3 python \\ &amp;&amp; pip3 install --upgrade pip \\ &amp;&amp; apt-get autoremove -y \\ &amp;&amp; rm -rf /var/lib/apt/lists/* # clone deep-high-resolution-netARG POSE_ROOT=/workspace/deep-high-resolution-netRUN git clone https://gitee.com/clover978/deep-high-resolution-net.pytorch.git $POSE_ROOTWORKDIR $POSE_ROOTRUN pip3 install --no-cache-dir -r requirements.txt &amp;&amp; \\ pip3 install --no-cache-dir pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors \\ pip3 install --no-cache-dir torch==1.7.0+cu110 torchvision==0.8.1+cu110 -f https://download.pytorch.org/whl/torch_stable.html # build deep-high-resolution-net libWORKDIR $POSE_ROOT/libRUN make# install COCO APIARG COCOAPI=/cocoapiRUN git clone https://gitee.com/clover978/cocoapi $COCOAPIWORKDIR $COCOAPI/PythonAPIRUN make install# clone ARG DET_ROOT=/workspace/Yet-Another-EfficientDet-PytorchRUN git clone https://gitee.com/clover978/Yet-Another-EfficientDet-Pytorch $DET_ROOTWORKDIR $POSE_ROOT clover fork 的 HRNET(https://gitee.com/clover978/deep-high-resolution-net.pytorch.git) 进行了一些改动： 支持 QDTM 数据集，数据集包含 18 个关键点，不同于 COCO 的 17 个点； 添加代码保存中间模型，每 10 个 epoch 保存一次； 添加 inference 代码，可以对图片文件夹进行遍历，并将结果生成视频。 clover fork 的 cocoapi(https://gitee.com/clover978/cocoapi) 进行了一个改动： 更改 PythonAPI/pycocotools/cocoeval.py:523，以适应 QDTM 数据集的 18 个关键点。 2. 训练、测试12345docker run -itd --name hrnet --ipc host -v /home/dxx:/home/dxx -v /data:/data --net host dxx/hrnet:v1$ deep-high-resolution-net.pytorchpython tools/train.py --cfg experiments/qdtm/hrnet/w48_384x288_adam_lr1e-3_vault_v3.yaml python tools/inference.py --cfg experiments/qdtm/hrnet/w48_384x288_adam_lr1e-3_vault_v3.yaml TEST.MODEL_FILE output/output/qdtm/pose_hrnet/w48_384x288_adam_lr1e-3_vault_v3/model_best.pth","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"}]},{"title":"Git commitzen","slug":"others-20210106-Git-commitzen","date":"2021-01-06T12:36:23.000Z","updated":"2021-01-06T12:38:36.300Z","comments":true,"path":"2021/01/06/others-20210106-Git-commitzen/","link":"","permalink":"http://example.com/2021/01/06/others-20210106-Git-commitzen/","excerpt":"Git0. IntroGit 是一种非常常见的 分布式版本管理工具，学习使用 git应该算是程序员的一项基本功了，毕竟稍微有一些体量的公司，都会有自己的版本管理系统。Git 的子命令相当之多，比较复杂的那些，有相关需求的时候上网查就可以了，这篇笔记只记录最简单常用的命令。","text":"Git0. IntroGit 是一种非常常见的 分布式版本管理工具，学习使用 git应该算是程序员的一项基本功了，毕竟稍微有一些体量的公司，都会有自己的版本管理系统。Git 的子命令相当之多，比较复杂的那些，有相关需求的时候上网查就可以了，这篇笔记只记录最简单常用的命令。 1. Why Git以下内容纯属个人理解，没有参考网上资料，不一定正确Git 中引入了几个重要的概念： 分支：一个代码仓库（repo）至少拥有一个 master 分支，还可以拥有多个其他分支，在分支上的开发各自分开，然后合并（merge）到主分支中。 暂存区：在 git 中，磁盘上的代码可以叫做本地代码，被加入到 git仓库的代码处于工作区，工作区的代码的改动不会直接进入分支，而是被放在暂存区中。使用 git diff 命令，可以轻松查看开发迭代在上版本代码上的所有改动。在实际使用过程中会对这些概念有更清晰的认识。 origin/master：git 的代码管理是分布式的，同样一份代码，可以存储在各个不同的地方。这需要依赖一个 git 服务器和一个 origin 分支。因此 git仓库 中的代码总共存在于 5 个地方：本地磁盘、工作区、暂存区、本地分支、云端分支。前四个在本地存储，最后一个在服务器存储，没有 git 服务器同样可以使用 git 做版本管理，只是不支持分布式，本地删掉之后代码就没有了。其中 origin 就是远程仓库的名字，master 是远程分支的名字，当仓库只有一个分支的时候，origin/master 即代表了远端仓库的代码。 2. Git in action使用 git，首先需要配置一下开发者用户名123git config --global user.name &quot;yourname&quot;git config --global user.email &quot;your.email@gmail.com&quot;git config --list下面用几个命令，简单介绍 git 的最常见用法123456789101112131415161718192021222324252627282930313233343536# 创建一个 git 仓库git init # 一般这个命令会在空文件夹下执行，此时工作区为空# 创建一个新文件touch readme.txt # 此时磁盘代码发生了变动，工作区依然为空# 查看分支状态, 将代码加入 git 仓库 ## --&gt; 如果文件是 new file，使用 git add &lt;file&gt; 将其加入仓库，这样文件同时进入了工作区和暂存区 git status git add readme.txt ## --&gt; 如果文件是 modified，代表文件本来处于工作区，但是进行了改动，使用 git add &lt;file&gt; 将其加入暂存区，或者使用 git checkout -- &lt;file&gt; 撤销工作区改动。git diff &lt;file&gt; 可以查看文件的改动 echo &quot;new changes&quot; &gt; readme.txt # 现在工作区和暂存区中的文件是一致的。对文件进行改动 git status git diff git add readme.txt | git checkout -- readme.txt# 提交改动# 使用 git commit 将 暂存区 的修改提交到 git 分支上，每一次 commit 对应 git 分支上的 一个节点。git commit -m &quot;&lt;massage&gt;&quot;orgit commit # edit commit massage in vim# 查看 loggit log# 提交到远端仓库 ## --&gt; 如果本地是一个新仓库，则需要配置远端仓库地址，如果本地仓库是从远端 clone/pull 下来的，则不需要配置 git remote add origin &lt;url&gt; # url 示例 https://github.com/clover978/tsn-pytorchgit push origin master # 将本地分支 push 到远端的 master 分支 （origin 代表远端仓库的地址）# 从远程仓库拉取。代码提交到远端之后，就可以在别的地方拉取代码了 ## --&gt; 如果没有从远端拉取过代码，则使用 git clone 克隆一个新的代码仓库 git clone &lt;url&gt; or git remote add origin &lt;url&gt; # 配置远端仓库地址，然后拉取git pull origin master # 拉取远端的 master 分支到本地 Git commitzen0.. Intro用工具思路规范化 git commit massageGit commitzen 是一个 规范 commit massage 的工具。掌握了 git 之后，自然也就知道 commit massage 是什么东西了。commit massage 需不需要规范化属于个人喜好问题，这篇笔记推荐一种目前流行的 commit massage 规范以及配置方法。 1.. Angular Git Commit Guidelines1234567891011&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;BLANK LINE&gt;&lt;body&gt;&lt;BLANK LINE&gt;&lt;footer&gt;type: 本次 commit 的类型，诸如 bugfix docs style 等scope: 本次 commit 波及的范围subject: 简明扼要的阐述下本次 commit 的主旨，在原文中特意强调了几点 1. 使用祈使句 2. 首字母不要大写 3. 结尾无需添加标点body: 同样使用祈使句，在主体内容中我们需要把本次 commit 详细的描述一下，比如此次变更的动机，如需换行，则使用 |footer: 描述下与之关联的 issue 或 break change 2. commitizen看完上面规范之后会不会感觉很麻烦，每次都要回忆该怎么写 commit massage。commitzen 就是一个帮助规范化 commit massage 的工具。可以在 github 上欣赏一下该项目的 commits，或者 clone 到本地用 git log 看一下。下面介绍怎么使用 commitzen 命令行安装安装 commitzen-cli 需要先安装 npm 1234# windows下载安装 node.js. https://nodejs.org/en/# linuxapt install npm 安装 commitzen-cli 及 conventional change log 123npm install -g commitizennpm install -g cz-conventional-changelogecho &#x27;&#123; &quot;path&quot;: &quot;cz-conventional-changelog&quot; &#125;&#x27; &gt; ~/.czrc 使用 git cz 代替 git commit 提交代码 VSCode（推荐如果使用 vscode 的话，可以直接安装 Visual Studio Code Commitizen Support 插件","categories":[{"name":"others","slug":"others","permalink":"http://example.com/categories/others/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"Git","slug":"Git","permalink":"http://example.com/tags/Git/"}]},{"title":"Tesseract-OCR","slug":"others-20210106-Tesseract-OCR","date":"2021-01-06T12:34:38.000Z","updated":"2021-01-06T12:38:45.152Z","comments":true,"path":"2021/01/06/others-20210106-Tesseract-OCR/","link":"","permalink":"http://example.com/2021/01/06/others-20210106-Tesseract-OCR/","excerpt":"Tesseract 是 Github 上一个 OCR 的开源项目，目前最新版已经更新到 v4.0.x，最近小小尝试了一下怎么使用这个开源库，记一下笔记。","text":"Tesseract 是 Github 上一个 OCR 的开源项目，目前最新版已经更新到 v4.0.x，最近小小尝试了一下怎么使用这个开源库，记一下笔记。 Install环境： Ubuntu16.04Tesseract 可以使用 apt 安装，但是在 Ubuntu18.04 之前的版本，官方库里面的最新版本只有 3.x.x，作为 Ubuntu16.04 用户，需要先添加 PPA，其他系统版本的 PPA 可以在这里找到。123apt-get install python-software-properties software-properties-commonadd-apt-repository ppa:alex-p/tesseract-ocrapt-get update添加完 PPA 之后就可以直接 apt 安装了1apt-get install tesseract-ocr libtesseract-dev除了 apt 安装外，也可以直接从 github clone代码，然后编译安装， 不再详述。 Language supportTesseract 的模型文件放在 $TESSDATA_PREFIX 指定的位置，当找不到这个环境变量是，默认路径是 /usr/share/tesseract-ocr/4.00/tessdata Teseeract 支持的语言及对应的模型文件可以在 这里 找到 Tesseract 的中文支持有两种方式： 直接 apt 安装apt install tesseract-ocr-chi-sim tesseract-ocr-chi-sim-vert 简体中文语言包模型 12wget https://github.com/tesseract-ocr/tessdata/raw/4.00/chi_sim.traineddatamv chi_sim.traineddata /usr/share/tesseract-ocr/4.00/tessdata/ tesseract --list-langs 查看目前支持的语言 Command Line Usagehttps://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage Tesseract 的命令行使用可以参考上面的链接这里列举一个最简答的用法1234567tesseract input.png output -l eng --oem 3 --psm 3# input.png： 输入的图片# output： 输出文件名（指定 stdout 输出到标准输出）# -l：指定语言，eng（英语，默认），chi_sim（简体中文）# --oem[0,1,2,3]： OCR Engine Mode，4.x.x 版本默认为 3, 就是使用 available 的模型，即 LSTM 模型# --psm[0-13]： Page Segement Mode，默认为 3，自动分页# Tesseract 还有许多许多其他的参数可以设置，具体可以参照上方的链接 Python APIhttps://github.com/sirfz/tesserocrTesseract 官方给出了一个使用 Python API 的 脚本，脚本的原理是使用 ctypes + .so/.dll文件，用 Python 调用 C 的接口，这种方法还需要写 platform specified 的代码，所以比较麻烦。我在网上找到有人写了一个 Python Wrapper， 在 官方 repo 下还可以找到更多 Wrapper 的链接，下面介绍利用这个方法使用 Python 接口。 安装 123apt-get install tesseract-ocr libtesseract-dev libleptonica-dev# 前两个在装 Tesseract 的时候已经安装过了，一定记得要装第三个东西CPPFLAGS=-I/usr/local/include pip3 install tesserocr Usage官方 repo 里面给出了很多 接口示例，这里摘抄一个最基本的用法，tesserocr 支持输入 PIL Image 对象或者 图片文件。1234567891011from tesserocr import PyTessBaseAPIimages = [&#x27;sample.jpg&#x27;, &#x27;sample2.jpg&#x27;, &#x27;sample3.jpg&#x27;]with PyTessBaseAPI() as api: for img in images: api.SetImageFile(img) print api.GetUTF8Text() print api.AllWordConfidences()# api is automatically finalized when used in a with-statement (context manager).# otherwise api.End() should be explicitly called when it&#x27;s no longer needed. ERROR按照上面的步骤，确实可以安装成功 tesserocr， 然后执行 python3 -c &#39;import tesserocr; print(tesserocr.tesseract_version())&#39;，发现版本号竟然是 3.4.0. 于是开始了漫长的升级之旅 卸载 Tesseract3.4.0， tesserocr首先将已经安装的版本卸载掉 重新安装 tesserocr安装之后又出错了， import 的时候找不着 tesseract.so.3。这个就很烦了，无奈之下我只能从源码开始安装 Tesseract， 再从 源码安装 tesserocr， 最后终于成功了。 从源码安装 Tesseracthttps://github.com/tesseract-ocr/tesseract/wiki/Compiling 从源码安装 tesserocr 123git clone https://github.com/sirfz/tesserocrexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/:/usr/local/lib/pip3 install . 最后问题是终于解决了，具体这两个东西到底哪个是必须要源码编译，哪个可以用 apt/pip 安装，我也不知道，反正两个全部都编译安装的话是可以成功的。注意安装 tesserocr 的时候，确保电脑上只有最新版的 Tesseract","categories":[{"name":"others","slug":"others","permalink":"http://example.com/categories/others/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"OCR","slug":"OCR","permalink":"http://example.com/tags/OCR/"}]},{"title":"Useful bash/powershell commands","slug":"Linux-20210106-Useful-bash-powershell-commands","date":"2021-01-06T12:02:46.000Z","updated":"2021-01-11T06:20:18.948Z","comments":true,"path":"2021/01/06/Linux-20210106-Useful-bash-powershell-commands/","link":"","permalink":"http://example.com/2021/01/06/Linux-20210106-Useful-bash-powershell-commands/","excerpt":"收集一些常用的 Windows Powershell 和 Linux bash 命令","text":"收集一些常用的 Windows Powershell 和 Linux bash 命令 1. Powershell12# 创建软连接New-Item -Path images -ItemType SymbolicLink -Value E:\\coco\\images\\ 12345678910# windows 命令行配置: 设置codepage为 936，GUI的设置经常不能生效Windows Registry Editor Version 5.00[HKEY_CURRENT_USER\\Console\\%SystemRoot%_system32_cmd.exe]&quot;CodePage&quot;=dword:0000fde9&quot;FontFamily&quot;=dword:00000036&quot;FontWeight&quot;=dword:00000190&quot;FaceName&quot;=&quot;Consolas&quot;&quot;ScreenBufferSize&quot;=dword:232900d2&quot;WindowSize&quot;=dword:002b00d2 2. Bash123# 同时在 命令行 和 文件中输出&lt;cmd&gt; | tee &lt;log-file&gt;# echo &quot;hello&quot; 2&gt;&amp;1 | tee output.txt 12# 查看终端大小stty size 12# 查看程序占用swapawk &#x27;/^Swap:/ &#123;SWAP+=$2&#125;END&#123;print SWAP&quot; KB&quot;&#125;&#x27; /proc/$(pid)/smaps 12# 重启 kdekquitapp5 plasmashell &amp;&amp; kstart plasmashell 123456789# 设置系统语言apt install localeslocale-gen zh_CN.utf8locale-gen zh_CNexport LANG=zh_CN.utf8export LC_ALL=zh_CN.utf8export LANGUAGE=zh_CN.utf8update-locale LANG=zh_CN.utf8 LC_ALL=zh_CN.utf8 LANGUAGE=zh_CN.utf8locale -a 1234# sed 命令操作文本sed [-i] &#x27;s/^/HEAD/g&#x27; test.filesed [-i] &#x27;s/$/TAIL/g&#x27; test.filesed &#x27;10,20i new_line_between_10_and_20&#x27; test.file 1234# vncstart: vncserver -geometry 1920x1080 exec: ip:&lt;port&gt;stop: vncserver -kill :&lt;port&gt; 12# 批量重命名文件rename .jpeg .jpg ./* 1234# nohup 后台运行程序nohup ./run.sh &gt; log.txt &amp;# 其中 &amp; 表示将命令放到后台执行; nohup 表示让进程忽略 HUP 信号。这样，关闭当前窗口后，程序仍然会执行，如果想停止程序，只能通过 ps 找到进程号，然后 kill 掉 1234567891011121314151617181920# `screen` 命令提供了一种更强大的后台执行命令方法# 新建一个 会话窗口（Session）screen -S &lt;会话名称&gt;# 列出所有会话screen -ls# 进入一个会话screen -r &lt;会话名称&gt;# 进入会话窗口后，就可以执行需要执行的命令了，在执行过程中退出会话不会中断程序# 在会话窗口中退出会话Ctrl + a + d# 在会话窗口外退出会话screen -d &lt;会话名称&gt;# 可以看到 `screen` 将任务放到后台后，支持随时随地查看任务状态，或者与任务进行交互。不需要像 `nohup` 那样必须重定向出 log 文件（当然实际情况下重定向更为方便）。 123456789101112131415161718192021# 将已经执行的前台任务放到后台# 执行命令./run.sh# 将任务放到后台，此时任务状态是 StoppedCtrl + z# 使用 `jobs` 可以查看任务jobs# 将任务放到后台执行bg &lt;job-id&gt;# 将任务放到前台执行fg &lt;job-id&gt;# 任务放到后台之后，一旦关闭当前窗口，这个窗口下的所有任务都会收到 HUP 信号，从而中断运行，因此还需要另外一个命令。 # 使用 disown 命令，让任务不再附属于当前窗口，这样，使用 jobs 命令会发现任务不见了，关闭当前窗口也不会中断任务。disown -h &lt;job-id&gt;disown -ah # 将所有任务移出作业列表 12# 批量删除进程ps -ef | grep firefox | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9 123# github content DNS 污染199.232.68.133 raw.githubusercontent.com199.232.68.133 userimage.githubusercontent.com 3. 图像处理1234567# resize 视频，保持较短边大于 256 ffmpeg -i input.mp4 -vf scale=256:256:force_original_aspect_ratio=increase output.mp4# 这条命令计算出来的 w/h 可能是奇数，对于 libx264 编码，无法生成视频ffmpeg -i input.mp4 -vf scale=if(lt(iw\\,ih)\\,256\\,-2):if(lt(iw\\,ih)\\,-2\\,256) output.mp4# 这个命令可以完美实现功能，并且只需要读取一次视频 123# 提取视频帧， 设置 strideffmpeg -i input.mp4 -vf &quot;scale=256:256:force_original_aspect_ratio-increase, select=not(mod(n\\,4))&quot; -vsync vfr images/img-%06d.jpg 12# ffmpeg 抽帧ffmpeg -i test.mp4 -r 1 -ss 0:0:0 -t 0:0:3 -s 1280x720 -f image2 %4d.jpg 12# ffmpeg 剪切视频ffmpeg -ss 00:00:15 -t 00:00:05 -i input.mp4 -vcodec copy -acodec copy output.mp4 123# imagemagick 处理图片convert in.jpg -rotate 90 out.jpgconvert in.jpg -resize 640x360 out.jpg 4. pip 镜像源12# 临时用法： pip install example.whl -i https://pypi.douban.com/simple --trusted-host=pypi.douban.com 12345# 永久配置：# vi ~/.pip/pip.conf [global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Packages Collections","slug":"Linux-20210106-Packages-Collections","date":"2021-01-06T11:59:23.000Z","updated":"2021-01-07T02:41:24.253Z","comments":true,"path":"2021/01/06/Linux-20210106-Packages-Collections/","link":"","permalink":"http://example.com/2021/01/06/Linux-20210106-Packages-Collections/","excerpt":"收集一些比较冷门的 lib, packages 安装命令","text":"收集一些比较冷门的 lib, packages 安装命令 Ubuntu:1234libgthread-2.0.so.0 apt install libglib2.0-0libSM.so.6 apt install libsm6libXrender.so.1 apt install libxrender1libXext.so.6 apt install libxext6 Python312pycocotools pip install &quot;git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI&quot;skbuild pip install scikit-build Caffe123456789101112131415161718apt install -y \\ protobuf-compiler \\ libprotobuf-dev \\ libboost-all-dev \\ libgflags-dev \\ libgoogle-glog-dev \\ libatlas-base-dev \\ libopencv-dev \\ libhdf5-dev \\ libleveldb-dev \\ liblmdb-dev \\ libsnappy-dev \\ python-numpy \\ python-setuptools \\ python-opencv \\ ipython \\pip install scikit-image protobuf","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Ubuntu installation","slug":"Linux-20210106-Ubuntu-installation","date":"2021-01-06T11:51:30.000Z","updated":"2021-01-07T03:33:19.420Z","comments":true,"path":"2021/01/06/Linux-20210106-Ubuntu-installation/","link":"","permalink":"http://example.com/2021/01/06/Linux-20210106-Ubuntu-installation/","excerpt":"0. 备份数据 bashrc 文件 docker123docker save -o images.tar $image_id1 $ image_id2docker export -o container_1.tar container_id1docker export -o container_2.tar container_id2 记录重要文件路径","text":"0. 备份数据 bashrc 文件 docker123docker save -o images.tar $image_id1 $ image_id2docker export -o container_1.tar container_id1docker export -o container_2.tar container_id2 记录重要文件路径 1. 制作系统盘 下载 镜像文件 下载安装 大白菜 制作启动盘 2. 安装系统 按照安装流程安装系统 安装 net-tools, openssh123sudo apt update &amp;&amp; sudo apt install -y net-tools, openssh-serverservice ssh startifconfig 关闭图形显示，确认ip地址1234# 关闭图形化显示sudo systemctl set-default multi-user.targetsudo rebootifconfig 3. 挂载硬盘 格式化硬盘，推荐 ext4 创建挂载点 挂载硬盘123sudo mkfs.ext4 /dev/sda1sudo mkdir /datasudo mount /dev/sda1 /data 4. 更新国内源123456789101112131415sudo mv /etc/apt/sources.list /etc/apt/sourses.list.backupsudo vi /etc/apt/sources.listdeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversesudo apt update 5. 安装显卡驱动12345下载显卡驱动https://www.nvidia.cn/Download/index.aspxsudo apt install gcc makesudo ./NVIDIA-Linux-x86_64-455.23.04.run –no-opengl-files -no-x-check -no-nouveau-check 6. 安装 docker123456789101112131415161718192021222324252627# 安装 docker-cecurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyunsudo usermod -aG docker $USERdocker -v# 安装 nvidia-docker2curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get updatesudo apt-get install nvidia-docker2# 配置默认 nvidia runtimesudo vi /etc/docker/daemon.json&#123; &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] &#125; &#125;&#125;service docker restartdocker run --rm hello-world nvidia-smi","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"COCO dataset format","slug":"others-20210106-COCO-dataset-format","date":"2021-01-06T11:28:43.000Z","updated":"2021-01-06T12:38:49.882Z","comments":true,"path":"2021/01/06/others-20210106-COCO-dataset-format/","link":"","permalink":"http://example.com/2021/01/06/others-20210106-COCO-dataset-format/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556- annotations - &#x27;info&#x27;: dict - &#x27;licenses&#x27; list - &#x27;images&#x27; [ &#123; - &#x27;filename&#x27;: str - &#x27;id&#x27;: int - &#x27;height&#x27;: int - &#x27;width&#x27;: int - &#x27;flickr_url&#x27;: str - &#x27;license&#x27;: int - &#x27;coco_url&#x27;: str - &#x27;date_captured&#x27;: str &#125;, ... ... ] - &#x27;annotations&#x27; [ &#123; - &#x27;segmentation&#x27;: list - &#x27;num_keypoints&#x27;: int - &#x27;area&#x27;: float - &#x27;is_crowd&#x27;: bool - &#x27;keypoint&#x27;: - x1, y1, k1 # k=0 不可见未标注；k=1 不可见标注； k=2 可见标注 - x2, y2, k2 - ...... - &#x27;bbox&#x27;: list - ( x1, y1, w1, h1 ) - ( x2, y2, w2, h2 ) - ...... - &#x27;category_id&#x27;: int - &#x27;image_id&#x27;: int - &#x27;id&#x27;: int &#125;, ... ... ] - &#x27;categories&#x27;: [ &#123; - &#x27;id&#x27;: int - &#x27;keypoints&#x27;: - kpt-name1 - kpt-name2 - ... - &#x27;skeleton&#x27;: - (pair1_index1, pair1_index2) - (pair2_index1, pair2_index2) - ... - &#x27;name&#x27;: &#x27;person&#x27; - &#x27;supercategory&#x27;: &#x27;person&#x27; &#125; ]","categories":[{"name":"others","slug":"others","permalink":"http://example.com/categories/others/"}],"tags":[{"name":"COCO","slug":"COCO","permalink":"http://example.com/tags/COCO/"}]},{"title":"Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition","slug":"Paper-Notes-FGVC-20210106-Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition","date":"2021-01-06T06:10:48.000Z","updated":"2021-01-18T04:44:07.761Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition/","excerpt":"Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition CVPR 2016 这篇文章提出了一种使用车辆 3D box 作为网络输入，来识别车型的方法。首先检测出车辆的 3D包围框，然后展开成 2D 图形，最后加入视角信息作为额外输入进行训练。同时还发布了一个新的数据集 BoxCars","text":"Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition CVPR 2016 这篇文章提出了一种使用车辆 3D box 作为网络输入，来识别车型的方法。首先检测出车辆的 3D包围框，然后展开成 2D 图形，最后加入视角信息作为额外输入进行训练。同时还发布了一个新的数据集 BoxCars 1. Method 3D 包围框：文中没有介绍获得车辆的 3D 包围框 的方法。这是作者之前的一个工作。Automatic camera calibration for traffic understanding 车辆图片平展：获得 3D 包围框后，将其展开成 2D 图像，具体方法类似于将立体的纸盒子在平面上铺开。很简单的一个思路。 车辆的视角信息：作者用三个向量来对车辆的视角信息进行编码。分别是 车尾-车头向量、车内-车外向量，车底-车顶向量。通过这三个向量，可以完整表示出车辆的视角。 栅格化包围框：作者同时提出另外一种方法描述车辆的视角。将车辆的矩形包围框中用四种颜色进行表示。其中 车顶用黄色，车侧用红色，车头/车尾同蓝色，其他位置用白色。这样一幅新的栅格化图像同样可以确定出车辆的视角。 平铺图 + 视角辅助信息 训练：平铺图直接进入 CNN 进行卷积操作；视角编码通过 6x6 的矩阵表示，使用三向量表示的情况下，矩阵的第一行表示 3 个二维向量，其他行用 0 填充；使用栅格化包围框表示的情况下，直接将 bbox rescale 成 6x6 的矩阵。文中只说 view encoding 是加在卷积操作之后，但是具体怎么加没有说明。 Bbox Cars 数据集：作者同时还公布了一个新的数据集，数据集中的图片是从道路车辆监控视频中获取的，主要特点就是包含了车辆的 3D box 信息。 2. 实验结果： 作者提出的方法需要数据集标注了车辆的 3D box 信息，具体怎么在 CompCars 数据集上做的实验阐述的不太清楚，下面是实验结果： top1 top5 baseline 0.767 0.917 ours 0.848 0.954","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"A Large-Scale Car Dataset for Fine-grained Categotozation and Verification","slug":"Paper-Notes-FGVC-20210106-A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification","date":"2021-01-06T06:10:34.000Z","updated":"2021-01-18T04:44:10.927Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification/","excerpt":"A Large-Scale Car Dataset for Fine-grained Categotozation and Verification CVPR 2015 这篇文章提出了一个大型的数据集 CompCars，里面包含了从网络上收集的 13.6w 张整车图片， 2.7w 张车辆局部图片，还有从监控场景下拍摄的 5k 张车辆正脸图片。 实验部分使用这个数据集分别用于 车辆细粒度分类、车辆属性所识别、车辆验证 三个任务。","text":"A Large-Scale Car Dataset for Fine-grained Categotozation and Verification CVPR 2015 这篇文章提出了一个大型的数据集 CompCars，里面包含了从网络上收集的 13.6w 张整车图片， 2.7w 张车辆局部图片，还有从监控场景下拍摄的 5k 张车辆正脸图片。 实验部分使用这个数据集分别用于 车辆细粒度分类、车辆属性所识别、车辆验证 三个任务。 1. Method CompCars 数据集： 品牌（Make）：163 车型（Model）：1716 年份（Year）：4455 整车图片数：136,727 F ：18431 R ：13513 S ：23551 FS：49301 RS：31150 局部图片数：27,618 outter part: headlight, rearlight, fog light, air intake inter part: console, steering wheel, dashboard, gear lever Fine-grained classification 数据集： 车型： 431 | | Train | Test | Total | |- |- |- |- | | 图片数（整车）| 16016 | 14939| 30955 | Baseline： model: overfeat dataset: Fine-grained classification result: | view | F | R | S | FS | RS | all | |- |- |- |- |- |- |- | | top1 | 0.524 | 0.431 | 0.428 | 0.563 | 0.598 | 0.767 | | top5 | 0.748 | 0.647 | 0.602 | 0.769 | 0.777 | 0.917 | | Make | 0.701 | 0.521 | 0.507 | 0.680 | 0.656 | 0.829 |单视角识别车型： RS 效果最好；单视角识别品牌： F 效果最好；多视角 Baseline： model: 0.767, make: 0.829 Supplementary experiment:在整个 CompCars 数据集上补充一个实验，比较三种模型的效果 | model | AlexNet | Overfeat | GoogLeNet | |- |- |- |- | | top1 | 0.819 | 0.879 | 0.912 | | top5 | 0.949 | 0.969 | 0.981 |","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition","slug":"Paper-Notes-FGVC-20210106-Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition","date":"2021-01-06T05:54:07.000Z","updated":"2021-01-13T03:30:59.757Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition/","excerpt":"Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition arxiv(Baidu) 这篇文章的思路是 attention + 度量学习。其中，attention 部分跟 SENet 如出一辙；度量学习比较像 contrastive loss。文章中定义了四种类型的特征，sasc, sadc, dasc, dadc, （s: same, a: attention, d: different, c: class），进行度量学习的时候，定义只有 sasc 是正样本，约束相同类别在相同attention区域学习到相似的特征。3.s 在 CUB 数据集的准确率达到了 86.5。从 这篇文章 和 MSRA的WS-LAN 来看，attention + 度量学习 的方法是细粒度分类里面比较好的一个研究方向。","text":"Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition arxiv(Baidu) 这篇文章的思路是 attention + 度量学习。其中，attention 部分跟 SENet 如出一辙；度量学习比较像 contrastive loss。文章中定义了四种类型的特征，sasc, sadc, dasc, dadc, （s: same, a: attention, d: different, c: class），进行度量学习的时候，定义只有 sasc 是正样本，约束相同类别在相同attention区域学习到相似的特征。3.s 在 CUB 数据集的准确率达到了 86.5。从 这篇文章 和 MSRA的WS-LAN 来看，attention + 度量学习 的方法是细粒度分类里面比较好的一个研究方向。 1. Method OSME (One-Squeeze Multi-Excitation Attention Module)：这一部分，按照我的理解，可以说是完全照搬的 SENet，没有任何创新点。作者说明了 SENet 中 SE module 的原理。文中提出的网络结构，将提取到的特征，分别经过两个 SE module，得到的两个 re-weighted feature map，称为 attention1， attention2.我理解的 OSME module 与 SE module 的区别就是一个是单路的，一个是双路的。 Multi-Attention Multi-Class Constraint：这一部分，作者使用了度量学习的方法，每次训练的时候，网络会输入 2N 对图片，其中每一对图片都来自于同一类别。然后一对图片分别经过 OSME 的上半支和下半支，这样可以得到四种类别的特征。sasc, sadc, dasc, dadc，在 softmax 的基础上，作者对 hinge loss 进行改进，提出 MAMC Constraint，四种特征。挑选出一个特征作为 anchor，然后分三种情况： 正样本是 sasc， 负样本是 {sadc, dasc, dadc}; 正样本是 sadc， 负样本是 {dadc}; 正样本是 dasc， 负样本是 {dadc};在每种情况下，定义出 MAMC loss，最小化与正样本的特征距离，最大化与负样本的特征距离。 2. 实验结果： acc VGG19 79.0 ResNet50 81.7 ResNet101 82.5 ResNet-50 + OSME 84.9 ResNet-50 + OSME + MAMC_1 85.4 ResNet-50 + OSME + MAMC 86.2 ResNet-50 + OSME_3 + MAMC 86.3 ResNet-101 + OSME + MAMC 86.5 RACNN 85.3 MACNN 86.5 OSME_3: 使用 3 个 attention MAMC_1：定义 MAMC loss 的时候，只区分第一种情况。 使用 ResNet50 作为 backbone， OSME 提升了 3.2%， MAMC 提升了 1.3% 的准确率。值得一提的是，相比于 MACNN，文中提出的网络结构要简单的多，在效率上具有很大的优势。","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Fine-grained Image Classification by Visual-Semantic Embedding","slug":"Paper-Notes-FGVC-20210106-Fine-grained-Image-Classification-by-Visual-Semantic-Embedding","date":"2021-01-06T05:53:57.000Z","updated":"2021-01-13T03:30:33.687Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-Fine-grained-Image-Classification-by-Visual-Semantic-Embedding/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-Fine-grained-Image-Classification-by-Visual-Semantic-Embedding/","excerpt":"Fine-grained Image Classification by Visual-Semantic Embedding IJCAI 2018 这篇文章的创新点是利用到了细粒度分类中子类别的语义信息。文中提到了两种语义信息，一种是 text context，以 CUB 数据集为例，wiki 上对某一子类鸟的描述就是 text context；另一种是 knowledge based context，收集子类鸟的各种属性，建立知识库，有点属性学习的意思。 这篇文章的想法很有创新，并且也有很好的效果，在 CUB 数据集上达到 86.2 的准确率。但是没有验证在其他数据集上是否同时有效。","text":"Fine-grained Image Classification by Visual-Semantic Embedding IJCAI 2018 这篇文章的创新点是利用到了细粒度分类中子类别的语义信息。文中提到了两种语义信息，一种是 text context，以 CUB 数据集为例，wiki 上对某一子类鸟的描述就是 text context；另一种是 knowledge based context，收集子类鸟的各种属性，建立知识库，有点属性学习的意思。 这篇文章的想法很有创新，并且也有很好的效果，在 CUB 数据集上达到 86.2 的准确率。但是没有验证在其他数据集上是否同时有效。 1. Method Two leval CNN:作者设计了一个双层的网络结构，F(a) 是定位网络，F(b) 是回归排序网络，网络结构图参考论文。 Localization Network：这一部分是传统的目标检测网络模型。提取 定位网络的特征，与回归网络提取的特征点乘，起到 Attention 的作用。 Regression Ranking Network：这一部分通过 CNN 提取网络的特征，然后加入定位网络提取的特征作为 Attention，得到视觉特征，接下来通过 FC 层，将视觉特征映射到语义空间，网络的约束条件就是特征在语义空间中的距离，优化网络，减小学习到的语义特征与 gt 在语义空间中的特征距离。作者使用了两种语义空间，因此网络的视觉特征同时平行通过了两个 FC 层。 Knowledge Base Embedding：这里参考了 Learning Entity and Relation Embeddings for Knowledge Graph Completion 中提出的 TransR 方法。并在此基础上针对细粒度分类问题提出了 Attribute Base Embedding。这一部分属于 知识图谱 的领域，看得不是很明白。首先，传统的 知识库嵌入 中，知识库由 三元组(h, r, t) 组成，其中 h，t 表示两个实体，h 是起点，t 是终点；r 表示是实体之间的关系。实体(h,t) 由 d 维数组表示，关系(r) 由 r 维数组表示，映射矩阵 M(r) 是一个 d*r 的矩阵，将实体空间映射到关系空间。知识图谱的约束条件定义为：f(h, t) = ||h(r) + r - t(r) ||。基于 知识库嵌入 改进的 属性知识库嵌入，三元组(h, r, t)中 实体h 是样本标签y， 关系r 是 has_property_of， 实体t 是样本的属性。通过优化映射矩阵，将样本标签映射到 属性知识库空间。 Text Embedding：文本嵌入部分通过 word2vec 实现。作者首先 finetune 了一个 word2vec 模型，然后利用模型将 类别名称 映射到 文本语义空间。 2. 实验结果： 作者在 CUB 数据集上做的实验，按照论文所述，训练的过程中既没有使用 bbox 信息，也没有使用 part annotation 信息，这一点不是很明白，和我理解的训练过程不太一样。 CUB 准确率： 0.862","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses","slug":"Paper-Notes-FGVC-20210106-Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses","date":"2021-01-06T05:53:45.000Z","updated":"2021-01-18T04:44:12.712Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses/","excerpt":"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses TNNLS 2018 这篇文章提出了两个创新点，一个是 级联softmax结构，另一个是 泛化 large-margin loss。主要的思想就是细粒度分类中的标签是具有层次结构的，large-margin loss 是度量学习中提出的概念，这里在细粒度分类问题中进行了改进。 文章摆了大量的公式，所以我是没怎么仔细看的，并且它的结果并不很出色。使用 VGG 作为 backbone，在 CUB 上准确率是 77.0，结合 bilinear-CNN，从 84.1 提升到了 85.4","text":"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses TNNLS 2018 这篇文章提出了两个创新点，一个是 级联softmax结构，另一个是 泛化 large-margin loss。主要的思想就是细粒度分类中的标签是具有层次结构的，large-margin loss 是度量学习中提出的概念，这里在细粒度分类问题中进行了改进。 文章摆了大量的公式，所以我是没怎么仔细看的，并且它的结果并不很出色。使用 VGG 作为 backbone，在 CUB 上准确率是 77.0，结合 bilinear-CNN，从 84.1 提升到了 85.4 1. Method Cascaded Softmax Loss：级联softmax，见文中的 插图3，其实是一个很简单的结构，假设识别任务中有 50 个粗分类，每个分类中有 4 个细分类，总共 200 分类。传统的网络，直接通过 fc8(200)+softmax 进行训练，这里改成了 fc8(200)+ softmax + fc9(50)+softmax 训练，并且在 fc7 和 fc(9) 之间添加了一个 skip connection. 网络的 loss 则变成了每一层 softmax 的 loss 相加。 Generalized Large-Margin Loss：泛化 large-margin loss，这个用起来其实也很简单，就是在 fc7 添加一个 loss层 进行监督，large-margin loss 的作用就是增加类间距离，减小类内距离，在标签具有分层结构的情况下，对每个 level 的标签都进行这样的约束，这一部分文中使用了大量的公式，没有仔细看。 2. 实验结果： 作者的这个改动可以应用到任何 CNN 结构中，所以作者做了大量的实验： w/o bbox with bbox googlenet+SM 73.6 77.4 googlenet+CSM 74.6 78.4 googlenet+SC+CSM 75.3 79.0 googlenet+SM+GLM 76.8 80.5 googlenet+CSM+GLM 77.1 81.3 googlenet+SC+CSM+GLM 77.6 82.0 VGG+SM 72.5 78.6 VGG+SC+CSM+GLM 77.0 82.4 B-CNN 84.1 84.8 B-CNN+SC+CSM+GLM 85.4 85.7 googlenet+SM+contrastive 74.1 77.8 googlenet+SM+triplet 74.1 78.0 googlenet+SM+center loss 74.5 78.4 googlenet+SM+min-max 75.1 78.9 `SM: softmax` `CSM: cascaded softmax` `SC: skip connection` `GLM: generlized large-margin loss` 通过对比可以看出： + GLM 对结果的影响是最大的， 76.8 + 单独 CSM 的效果并不明显，加上 SC 后还能提高一下。 74.6 -&gt; 75.3 + 在 VGG， googlenet 上有明显的提高，但是在 B-CNN 上的提高就比较小了。 + GLM 相对于其他的 度量学习 方法效果也是最好的。","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Weakly Supervised Local Attention Network for Fine-Grained Visual Classification","slug":"Paper-Notes-FGVC-20210106-Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification","date":"2021-01-06T05:53:32.000Z","updated":"2021-01-18T04:44:15.760Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification/","excerpt":"Weakly Supervised Local Attention Network for Fine-Grained Visual Classification arxiv (MSRA) 这篇文章提出一种 LAP(Local Attention Pooling) 的机制，利用 attention map 去提取更具区分性的特征图；文中还提出一种弱监督的方式去训练网络的方法，在训练过程中加入了 attention dropout 和 attention center loss。 文中提出的网络 WS-LAN(Weakly Supervised Local Attention Network)在 CUB 数据集上准确率达到了 87.9","text":"Weakly Supervised Local Attention Network for Fine-Grained Visual Classification arxiv (MSRA) 这篇文章提出一种 LAP(Local Attention Pooling) 的机制，利用 attention map 去提取更具区分性的特征图；文中还提出一种弱监督的方式去训练网络的方法，在训练过程中加入了 attention dropout 和 attention center loss。 文中提出的网络 WS-LAN(Weakly Supervised Local Attention Network)在 CUB 数据集上准确率达到了 87.9 1. Method Local Attention Pooling:这一部分通过文中的 插图1 很容易就能看懂。作者首先通过 CNN 分别提取到图片的 特征图 和 注意力图，其中每一个注意力图用于聚焦目标的一个 part 上。然后将特征图与 k 个注意力图分别点乘，得到 k 个 part 的特征图。对 k 个特征图进行卷积核池化操作，得到每一个 part 的特征，将 k 个 part 特征合并到一起形成最终的特征。这一部分实际上与 SE-Net 很相似。SE-Net 将特征经过卷积之后，再经过一个 SE 结构，等同于在卷积过程中对卷积核的不同 channel 赋予不同的权重，上一层的特征会分别与卷积核的不同 channel 进行卷积操作。假设将上一层特征看做 LAN 中的特征图，channel间 attention 最大的 topk 个卷积核看做 LAN 中的注意力图，那么两个网络的区别就是一个是进行 卷积操作，一个是进行 点乘操作。所以这一部分实际上可以看做是一个稍加改动，更为复杂的 SE-Net。但是这种改动是必要的，因为下面的 WS-LAN 需要在这个网络结构上进行训练。 WS-LAN：这一部分，作者将两个传统网络训练中的 trick 迁移到了上面的 LAN 中，这应该是文章最大的两个提升点。这部分的两个 trick 实际上想解决的问题只有一个，就是 提取到的 attention map 很容易只聚焦到目标的一两个最具区分度的区域。 attention dropout：作者将 dropout 加入到 LAP 操作中。attention map 和 feature map 进行点乘的时候，attention map 以 概率(1-p) 被 drop 掉，以概率 p 被保留，并乘以 1/p，这个操作与传统的 dropout 如出一辙。 attention center loss：人脸识别文章中提出过一个 center loss，作者同样将其迁移到 WS-LAN 中。attention map 与 feature map 点乘之后得到 k 个 part feature，作者为这 k 个 feature 维护 k 个 center，然后每次训练的时候，计算每个 feature 和 center 的距离，最小化同一个 part 的 feature distance，最大化不同 part 的 feature distance，作为 attention center loss 的约束条件。 2. 实验结果： acc LAN 85.5 WS-LAN 87.9","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"Embedding label structures for fine-grained feature representation","slug":"Paper-Notes-FGVC-20210106-Embedding-label-structures-for-fine-grained-feature-representation","date":"2021-01-06T05:53:11.000Z","updated":"2021-01-13T03:30:28.245Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-Embedding-label-structures-for-fine-grained-feature-representation/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-Embedding-label-structures-for-fine-grained-feature-representation/","excerpt":"Embedding label structures for fine-grained feature representation CVPR 2016 这篇文章提出两个创新点，一个是使用 “structured label” 改进 triple loss；另一个是同时使用 triple loss 和 softmax loss 进行训练。","text":"Embedding label structures for fine-grained feature representation CVPR 2016 这篇文章提出两个创新点，一个是使用 “structured label” 改进 triple loss；另一个是同时使用 triple loss 和 softmax loss 进行训练。 1. Method 同时使用 softmax loss 和 triple loss 约束训练：这个没什么好说的，一个很简单的想法。使用三路的网络，提取特征，然后分为两支，一支过 fc+softmax 然后分类，一支对比 anchor, positive, negative 的特征距离，然后用 triple loss 进行约束。网络结果参照论文链接。 结构化目标嵌入（embed label structures）:这一部分是对 triple loss 的一个改进，在细粒度分类问题中，目标的分类可以看做层级的，以车辆分类为例，目标结构由粗到细可以是 品牌-模型-年份，因此做模型分类的时候，可以使用四元组进行训练：r(reference), p+(same model), p-(same make, diffrent model), n(diffrent make)，四元组的 loss 等价于 L(r, p+, p-) + L(r, p-, n)。 2. 实验结果： 作者在 standFord Cars 上面进行实验，实验结果略（笔记只记录 CUB 数据集的结果）","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"The unreasonable effectiveness of noisy data for fine-grained recognition","slug":"Paper-Notes-FGVC-20210106-The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition","date":"2021-01-06T05:46:06.000Z","updated":"2021-01-13T03:31:06.592Z","comments":true,"path":"2021/01/06/Paper-Notes-FGVC-20210106-The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/","link":"","permalink":"http://example.com/2021/01/06/Paper-Notes-FGVC-20210106-The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/","excerpt":"The unreasonable effectiveness of noisy data for fine-grained recognition ECCV 2016 这篇文章提出使用网络爬取的数据进行细粒度分类任务。与其说是提出一种新的方法，不如说是新建了一个数据集。 网络爬取的图片并不完全准确，因此讨论了对这些噪声的处理方法。 加入大量网络数据后，训练的效果得到了大幅度的提升，目前在 CUB 数据集上准确率达到了 92.3","text":"The unreasonable effectiveness of noisy data for fine-grained recognition ECCV 2016 这篇文章提出使用网络爬取的数据进行细粒度分类任务。与其说是提出一种新的方法，不如说是新建了一个数据集。 网络爬取的图片并不完全准确，因此讨论了对这些噪声的处理方法。 加入大量网络数据后，训练的效果得到了大幅度的提升，目前在 CUB 数据集上准确率达到了 92.3 1. Method cross-domain noise:cross-domain noise 指的是不属于这一大类的图片，例如搜索某种鸟，出来的结果是昆虫的图片。通过人工标注量化了这种噪声，发现这种情况比较少，并且当图片数目增多的时候，噪声占的比例也会减少。这种噪声对结果的影响也比较小。 cross-category noise:cross-category noise 指的是将其他子类的图片混入搜索结果的情况。这种噪声的比例难以量化，并且对结果的影响比较大，作者将搜索结果中重复的图片去除掉来减少这些噪声。 active learning：作者还提出两种标注方法辅助去除噪声。1）使用预训练好的模型，挑选搜索结果中置信度高的结果；2）人工筛选搜索结果。 2. 实验结果： CUB web-raw web-filter L-bird L-bird(MC) L-bird+CUB L-bird+CUB(MC) acc 84.4 87.7 89.0 91.9 92.3 92.2 92.8 `CUB`：CUB 数据集 `web-raw`：web 爬取数据集 `web-filter`： 去除 `cross-category noise` `L-Bird`: 爬取所有鸟类的图片，进行预训练，然后在 web-filter 上 finetune `MC`：测试的时候使用 multi-crop","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"}]},{"title":"hexo installation","slug":"hexo-20210106-hexo-installation","date":"2021-01-06T01:32:59.000Z","updated":"2021-01-06T06:08:35.376Z","comments":true,"path":"2021/01/06/hexo-20210106-hexo-installation/","link":"","permalink":"http://example.com/2021/01/06/hexo-20210106-hexo-installation/","excerpt":"1. 安装 git, node.js, Hexo 参考 Hexo 官方教程 2. 添加 github pages 新建 github repository 新建 hexo 分支，并设置为默认分支 添加 ssh hey 12ssh-keygen -t rsacat .ssh\\id_rsa.pub 在 settings -&gt; SSH and GPG keys -&gt; New SSH key 里面添加公钥。 新建 hexo 博客 123git clone clover978.github.iohexo init clover978.github.ionpm install hexo-deployer-git --save 在 clover978.github.io/_config.yml 里设置如下字段： 1234deploy:type: &#x27;git&#x27;repo: https://github.com/clover978/clover978.github.iobranch: master","text":"1. 安装 git, node.js, Hexo 参考 Hexo 官方教程 2. 添加 github pages 新建 github repository 新建 hexo 分支，并设置为默认分支 添加 ssh hey 12ssh-keygen -t rsacat .ssh\\id_rsa.pub 在 settings -&gt; SSH and GPG keys -&gt; New SSH key 里面添加公钥。 新建 hexo 博客 123git clone clover978.github.iohexo init clover978.github.ionpm install hexo-deployer-git --save 在 clover978.github.io/_config.yml 里设置如下字段： 1234deploy:type: &#x27;git&#x27;repo: https://github.com/clover978/clover978.github.iobranch: master 3. 安装 theme1git clone https://github.com/theme-next/hexo-theme-next themes/next 4. 其他设置 参考 _config.yml 和 themes/next/_config.yml 里的各种字段，设置页面布局。 5. 发布 blog 编写 blog1234hexo new &lt;title&gt;# 编辑 _post 里面生成 md 文件hexo ghexo d 同步 hexo123git add .git commit -m &quot;add new blog&quot;git push","categories":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://example.com/categories/Paper-Notes/"},{"name":"Pose Estimation","slug":"Paper-Notes/Pose-Estimation","permalink":"http://example.com/categories/Paper-Notes/Pose-Estimation/"},{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"},{"name":"C++","slug":"C","permalink":"http://example.com/categories/C/"},{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/categories/Caffe/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://example.com/categories/TensorFlow/"},{"name":"Caffe2","slug":"Caffe2","permalink":"http://example.com/categories/Caffe2/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"},{"name":"others","slug":"others","permalink":"http://example.com/categories/others/"},{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"},{"name":"FGVC","slug":"Paper-Notes/FGVC","permalink":"http://example.com/categories/Paper-Notes/FGVC/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/categories/hexo/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://example.com/tags/deep-learning/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"http://example.com/tags/Pose-Estimation/"},{"name":"poseplugin","slug":"poseplugin","permalink":"http://example.com/tags/poseplugin/"},{"name":"topdown","slug":"topdown","permalink":"http://example.com/tags/topdown/"},{"name":"benchmark","slug":"benchmark","permalink":"http://example.com/tags/benchmark/"},{"name":"bottomup","slug":"bottomup","permalink":"http://example.com/tags/bottomup/"},{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"动态链接库","slug":"动态链接库","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"混合编程","slug":"混合编程","permalink":"http://example.com/tags/%E6%B7%B7%E5%90%88%E7%BC%96%E7%A8%8B/"},{"name":"argparser","slug":"argparser","permalink":"http://example.com/tags/argparser/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Caffe","slug":"Caffe","permalink":"http://example.com/tags/Caffe/"},{"name":"Action Recgnition","slug":"Action-Recgnition","permalink":"http://example.com/tags/Action-Recgnition/"},{"name":"LMDB","slug":"LMDB","permalink":"http://example.com/tags/LMDB/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://example.com/tags/Object-Detection/"},{"name":"Faster RCNN","slug":"Faster-RCNN","permalink":"http://example.com/tags/Faster-RCNN/"},{"name":"OpenPose","slug":"OpenPose","permalink":"http://example.com/tags/OpenPose/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://example.com/tags/TensorFlow/"},{"name":"Caffe2","slug":"Caffe2","permalink":"http://example.com/tags/Caffe2/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"Git","slug":"Git","permalink":"http://example.com/tags/Git/"},{"name":"OCR","slug":"OCR","permalink":"http://example.com/tags/OCR/"},{"name":"COCO","slug":"COCO","permalink":"http://example.com/tags/COCO/"},{"name":"FGVC","slug":"FGVC","permalink":"http://example.com/tags/FGVC/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}