<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>The Devil Is in the Details-Delving Into Unbiased Data Processing for Human</title>
    <link href="/2021/01/13/Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human/"/>
    <url>/2021/01/13/Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human/</url>
    
    <content type="html"><![CDATA[<h1 id="The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human"><a href="#The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human" class="headerlink" title="The Devil Is in the Details-Delving Into Unbiased Data Processing for Human"></a><a href="https://arxiv.org/pdf/1911.07524.pdf">The Devil Is in the Details-Delving Into Unbiased Data Processing for Human</a></h1><blockquote><ol><li>本文用数学方法描述了 Pose Estimation 中的 Coordinate System Transformation（坐标系转换）和  Keypoint Format Transformation（关键点格式转换）过程，并分析其中的误差。文章发现：<br>Coordinate System Transformation 过程中，传统方法中的 flip test 操作是 Biased Coordinate System Transformation，会导致误差。<br>Keypoint Format Transformation 过程中，传统方法由于使用的量化坐标，也会导致关键点格式转换过程出现 bais。<br>作者针对上述问题，提出的 Unbiased Data Processing 方法，消除误差。</li><li>UDP 一种模型无关的算法，可以用在任意现有的算法上，提升算法效果。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><h3 id="1-1-连续空间-与-离散空间"><a href="#1-1-连续空间-与-离散空间" class="headerlink" title="1.1 连续空间 与 离散空间"></a>1.1 连续空间 与 离散空间</h3><p>  现实中的图像处于连续空间中，图像由 2维空间 中无数个点构成。一个 R<em>R 的图像中关键点的坐标由一个实数对(x,y) 表示。<br>  计算机中的图像处于离散空间中，图像由 若干像素点组成，一个 M</em>M 的图像中的关键点的坐标由一个 整数对(i,j) 表示。<br>  <strong>连续数据通过采样得到离散数据</strong>：现实中的图像通过采样（感知），形成像素点，变成离散数据，存储在计算机中。<br>  <strong>离散数据通过插值得到连续数据</strong>：计算机中的图像矩阵，可以将通过插值，将整数域的数据表示为实数域的数据，变成连续数据。  </p><h4 id="1-1-1-连续图像-与-离散图像-的转换"><a href="#1-1-1-连续图像-与-离散图像-的转换" class="headerlink" title="1.1.1 连续图像 与 离散图像 的转换"></a>1.1.1 连续图像 与 离散图像 的转换</h4><p>  <img src="/2021/01/13/Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human/连续数据与离散数据.jpg" alt="连续数据与离散数据"></p><ul><li>连续数据 到 离散数据<br>假设2维连续空间中有一幅图像，大小为 8cmx8cm，现在对其进行离散采样，我们考虑 x 轴方向上的采样情况。<br>在连续空间中，图像的 x 轴坐标范围为 [0,8]，以 x=1 处的数据为例，解释采样的过程。传感器感知 x=1 位置的数据，存储为 1 个像素，数据被转化为离散数据。可以发现，<strong>在连续空间中长度为 8 的数据，在离散空间中占据 9 个像素</strong>，离散空间中，图像的 x 轴坐标范围为 {0, 1, 2, 3, 4, 5, 6, 7, 8}。  </li><li>离散数据 到 连续数据<br>将上面的例子反过来，假设计算机中存储了一幅图像，大小为 9x9，我们通过插值的方式还原图像在现实连续空间中的样子，只考虑 x 轴方向上的情况。<br>每个像素的中心点，在连续空间中为 x=0,1,2…,7,8 的位置，连续空间中非整数部分的数据，通过插值算法生成，假设采用最邻近插值算法，图像在连续空间中的表示如图所示。可以发现，<strong>9x9 大小的离散图像，在连续空间中表示时，图像的宽度为 8 个单位长度</strong>。<br>注意，上面的例子中，图像在连续空间中的范围不是 [-0.5, 8.5]，我们在转换的过程中，坐标轴是始终对齐的，或者说，连续图像 与 离散图像 所在的坐标空间是一致的，只是数据的表示形式不同。  </li></ul><h3 id="1-2-坐标系统转换偏差分析（Coordinate-System-Transformation）"><a href="#1-2-坐标系统转换偏差分析（Coordinate-System-Transformation）" class="headerlink" title="1.2 坐标系统转换偏差分析（Coordinate System Transformation）"></a>1.2 坐标系统转换偏差分析（Coordinate System Transformation）</h3><p>  通过数学推导，UDP 得出，在姿态估计中，如果使用 flip test，关键点坐标会出现偏移，在网络输出的 heatmap 上，x 轴方向会出现一段 的偏移 $ \frac{(1-s)}{s} $ （ s 为输入输出像素比）；在原始图像的最终结果上，x 轴方向会出现 $ \frac{bw_{s}(s-1)}{W_{i}^{p}} $ （ $ bw_{s} $为 ROI 的宽像素数，$ W_{i}^{p} $为网络输入大小的宽像素数）的偏移。下面用实际例子验证上述结论。<br>  <img src="/2021/01/13/Paper-Notes-Pose-Estimation-20210118-The-Devil-Is-in-the-Details-Delving-Into-Unbiased-Data-Processing-for-Human/坐标系转换偏差.jpg" alt="坐标系转换偏差"><br>  假设：  </p><ul><li>原始图像大小为 ：18x18 像素；  </li><li>网络输入图像大小为： 12x12像素；  </li><li><p>关键点位置为：x 轴第 10 个像素点，x=9。  </p><p>flip test 的流程如下：  </p></li></ul><ol><li>将原始图像 resize 为 12x12 大小；  </li><li>将 12x12 图像进行 flip；  </li><li>将 flip 后的图像输入网络，得到heatmap，在heatmap 上定位关键点输出坐标；  </li><li>将 heatmap 进行 flip back 操作，得到heatmap上关键点最终坐标；  </li><li><p>将关键点映射到原始图像上。  </p><p>逐步分析最终结果（分别假设网络的输入输出像素比 s 为 2:1 和 3:1）：  </p></li><li>原始图像：大小 18x18，关键点位置 x=9；  </li><li>Resize，得到网络输入图像：大小 12x12，关键点位置 x=6；  </li><li>flip，得到翻转图像：大小 12x12，关键点位置 x=5；  </li><li>推理，得到 heatmap：大小为 6x6，关键点位置 x=5/2；（或者 大小为 4x4，关键点位置 x=5/3）；  </li><li>flip back，得到未翻转图像对应的heatmap：大小为 6x6，关键点位置 x=5/2；（或者 大小为 4x4，关键点位置 x=4/3）；  </li><li><p>映射到原始图像：关键点位置 x=15/2；或者（关键点位置 x=6）  </p><p>首先分析 heatmap 上关键点的位置偏差，在没有 flip test 的情况下，即去掉步骤3和步骤5，可以得到，heatmap：大小为 6x6，关键点位置 x=3；（或者大小为 4x4，关键点位置 x=2），这种情况下heatmap 上关键点的位置与原始图像是对齐的。<br>按照理论推导，s=2 时，flip test heatmap上关键点偏差为 -1/2；s=3时，flip test 在heatmap 上引入的关键点偏差为 -2/3。观察步骤5的计算结果，偏差分别为 {5/2 vs. 3} 和 {4/3 vs. 2}，与理论推导一致。<br>继续分析在原始图像上关键点的位置偏差，根据论文给出的公式，s=2时，flip test 在原始图像上引入的关键点偏差为 ；s=3时，flip test 在原始图像上引入的关键点偏差为 。观察步骤6的计算结果，偏差分别为 {15/2 vs. 9} 和 {6 vs 9}，与理论推导一致。  </p></li></ol><h3 id="1-3-关键点格式转换偏差分析（Keypoint-Format-Transformation）"><a href="#1-3-关键点格式转换偏差分析（Keypoint-Format-Transformation）" class="headerlink" title="1.3 关键点格式转换偏差分析（Keypoint Format Transformation）"></a>1.3 关键点格式转换偏差分析（Keypoint Format Transformation）</h3><p>  人体姿态估计方法中，一般我们无法直接得出网络输出heatmap上，关键点的精确坐标，现有的方法一般是，首先找到最大值坐标，然后找到最大值领域的次大值坐标，关键点坐标为最大值向次大值偏移 0.25 像素的结果。这个过程显然存在量化误差。<br>  一般情况下，heatmap 与原图的比例为 1：4，heatmap上一个像素对应原图4个像素，通过上面的操作，输出结果的最小间隔由1像素变成了0.5像素，等价于heatmap上一个像素对应原图2个像素，因此可以消除一定的误差。<br>  论文分析结果，现有方法在输出的heatmap上，产生关键点位置的误差分布服从 E=0.125，V=1/192。映射到原图后，误差与heatmap上的误差线性相关，系数为 $ \frac{bw_{s}}{W_{o}} $ （ $ bw_{s} $为 ROI 的宽像素数，$ W_{o} $为heatmap的宽像素数）<br>  结合上一节 flip test 中的坐标系转换偏差分析，最终分析得到，坐标系转换和关键点格式转换两部分引入的偏差，分布服从 E=0.125，V=1/48。  </p><h3 id="1-4-无偏数据处理"><a href="#1-4-无偏数据处理" class="headerlink" title="1.4 无偏数据处理"></a>1.4 无偏数据处理</h3><p>  作者发现姿态估计中这两种误差后，分别提出了消除误差的无偏数据处理方法，推荐看原论文中的公式。  </p><h4 id="1-4-1-无偏坐标系统转换"><a href="#1-4-1-无偏坐标系统转换" class="headerlink" title="1.4.1 无偏坐标系统转换"></a>1.4.1 无偏坐标系统转换</h4><p>  坐标系转换的误差主要来源于，传统方法没有注意到连续数据和离散数据的表示区别，在原图坐标系、网络输入图像坐标系、heatmap图像坐标系三者之间的转换过程，图像并没有对齐。论文提出的方法是将传统方法中缩放时的因数，由之前的 像素数比 改成 像素长度比 ，也就是分子分母同时减1。（例如上面提到的例子中的第一步，关键点的坐标应当乘以 11/17，而不是 12/18）  </p><h4 id="1-4-2无偏关键点格式转换"><a href="#1-4-2无偏关键点格式转换" class="headerlink" title="1.4.2无偏关键点格式转换"></a>1.4.2无偏关键点格式转换</h4><p>  关键点格式转换的误差主要来源于量化操作，作者提出的方法是，在现有的基础上，引入回归loss，回归关键点在heatmap附近的offset。  </p><h2 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h2><p>  <code>COCO val</code></p><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input size</th><th>IPS/PPS</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td><strong>Bottom-up methods</strong></td></tr><tr><td>HigherHRNet [26]</td><td>HRNet-W32</td><td>512x512</td><td>0.8</td><td>64.4</td><td>-</td><td>-</td><td>57.1</td><td>75.6</td><td>-</td></tr><tr><td>+UDP</td><td>HRNet-W32</td><td>512x512</td><td>4.9 (×6.1)</td><td>67.0 (+2.6)</td><td>86.2</td><td>72.0</td><td>60.7</td><td>76.7</td><td>71.6</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W32</td><td>512x512</td><td>1.1</td><td>67.1</td><td>86.2</td><td>73.0</td><td>61.5</td><td>76.1</td><td>718</td></tr><tr><td>+UDP</td><td>HigherHRNet-W32</td><td>512x512</td><td>2.9 (×2.6)</td><td>67.8 (+0.7)</td><td>86.2</td><td>72.9</td><td>62.2</td><td>76.4</td><td>72.4</td></tr><tr><td>HigherHRNet [26]†</td><td>HRNet-W48</td><td>640x640</td><td>0.6</td><td>67.9</td><td>86.7</td><td>74.4</td><td>62.5</td><td>76.2</td><td>73.0</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>640x640</td><td>4.1 (×6.8)</td><td>68.9 (+1.0)</td><td>87.3</td><td>74.9</td><td>64.1</td><td>76.1</td><td>73.5</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W48</td><td>640x640</td><td>0.75</td><td>69.9</td><td>87.2</td><td>76.1</td><td>65.4</td><td>76.4</td><td>-</td></tr><tr><td>+UDP</td><td>HigherHRNet-W48</td><td>640x640</td><td>2.7 (×3.6)</td><td>69.9</td><td>87.3</td><td>76.2</td><td>65.9</td><td>76.2</td><td>74.4</td></tr><tr><td><strong>Bottom-up methods with multi-scale</strong> ([×2, ×1,×0.5]) test as in HigherHRNet [26]</td></tr><tr><td>UDP</td><td>HRNet-W32</td><td>512x512</td><td>-</td><td>70.4</td><td>88.2</td><td>75.8</td><td>65.3</td><td>77.6</td><td>74.7</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W32</td><td>512x512</td><td>-</td><td>69.9</td><td>87.1</td><td>76.0</td><td>65.3</td><td>77.0</td><td>-</td></tr><tr><td>+UDP</td><td>HigherHRNet-W32</td><td>512x512</td><td>-</td><td>70.2 (+0.3)</td><td>88.1</td><td>76.2</td><td>65.4</td><td>77.4</td><td>74.5</td></tr><tr><td>HigherHRNet [26]†</td><td>HRNet-W48</td><td>640x640</td><td>-</td><td>71.6</td><td>88.6</td><td>77.9</td><td>67.5</td><td>77.8</td><td>76.3</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>640x640</td><td>-</td><td>71.3 (-0.3)</td><td>89.0</td><td>77.1</td><td>66.9</td><td>77.7</td><td>75.7</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W48</td><td>640x640</td><td>-</td><td>72.1</td><td>88.4</td><td>78.2</td><td>67.8</td><td>78.3</td><td>-</td></tr><tr><td>+UDP</td><td>HigherHRNet-W48</td><td>640x640</td><td>-</td><td>71.5 (-0.6)</td><td>88.3</td><td>77.3</td><td>67.9</td><td>77.2</td><td>75.9</td></tr><tr><td><strong>Top-down methods</strong></td></tr><tr><td>Hourglass [40]</td><td>Hourglass</td><td>256x192</td><td>-</td><td>66.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CPN [27]</td><td>ResNet-50</td><td>256x192</td><td>-</td><td>69.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CPN [27]</td><td>ResNet-50</td><td>384x288</td><td>-</td><td>71.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MSPN [28]</td><td>MSPN</td><td>256x192</td><td>-</td><td>75.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SimpleBaseline [24]</td><td>ResNet-50</td><td>256x192</td><td>23.0</td><td>71.3</td><td>89.9</td><td>78.9</td><td>68.3</td><td>77.4</td><td>76.9</td></tr><tr><td>+UDP</td><td>ResNet-50</td><td>256x192</td><td>23.0</td><td>72.9(+1.6)</td><td>90.0</td><td>80.2</td><td>69.7</td><td>79.3</td><td>78.2</td></tr><tr><td>SimpleBaseline [24]</td><td>ResNet-152</td><td>256x192</td><td>11.5</td><td>72.9</td><td>90.6</td><td>80.8</td><td>69.9</td><td>79.0</td><td>78.3</td></tr><tr><td>+UDP</td><td>ResNet-152</td><td>256x192</td><td>11.5</td><td>74.3(+1.4)</td><td>90.9</td><td>81.6</td><td>71.2</td><td>80.6</td><td>79.6</td></tr><tr><td>SimpleBaseline [24]</td><td>ResNet-50</td><td>384x288</td><td>20.3</td><td>73.2</td><td>90.7</td><td>79.9</td><td>69.4</td><td>80.1</td><td>78.2</td></tr><tr><td>+UDP</td><td>ResNet-50</td><td>384x288</td><td>20.3</td><td>74.0(+0.8)</td><td>90.3</td><td>80.0</td><td>70.2</td><td>81.0</td><td>79.0</td></tr><tr><td>SimpleBaseline [24]</td><td>ResNet-152</td><td>384x288</td><td>11.1</td><td>75.3</td><td>91.0</td><td>82.3</td><td>71.9</td><td>82.0</td><td>80.4</td></tr><tr><td>+UDP</td><td>ResNet-152</td><td>384x288</td><td>11.1</td><td>76.2(+0.9)</td><td>90.8</td><td>83.0</td><td>72.8</td><td>82.9</td><td>81.2</td></tr><tr><td>HRNet [25]</td><td>HRNet-W32</td><td>256x192</td><td>6.9</td><td>75.6</td><td>91.9</td><td>83.0</td><td>72.2</td><td>81.6</td><td>80.5</td></tr><tr><td>+UDP</td><td>HRNet-W32</td><td>256x192</td><td>6.9</td><td>76.8(+1.2)</td><td>91.9</td><td>83.7</td><td>73.1</td><td>83.3</td><td>81.6</td></tr><tr><td>HRNet [25]</td><td>HRNet-W48</td><td>256x192</td><td>6.3</td><td>75.9</td><td>91.9</td><td>83.5</td><td>72.6</td><td>82.1</td><td>80.9</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>256x192</td><td>6.3</td><td>77.2(+1.3)</td><td>91.8</td><td>83.7</td><td>73.8</td><td>83.7</td><td>82.0</td></tr><tr><td>HRNet [25]</td><td>HRNet-W32</td><td>384x288</td><td>6.2</td><td>76.7</td><td>91.9</td><td>83.6</td><td>73.2</td><td>83.2</td><td>81.6</td></tr><tr><td>+UDP</td><td>HRNet-W32</td><td>384x288</td><td>6.2</td><td>77.8(+1.1)</td><td>91.7</td><td>84.5</td><td>74.2</td><td>84.3</td><td>82.4</td></tr><tr><td>HRNet [25]</td><td>HRNet-W48</td><td>384x288</td><td>5.3</td><td>77.1</td><td>91.8</td><td>83.8</td><td>73.5</td><td>83.5</td><td>81.8</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>384x288</td><td>5.3</td><td>77.8(+0.7)</td><td>92.0</td><td>84.3</td><td>74.2</td><td>84.5</td><td>82.5</td></tr></tbody></table></div><p>  <code>COCO test-dev</code></p><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input size</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td><strong>Bottom-up methods</strong></td></tr><tr><td>AE [49]</td><td>Hourglass [40]</td><td>512x512</td><td>56.6</td><td>81.8</td><td>61.8</td><td>49.8</td><td>67.0</td><td>-</td></tr><tr><td>G-RMI [47]</td><td>ResNet-101</td><td>353x257</td><td>64.9</td><td>85.5</td><td>71.3</td><td>62.3</td><td>70.0</td><td>69.7</td></tr><tr><td>PersonLab [51]</td><td>ResNet-152</td><td>1401x140</td><td>66.5</td><td>88.0</td><td>72.6</td><td>62.4</td><td>72.3</td><td>-</td></tr><tr><td>PifPaf [61]</td><td>-</td><td>-</td><td>66.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HigherHRNet [26]</td><td>HRNet-W32</td><td>512x512</td><td>64.1</td><td>86.3</td><td>70.4</td><td>57.4</td><td>73.9</td><td>-</td></tr><tr><td>+UDP</td><td>HRNet-W32</td><td>512x512</td><td>66.8 (+2.7)</td><td>88.2</td><td>73.0</td><td>61.1</td><td>75.0</td><td>71.5</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W32</td><td>512x512</td><td>66.4</td><td>87.5</td><td>72.8</td><td>61.2</td><td>74.2</td><td>-</td></tr><tr><td>+UDP</td><td>HigherHRNet-W32</td><td>512x512</td><td>67.2 (+0.8)</td><td>88.1</td><td>73.6</td><td>62.0</td><td>74.3</td><td>72.0</td></tr><tr><td>HigherHRNet [26]†</td><td>HRNet-W48</td><td>640x640</td><td>67.4</td><td>88.6</td><td>74.2</td><td>62.6</td><td>74.3</td><td>72.8</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>640x640</td><td>68.1 (+0.2)</td><td>88.3</td><td>74.6</td><td>63.9</td><td>74.1</td><td>73.1</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W48</td><td>640x640</td><td>68.4</td><td>88.2</td><td>75.1</td><td>64.4</td><td>74.2</td><td>-</td></tr><tr><td>+UDP</td><td>HigherHRNet-W48</td><td>640x640</td><td>68.6 (+0.2)</td><td>88.2</td><td>75.5</td><td>65.0</td><td>74.0</td><td>73.5</td></tr><tr><td><strong>Bottom-up methods with multi-scale</strong> ([×2, ×1,×0.5]) test as in HigherHRNet [26]</td></tr><tr><td>UDP</td><td>HRNet-W32</td><td>512x512</td><td>69.3</td><td>89.2</td><td>76.0</td><td>64.8</td><td>76.0</td><td>74.1</td></tr><tr><td>HigherHRNet [26]†</td><td>HRNet-W32</td><td>512x512</td><td>68.8</td><td>88.8</td><td>75.7</td><td>64.4</td><td>75.0</td><td>73.5</td></tr><tr><td>UDP</td><td>HigherHRNet-W32</td><td>512x512</td><td>69.1</td><td>89.1</td><td>75.8</td><td>64.4</td><td>75.5</td><td>73.8</td></tr><tr><td>HigherHRNet [26]†</td><td>HRNet-W48</td><td>640x640</td><td>70.4</td><td>89.7</td><td>77.4</td><td>66.4</td><td>75.7</td><td>75.2</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>640x640</td><td>70.3</td><td>90.1</td><td>76.7</td><td>66.6</td><td>75.3</td><td>75.1</td></tr><tr><td>HigherHRNet [26]</td><td>HigherHRNet-W48</td><td>640x640</td><td>70.5</td><td>89.3</td><td>77.2</td><td>66.6</td><td>75.8</td><td>-</td></tr><tr><td>+UDP</td><td>HigherHRNet-W48</td><td>640x640</td><td>70.5</td><td>89.4</td><td>77.0</td><td>66.8</td><td>75.4</td><td>75.1</td></tr><tr><td><strong>Top-down methods</strong></td></tr><tr><td>Mask-RCNN [53]</td><td>ResNet-50-FPN [62]</td><td>-</td><td>63.1</td><td>87.3</td><td>68.7</td><td>57.8</td><td>71.4</td><td>-</td></tr><tr><td>Integral Pose Regression [63]</td><td>ResNet-101 [64]</td><td>256x256</td><td>67.8</td><td>88.2</td><td>74.8</td><td>63.9</td><td>74.0</td><td>-</td></tr><tr><td>SCN [56]</td><td>Hourglass [40]</td><td>-</td><td>70.5</td><td>88.0</td><td>76.9</td><td>66.0</td><td>77.0</td><td>-</td></tr><tr><td>CPN [27]</td><td>ResNet-Inception</td><td>384x288</td><td>72.1</td><td>91.4</td><td>80.0</td><td>68.7</td><td>77.2</td><td>78.5</td></tr><tr><td>RMPE [65]</td><td>PyraNet [66]</td><td>320x256</td><td>72.3</td><td>89.2</td><td>79.1</td><td>68.0</td><td>78.6</td><td>-</td></tr><tr><td>CFN [67]</td><td>-</td><td>-</td><td>72.6</td><td>86.1</td><td>69.7</td><td>78.3</td><td>64.1</td><td>-</td></tr><tr><td>CPN(ensemble) [27]</td><td>ResNet-Inception</td><td>384x288</td><td>73.0</td><td>91.7</td><td>80.9</td><td>69.5</td><td>78.1</td><td>79.0</td></tr><tr><td>Posefix [68]</td><td>ResNet-152</td><td>384x288</td><td>73.6</td><td>90.8</td><td>81.0</td><td>70.3</td><td>79.8</td><td>79.0</td></tr><tr><td>CSANet [69]</td><td>ResNet-152</td><td>384x288</td><td>74.5</td><td>91.7</td><td>82.1</td><td>71.2</td><td>80.2</td><td>80.7</td></tr><tr><td>MSPN [28]</td><td>MSPN [28]</td><td>384x288</td><td>76.1</td><td>93.4</td><td>83.8</td><td>72.3</td><td>81.5</td><td>81.6</td></tr><tr><td>SimpleBaseline [27]</td><td>ResNet-50</td><td>256x192</td><td>70.2</td><td>90.9</td><td>78.3</td><td>67.1</td><td>75.9</td><td>75.8</td></tr><tr><td>+UDP</td><td>ResNet-50</td><td>256x192</td><td>71.7 (+1.5)</td><td>91.1</td><td>79.6</td><td>68.6</td><td>77.5</td><td>77.2</td></tr><tr><td>SimpleBaseline [27]</td><td>ResNet-50</td><td>384x288</td><td>71.3</td><td>91.0</td><td>78.5</td><td>67.3</td><td>77.9</td><td>76.6</td></tr><tr><td>+UDP</td><td>ResNet-50</td><td>384x288</td><td>72.5 (+1.2)</td><td>91.1</td><td>79.7</td><td>68.8</td><td>79.1</td><td>77.9</td></tr><tr><td>SimpleBaseline [27]</td><td>ResNet-152</td><td>256x192</td><td>71.9</td><td>91.4</td><td>80.1</td><td>68.9</td><td>77.4</td><td>77.5</td></tr><tr><td>+UDP</td><td>ResNet-152</td><td>256x192</td><td>72.9 (+1.0)</td><td>91.6</td><td>80.9</td><td>70.0</td><td>78.5</td><td>78.4</td></tr><tr><td>SimpleBaseline [27]</td><td>ResNet-152</td><td>384x288</td><td>73.8</td><td>91.7</td><td>81.2</td><td>70.3</td><td>80.0</td><td>79.1</td></tr><tr><td>+UDP</td><td>ResNet-152</td><td>384x288</td><td>74.7 (+0.9)</td><td>91.8</td><td>82.1</td><td>71.5</td><td>80.8</td><td>80.0</td></tr><tr><td>HRNet [25]</td><td>HRNet-W32</td><td>256x192</td><td>73.5</td><td>92.2</td><td>82.0</td><td>70.4</td><td>79.0</td><td>79.0</td></tr><tr><td>+UDP</td><td>HRNet-W32</td><td>256x192</td><td>75.2 (+1.7)</td><td>92.4</td><td>82.9</td><td>72.0</td><td>80.8</td><td>80.4</td></tr><tr><td>HRNet [25]</td><td>HRNet-W32</td><td>384x288</td><td>74.9</td><td>92.5</td><td>82.8</td><td>71.3</td><td>80.9</td><td>80.1</td></tr><tr><td>+UDP</td><td>HRNet-W32</td><td>384x288</td><td>76.1 (+1.2)</td><td>92.5</td><td>83.5</td><td>72.8</td><td>82.0</td><td>81.3</td></tr><tr><td>HRNet [25]</td><td>HRNet-W48</td><td>256x192</td><td>74.3</td><td>92.4</td><td>82.6</td><td>71.2</td><td>79.6</td><td>79.7</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>256x192</td><td>75.7 (+1.4)</td><td>92.4</td><td>83.3</td><td>72.5</td><td>81.4</td><td>80.9</td></tr><tr><td>HRNet [25]</td><td>HRNet-W48</td><td>384x288</td><td>75.5</td><td>92.5</td><td>83.3</td><td>71.9</td><td>81.5</td><td>80.5</td></tr><tr><td>+UDP</td><td>HRNet-W48</td><td>384x288</td><td>76.5 (+1.0)</td><td>92.7</td><td>84.0</td><td>73.0</td><td>82.4</td><td>81.6</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>poseplugin</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
    <link href="/2021/01/13/Paper-Notes-Pose-Estimation-20210113-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/"/>
    <url>/2021/01/13/Paper-Notes-Pose-Estimation-20210113-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation"><a href="#Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation" class="headerlink" title="Distribution-Aware Coordinate Representation for Human Pose Estimation"></a><a href="https://arxiv.org/pdf/1910.06278v1.pdf">Distribution-Aware Coordinate Representation for Human Pose Estimation</a></h1><blockquote><ol><li>这篇文章从 <strong>关节坐标表征</strong> 方向研究，发现传统方法中的 coordinate encoding （关键点转化为高斯热力图）过程存在量化误差； coordinate decoding（网络输出热力图转化为关键点坐标）过程对 shift 操作的依赖很高。<br>作者提出 Distribution-Aware coordinate Representation of Keypoints (DARK) 方法，在 关节坐标表征（coordinate Representation）层面提升现有方法的效果。</li><li>DARK 方法可以不改变现有算法的条件下，无缝提升算法效果。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>Coordinate Decoding<br>现有方法： 坐标极值点，向第二极值点偏移1/4像素。<br>DARK：  <ul><li>假设网络输出服从高斯分布，通过求解一阶导数零点计算中心点，根据论文公式9，中心点坐标可以表示为极值点坐标及其附近梯度值的函数。</li><li>Heatmap distribution modulation： 网络输出的 heatmap 通常不够平滑，文章提出 Heatmap distribution modulation 的方法，使用一个 Gaussian kernel 在原始特征图上进行卷积操作，Modulated heatmap 更有利于 Coordinate Decoding  </li></ul></li><li>Coordinate Encoding<br>现有方法：将 keypoint 坐标量化，然后以坐标点为中心生成高斯分布的 heatmap<br>DARK：  取消量化操作，计算 heatmap 时，heatmap 中心点的坐标以浮点数表示。</li></ul><h2 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h2><p>  <code>COCO test-dev</code></p><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input size</th><th>#Params</th><th>GFLOPs</th><th>AP</th><th>AP50</th><th>AP75</th><th>AP M</th><th>AP L</th><th>AR</th></tr></thead><tbody><tr><td>G-RMI[23]</td><td>ResNet-101</td><td>353 × 257</td><td>42.6M</td><td>57.0</td><td>64.9</td><td>85.5</td><td>71.3</td><td>62.3</td><td>70.0</td><td>69.7</td></tr><tr><td>IPR [27]</td><td>ResNet-101</td><td>256 × 256</td><td>45.1M</td><td>11.0</td><td>67.8</td><td>88.2</td><td>74.8</td><td>63.9</td><td>74.0</td><td>-</td></tr><tr><td>CPN [6]</td><td>ResNet-Inception</td><td>384 × 288</td><td>-</td><td>-</td><td>72.1</td><td>91.4</td><td>80.0</td><td>68.7</td><td>77.2</td><td>78.5</td></tr><tr><td>RMPE [11]</td><td>PyraNet</td><td>320 × 256</td><td>28.1M</td><td>26.7</td><td>72.3</td><td>89.2</td><td>79.1</td><td>68.0</td><td>78.6</td><td>-</td></tr><tr><td>CFN [13]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>72.6</td><td>86.1</td><td>69.7</td><td>78.3</td><td>64.1</td><td>-</td></tr><tr><td>CPN (ensemble) [6]</td><td>ResNet-Inception</td><td>384 × 288</td><td>-</td><td>-</td><td>73.0</td><td>91.7</td><td>80.9</td><td>69.5</td><td>78.1</td><td>79.0</td></tr><tr><td>SimpleBaseline[33]</td><td>ResNet-152</td><td>384 × 288</td><td>68.6M</td><td>35.6</td><td>73.7</td><td>91.9</td><td>81.1</td><td>70.3</td><td>80.0</td><td>79.0</td></tr><tr><td>HRNet[25]</td><td>HRNet-W32</td><td>384 × 288</td><td>28.5M</td><td>16.0</td><td>74.9</td><td>92.5</td><td>82.8</td><td>71.3</td><td>80.9</td><td>80.1</td></tr><tr><td>HRNet[25]</td><td>HRNet-W48</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>75.5</td><td>92.5</td><td>83.3</td><td>71.9</td><td>81.5</td><td>80.5</td></tr><tr><td>DARK</td><td>HRNet-W32</td><td>128 × 96</td><td>28.5M</td><td>1.8</td><td>70.0</td><td>90.9</td><td>78.5</td><td>67.4</td><td>75.0</td><td>75.9</td></tr><tr><td>DARK</td><td>HRNet-W48</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>76.2</td><td>92.5</td><td>83.6</td><td>72.5</td><td>82.4</td><td>81.1</td></tr><tr><td>G-RMI (extra data)</td><td>ResNet-101</td><td>353 × 257</td><td>42.6M</td><td>57.0</td><td>68.5</td><td>87.1</td><td>75.5</td><td>65.8</td><td>73.3</td><td>73.3</td></tr><tr><td>HRNet (extra data)</td><td>HRNet-W48</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>77.0</td><td>92.7</td><td>84.5</td><td>73.4</td><td>83.1</td><td>82.0</td></tr><tr><td>DARK (extra data)</td><td>HRNet-W48</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>77.4</td><td>92.6</td><td>84.6</td><td>73.6</td><td>83.7</td><td>82.3</td></tr></tbody></table></div><p>  <code>MPII</code></p><div class="table-container"><table><thead><tr><th>Method</th><th>Head</th><th>Sho.</th><th>Elb.</th><th>Wri.</th><th>Hip</th><th>Kne.</th><th>Ank.</th><th>Mean</th></tr></thead><tbody><tr><td>HRN32</td><td>97.1</td><td>95.9</td><td>90.3</td><td>86.5</td><td>89.1</td><td>87.1</td><td>83.3</td><td>90.3</td></tr><tr><td>DARK</td><td>97.2</td><td>95.9</td><td>91.2</td><td>86.7</td><td>89.7</td><td>86.7</td><td>84.0</td><td>90.6</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>poseplugin</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PoseFix: Model-agnostic General Human Pose Refinement Network</title>
    <link href="/2021/01/13/Paper-Notes-Pose-Estimation-20210113-PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network/"/>
    <url>/2021/01/13/Paper-Notes-Pose-Estimation-20210113-PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network"><a href="#PoseFix-Model-agnostic-General-Human-Pose-Refinement-Network" class="headerlink" title="PoseFix: Model-agnostic General Human Pose Refinement Network"></a><a href="https://arxiv.org/abs/1812.03595.pdf">PoseFix: Model-agnostic General Human Pose Refinement Network</a></h1><blockquote><ol><li>现有的 Pose Estimation 方法的 error 具有相似的分布，收集这些 error statistics 作为先验知识，结合标注数据，可以合成 Pose 数据。<br>利用合成 Pose 数据训练网络，就可以让网络学习 error statistics，进而 refine 其他网络的 Pose 结果。</li><li>PoseFix 可对任意模型进行 refine，实验结果表明对多种模型都有提升效果。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li><p>Synthesizing poses for training<br>Pose 关键点一共有 5 中类型：</p><ul><li>Good： 预测结果完全正确  </li><li>Jitter： 预测结果与 GT 有小距离偏差  </li><li>Inversion： 预测结果将对称关键点预测错误  </li><li>Swap： 预测结果将关键点标注到旁边的人体目标上  </li><li>Miss： 预测结果与 GT 有大距离偏差<br>根据之前的研究，Pose Estimation 中这 5 种类型关键点服从一定的分布，依据不同类型关键点出现的频率，随机合成 Pose，然后输入 PoseFix 训练。</li></ul></li><li><p>Architecture and learning of PoseFix<br>PoseFix 将 原始图像 和 合成Pose 同时输入网络，网络输出 refined pose。PoseFix 采用和 simplebaseline 相同的骨干网络。<br>PoseFix 利用 Coarse-to-fine 的模式进行预测，网络输入的 合成Pose 为粗粒度结果，网络输出为 精细化结果。refined pose 被两个 Loss 约束。  </p><ul><li>L(H): 分别在 x方向 和 y方向 上计算 交叉熵  </li><li>L(C): 利用 softmax 函数，计算出对应的 Pose 坐标 C，然后计算 C 与 GT 的 L1 距离。<br>L(H) 约束网络只产生一个关键点，L(C) 约束网络输出的关键点坐标更精确。</li></ul></li></ul><h2 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h2><p>  <code>COCO test-dev</code></p><div class="table-container"><table><thead><tr><th>Methods</th><th>AP</th><th>AP.50</th><th>AP.75</th><th>APM</th><th>APL</th><th>AR</th><th>AR.50</th><th>AR.75</th><th>ARM</th><th>ARL</th></tr></thead><tbody><tr><td>AE [18]</td><td>56.6</td><td>81.7</td><td>62.1</td><td>48.1</td><td>69.4</td><td>62.5</td><td>84.9</td><td>67.2</td><td>52.2</td><td>76.5</td></tr><tr><td>+ PoseFix (Ours)</td><td>63.9</td><td>83.6</td><td>70.0</td><td>56.9</td><td>73.7</td><td>69.1</td><td>86.6</td><td>74.2</td><td>61.1</td><td>79.9</td></tr><tr><td>PAFs [4]</td><td>61.7</td><td>84.9</td><td>67.4</td><td>57.1</td><td>68.1</td><td>66.5</td><td>87.2</td><td>71.7</td><td>60.5</td><td>74.6</td></tr><tr><td>+ PoseFix (Ours)</td><td>66.7</td><td>85.7</td><td>72.9</td><td>62.9</td><td>72.3</td><td>71.3</td><td>88.0</td><td>76.7</td><td>66.3</td><td>78.1</td></tr><tr><td>Mask R-CNN (ResNet-50) [9]</td><td>62.9</td><td>87.1</td><td>68.9</td><td>57.6</td><td>71.3</td><td>69.7</td><td>91.3</td><td>75.1</td><td>63.9</td><td>77.6</td></tr><tr><td>+ PoseFix (Ours)</td><td>67.2</td><td>88.0</td><td>73.5</td><td>62.5</td><td>75.1</td><td>74.0</td><td>92.2</td><td>79.6</td><td>68.8</td><td>81.1</td></tr><tr><td>Mask R-CNN (ResNet-101)</td><td>63.4</td><td>87.5</td><td>69.4</td><td>57.8</td><td>72.0</td><td>70.2</td><td>91.8</td><td>75.6</td><td>64.3</td><td>78.2</td></tr><tr><td>+ PoseFix (Ours)</td><td>67.5</td><td>88.4</td><td>73.8</td><td>62.6</td><td>75.5</td><td>74.3</td><td>92.6</td><td>79.9</td><td>69.1</td><td>81.4</td></tr><tr><td>Mask R-CNN (ResNeXt-101-64)</td><td>64.9</td><td>88.6</td><td>71.0</td><td>59.6</td><td>73.3</td><td>71.4</td><td>92.4</td><td>76.8</td><td>65.9</td><td>78.9</td></tr><tr><td>+ PoseFix (Ours)</td><td>68.7</td><td>89.3</td><td>75.2</td><td>64.1</td><td>76.4</td><td>75.2</td><td>93.1</td><td>80.9</td><td>70.3</td><td>81.9</td></tr><tr><td>Mask R-CNN (ResNeXt-101-32)</td><td>64.9</td><td>88.4</td><td>70.9</td><td>59.5</td><td>73.2</td><td>71.3</td><td>92.2</td><td>76.7</td><td>65.8</td><td>78.9</td></tr><tr><td>+ PoseFix (Ours)</td><td>68.5</td><td>88.9</td><td>75.0</td><td>64.0</td><td>76.2</td><td>75.0</td><td>92.9</td><td>80.7</td><td>70.1</td><td>81.8</td></tr><tr><td>IntegralPose</td><td>66.3</td><td>87.6</td><td>72.9</td><td>62.7</td><td>72.7</td><td>73.2</td><td>91.8</td><td>79.1</td><td>68.3</td><td>79.8</td></tr><tr><td>+ PoseFix (Ours)</td><td>69.5</td><td>88.3</td><td>75.9</td><td>65.7</td><td>76.1</td><td>75.9</td><td>92.4</td><td>81.8</td><td>71.1</td><td>82.5</td></tr><tr><td>CPN (ResNet-50) [6]</td><td>68.6</td><td>89.6</td><td>76.7</td><td>65.3</td><td>74.6</td><td>75.6</td><td>93.7</td><td>82.6</td><td>70.8</td><td>82.0</td></tr><tr><td>+ PoseFix (Ours)</td><td>71.8</td><td>89.8</td><td>78.9</td><td>68.3</td><td>78.1</td><td>78.2</td><td>93.9</td><td>84.3</td><td>73.5</td><td>84.6</td></tr><tr><td>CPN (ResNet-101)</td><td>69.6</td><td>89.9</td><td>77.6</td><td>66.3</td><td>75.6</td><td>76.6</td><td>93.9</td><td>83.5</td><td>72.0</td><td>82.9</td></tr><tr><td>+ PoseFix (Ours)</td><td>72.6</td><td>90.2</td><td>79.7</td><td>69.0</td><td>78.9</td><td>78.9</td><td>94.1</td><td>85.0</td><td>74.2</td><td>85.1</td></tr><tr><td>Simple (ResNet-50) [28]</td><td>69.4</td><td>90.1</td><td>77.4</td><td>66.2</td><td>75.5</td><td>75.1</td><td>93.9</td><td>82.4</td><td>70.8</td><td>81.0</td></tr><tr><td>+ PoseFix (Ours)</td><td>72.5</td><td>90.5</td><td>79.6</td><td>68.9</td><td>79.0</td><td>78.0</td><td>94.1</td><td>84.4</td><td>73.4</td><td>84.1</td></tr><tr><td>Simple (ResNet-101)</td><td>70.5</td><td>90.7</td><td>78.8</td><td>67.5</td><td>76.3</td><td>76.2</td><td>94.3</td><td>83.7</td><td>72.1</td><td>81.9</td></tr><tr><td>+ PoseFix (Ours)</td><td>73.3</td><td>90.8</td><td>80.7</td><td>69.8</td><td>79.8</td><td>78.7</td><td>94.4</td><td>85.3</td><td>74.3</td><td>84.8</td></tr><tr><td>Simple (ResNet-152)</td><td>71.1</td><td>90.7</td><td>79.4</td><td>68.0</td><td>76.9</td><td>76.8</td><td>94.4</td><td>84.3</td><td>72.6</td><td>82.4</td></tr><tr><td>+ PoseFix (Ours)</td><td>73.6</td><td>90.8</td><td>81.0</td><td>70.3</td><td>79.8</td><td>79.0</td><td>94.4</td><td>85.7</td><td>74.8</td><td>84.9</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>poseplugin</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking on Multi-Stage Networks for Human Pose Estimation</title>
    <link href="/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation/"/>
    <url>/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation"><a href="#Rethinking-on-Multi-Stage-Networks-for-Human-Pose-Estimation" class="headerlink" title="Rethinking on Multi-Stage Networks for Human Pose Estimation"></a><a href="https://arxiv.org/pdf/1901.00148.pdf">Rethinking on Multi-Stage Networks for Human Pose Estimation</a></h1><blockquote><ol><li>作者将现有的 Pose Estimation 方法分为 single-stage 和 multi-stage 两类。并且认为现有的 multi-stage 方法的不足之处在于模型设计缺陷。<br>文章在经典的 stack hourglass 模型的基础上，提出三个方向的改进：1) 单阶段模型设计，2) 跨阶段特征聚合，3) coarse-to-fine 监督学习。  </li><li>本文设计的 Multi Stage Pose Network 在 COCO test-dev 上取得 78.1% 的准确率，在 MPII test 上取得 92.6% 的准确率。<a id="more"></a></li></ol></blockquote><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>  <strong>单阶段模型</strong>从图像分类领域迁移而来，一般基于单个骨干网络。<br>  <strong>多阶段模型</strong>由若干阶段组成，每个阶段是一个轻量级的网络，网络包含上采样和下采样通路。<br>  根据现有工作。单阶段模型和多阶段模型的优劣无法区分。</p><ul><li>improvements<ul><li>多阶段模型中，每个阶段的 module 设计不够好。</li><li>提出跨阶段的特征聚合方法，加强 information flow。</li><li>采用由粗到细的监督学习方法。</li></ul></li></ul><h2 id="2-Method"><a href="#2-Method" class="headerlink" title="2. Method"></a>2. Method</h2><ul><li>Analysis of a Single-Stage Module<br>Hourglass 设计中，网络下采样和上采样过程中，channel 数一致为常数，导致大量信息丢失。<br>现有改进通常会在 下采样和上采样过程中增加 channel 数，下采样过程 [256, 386, 512, 768]，上采样过程 [768, 512, 386, 256]。<br>MSPN 提出，上采样过程中不需要增加 channel 数，维持较少的 channel，即下采样过程 [256, 386, 512, 768]，上采样过程 [256, 256, 256, 256]。  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Res-50</th><th>Res-101</th><th>Res-152</th><th>Res-254</th></tr></thead><tbody><tr><td>AP</td><td>71.5</td><td>73.1</td><td>73.6</td><td>74.0</td></tr><tr><td>FLOPs(G)</td><td>4.4</td><td>7.5</td><td>11.2</td><td>18.0</td></tr></tbody></table></div><blockquote><p>单阶段模型的上限大约为 74.0。</p></blockquote><div class="table-container"><table><thead><tr><th>Stages</th><th>Hourglass</th><th></th><th>Stages</th><th>MSPN</th><th></th></tr></thead><tbody><tr><td></td><td>FLOPs(G)</td><td>AP</td><td></td><td>FLOPs(G)</td><td>AP</td></tr><tr><td>1</td><td>3.9</td><td>65.4</td><td>1</td><td>4.4</td><td>71.5</td></tr><tr><td>2</td><td>6.2</td><td>70.9</td><td>2</td><td>9.6</td><td>74.5</td></tr><tr><td>4</td><td>10.6</td><td>71.3</td><td>3</td><td>14.7</td><td>75.2</td></tr><tr><td>8</td><td>19.5</td><td>71.6</td><td>4</td><td>19.9</td><td>75.9</td></tr></tbody></table></div><blockquote><p>MSPN 设计的 Single-Stage Module，模型上限能达到 75.9</p></blockquote><div class="table-container"><table><thead><tr><th>Method</th><th>Res-50</th><th>2×Res-18</th><th>L-XCP</th><th>4× S-XCP</th></tr></thead><tbody><tr><td>AP</td><td>71.5</td><td>71.6</td><td>73.7</td><td>74.7</td></tr><tr><td>FLOPs</td><td>4.4G</td><td>4.0G</td><td>6.1G</td><td>5.7G</td></tr></tbody></table></div><blockquote><p>FLOPs 相近的情况下，多阶段模型的效果好于单阶段模型。</p></blockquote><ul><li>Cross Stage Feature Aggregation<br>将 stage(t-1) 的  downsampling 和 upsampling units 特征图，通过 1x1 卷积，连接到 stage(t) 的 downsampling unit，形成 residual 结构，实现 Cross Stage Feature Aggregation。  </li><li>Coarse-to-fine Supervision<br>MSPN 在靠前的阶段，heatmap 使用较大的高斯核，在靠后的阶段，heatmap 使用较小的高斯核，用这种方法构造 GT，监督网络学习。</li></ul><div class="table-container"><table><thead><tr><th>BaseNet</th><th>CTF</th><th>CSFA</th><th>Hourglass</th><th>MSPN</th></tr></thead><tbody><tr><td>√</td><td></td><td></td><td>71.3</td><td>73.3</td></tr><tr><td>√</td><td>√</td><td></td><td>72.5</td><td>74.2</td></tr><tr><td>√</td><td>√</td><td>√</td><td>73.0</td><td>74.5</td></tr></tbody></table></div><blockquote><p>加入 Coarse-to-fine Supervision(CTF) 和 Cross Stage Feature Aggregation(CSFA) 模型的精度提高。</p></blockquote><h2 id="3-Result"><a href="#3-Result" class="headerlink" title="3. Result"></a>3. Result</h2><p>  <code>COCO test-dev</code></p><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input Size</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th><th>AR50</th><th>AR75</th><th>ARM</th><th>ARL</th></tr></thead><tbody><tr><td>CMU Pose [5]</td><td>-</td><td>-</td><td>61.8</td><td>84.9</td><td>67.5</td><td>57.1</td><td>68.2</td><td>66.5</td><td>87.2</td><td>71.8</td><td>60.6</td><td>74.6</td></tr><tr><td>Mask R-CNN [16]</td><td>Res-50-FPN</td><td>-</td><td>63.1</td><td>87.3</td><td>68.7</td><td>57.8</td><td>71.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>G-RMI [31]</td><td>Res-152</td><td>353×257</td><td>64.9</td><td>85.5</td><td>71.3</td><td>62.3</td><td>70.0</td><td>69.7</td><td>88.7</td><td>75.5</td><td>64.4</td><td>77.1</td></tr><tr><td>AE [28]</td><td>-</td><td>512×512</td><td>65.5</td><td>86.8</td><td>72.3</td><td>60.6</td><td>72.6</td><td>70.2</td><td>89.5</td><td>76.0</td><td>64.6</td><td>78.1</td></tr><tr><td>CPN [9]</td><td>Res-Inception</td><td>384×288</td><td>72.1</td><td>91.4</td><td>80.0</td><td>68.7</td><td>77.2</td><td>78.5</td><td>95.1</td><td>85.3</td><td>74.2</td><td>84.3</td></tr><tr><td>Simple Base [46]</td><td>Res-152</td><td>384×288</td><td>73.7</td><td>91.9</td><td>81.1</td><td>70.3</td><td>80.0</td><td>79.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HRNet [39]</td><td>HRNet-W48</td><td>384×288</td><td>75.5</td><td>92.5</td><td>83.3</td><td>71.9</td><td>81.5</td><td>80.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours (MSPN)</td><td>4×Res-50</td><td>384×288</td><td>76.1</td><td>93.4</td><td>83.8</td><td>72.3</td><td>81.5</td><td>81.6</td><td>96.3</td><td>88.1</td><td>77.5</td><td>87.1</td></tr><tr><td>CPN+ [9]</td><td>Res-Inception</td><td>384×288</td><td>73.0</td><td>91.7</td><td>80.9</td><td>69.5</td><td>78.1</td><td>79.0</td><td>95.1</td><td>85.9</td><td>74.8</td><td>84.6</td></tr><tr><td>Simple Base+* [46]</td><td>Res-152</td><td>384×288</td><td>76.5</td><td>92.4</td><td>84.0</td><td>73.0</td><td>82.7</td><td>81.5</td><td>95.8</td><td>88.2</td><td>77.4</td><td>87.2</td></tr><tr><td>HRNet* [39]</td><td>HRNet-W48</td><td>384×288</td><td>77.0</td><td>92.7</td><td>84.5</td><td>73.4</td><td>83.1</td><td>82.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours (MSPN*)</td><td>4×Res-50</td><td>384×288</td><td>77.1</td><td>93.8</td><td>84.6</td><td>73.4</td><td>82.3</td><td>82.3</td><td>96.5</td><td>88.9</td><td>78.4</td><td>87.7</td></tr><tr><td>Ours (MSPN+*)</td><td>4×Res-50</td><td>384×288</td><td>78.1</td><td>94.1</td><td>85.9</td><td>74.5</td><td>83.3</td><td>83.1</td><td>96.7</td><td>89.8</td><td>79.3</td><td>88.2</td></tr></tbody></table></div><p>  <code>COCO test-challenge</code></p><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input Size</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th><th>AR50</th><th>AR75</th><th>ARM</th><th>ARL</th></tr></thead><tbody><tr><td>Mask R-CNN* [16]</td><td>ResX-101-FPN</td><td>-</td><td>68.9</td><td>89.2</td><td>75.2</td><td>63.7</td><td>76.8</td><td>75.4</td><td>93.2</td><td>81.2</td><td>70.2</td><td>82.6</td></tr><tr><td>G-RMI* [31]</td><td>Res-152</td><td>353×257</td><td>69.1</td><td>85.9</td><td>75.2</td><td>66.0</td><td>74.5</td><td>75.1</td><td>90.7</td><td>80.7</td><td>69.7</td><td>82.4</td></tr><tr><td>CPN+ [9]</td><td>Res-Inception</td><td>384×288</td><td>72.1</td><td>90.5</td><td>78.9</td><td>67.9</td><td>78.1</td><td>78.7</td><td>94.7</td><td>84.8</td><td>74.3</td><td>84.7</td></tr><tr><td>Sea Monsters+*</td><td>-</td><td>-</td><td>74.1</td><td>90.6</td><td>80.4</td><td>68.5</td><td>82.1</td><td>79.5</td><td>94.4</td><td>85.1</td><td>74.1</td><td>86.8</td></tr><tr><td>Simple Base+* [46]</td><td>Res-152</td><td>384×288</td><td>74.5</td><td>90.9</td><td>80.8</td><td>69.5</td><td>82.9</td><td>80.5</td><td>95.1</td><td>86.3</td><td>75.3</td><td>87.5</td></tr><tr><td>Ours (MSPN+*)</td><td>4×Res-50</td><td>384×288</td><td>76.4</td><td>92.9</td><td>82.6</td><td>71.4</td><td>83.2</td><td>82.2</td><td>96.0</td><td>87.7</td><td>77.5</td><td>88.6</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>topdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pose Estimation resources</title>
    <link href="/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Pose-Estimation-resources/"/>
    <url>/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Pose-Estimation-resources/</url>
    
    <content type="html"><![CDATA[<h1 id="Awsome-Pose-Estimation"><a href="#Awsome-Pose-Estimation" class="headerlink" title="Awsome Pose Estimation"></a>Awsome Pose Estimation</h1><h2 id="1-Pose-Estimation-Papers"><a href="#1-Pose-Estimation-Papers" class="headerlink" title="1. Pose Estimation Papers"></a>1. Pose Estimation Papers</h2><p>  <a href="https://github.com/wangzheallen/awesome-human-pose-estimation">awesome-human-pose-estimation</a>: a github repo which collects update-to-date <strong>Pose Estimation Papers</strong>.<br><a id="more"></a></p><h2 id="2-Pose-Estimation-SOTA"><a href="#2-Pose-Estimation-SOTA" class="headerlink" title="2. Pose Estimation SOTA"></a>2. Pose Estimation SOTA</h2><p>  <a href="https://paperswithcode.com/task/pose-estimation">Paper With Code</a>: Summary of SOTA over <code>MPII, COCO-val, COCO-test_dev, PoseTrack</code> datasets.  </p><h2 id="3-Pose-Estimation-Codebase"><a href="#3-Pose-Estimation-Codebase" class="headerlink" title="3. Pose Estimation Codebase"></a>3. Pose Estimation Codebase</h2><p>  <a href="https://github.com/open-mmlab/mmpose">MMPose</a>: an open-source toolbox for pose estimation based on PyTorch.<br>  <a href="https://mmpose.readthedocs.io">MMPose Document</a></p><h2 id="4-Pose-Estimation-benchmark"><a href="#4-Pose-Estimation-benchmark" class="headerlink" title="4. Pose Estimation benchmark"></a>4. Pose Estimation benchmark</h2><ul><li><code>COCO test-challenge</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>backbone</th><th>input-size</th><th>AP</th><th>remark</th></tr></thead><tbody><tr><td><strong>bottomup</strong></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>topdown</strong></td></tr><tr><td>G-RMI</td><td>-</td><td></td><td>69.1</td><td>extra data</td></tr><tr><td>CPN</td><td>ResNet-Inception</td><td></td><td>72.1</td><td>ensemble</td></tr><tr><td>DARK</td><td>HRNet-W48</td><td>384x288</td><td>76.4</td><td>extra data &amp; ensemble</td></tr></tbody></table></div><ul><li><code>COCO test-dev</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>backbone</th><th>input-size</th><th>AP</th><th>remark</th></tr></thead><tbody><tr><td><strong>bottomup</strong></td></tr><tr><td>OpenPose</td><td>-</td><td></td><td>61.8</td><td></td></tr><tr><td>AE</td><td>StackHourglass</td><td></td><td>65.0</td><td>multi scale</td></tr><tr><td>AE</td><td>StackHourglass</td><td></td><td>65.5</td><td>refine</td></tr><tr><td>HigherHRNet</td><td>HRNet-W32</td><td>512x512</td><td>66.4</td><td></td></tr><tr><td>HigherHRNet</td><td>HRNet-W48</td><td>640x640</td><td>68.4</td><td></td></tr><tr><td>HigherHRNet</td><td>HRNet-W48</td><td>640x640</td><td>70.5</td><td>multi scale</td></tr><tr><td><strong>topdown</strong></td></tr><tr><td>CPN</td><td>ResNet-Inception</td><td></td><td>72.1</td><td></td></tr><tr><td>CPN</td><td>ResNet-Inception</td><td></td><td>73.0</td><td>ensemble</td></tr><tr><td>RMPE</td><td>PyraNet</td><td>320x256</td><td>72.3</td><td></td></tr><tr><td>SimpleBaseline</td><td>ResNet-152</td><td>384x288</td><td>73.7</td><td></td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td>384x288</td><td>74.9</td><td></td></tr><tr><td>HRNet-W48</td><td>HRNet-W48</td><td>384x288</td><td>75.5</td><td></td></tr><tr><td>HRNet-W48</td><td>HRNet-W48</td><td>384x288</td><td>77.0</td><td>extra data</td></tr><tr><td>EvoPose2D-L</td><td></td><td>512x384</td><td>75.7</td><td></td></tr><tr><td>MSPN</td><td>4×Res-50</td><td>384x288</td><td>76.1</td><td></td></tr><tr><td>MSPN</td><td>4×Res-50</td><td>384x288</td><td>77.1</td><td>extra data</td></tr><tr><td>MSPN</td><td>4×Res-50</td><td>384x288</td><td>78.1</td><td>extra data &amp; ensemble</td></tr><tr><td>DARK</td><td>HRNet-W48</td><td>384x288</td><td>76.2</td><td>75.5(+0.7)</td></tr><tr><td>DARK</td><td>HRNet-W48</td><td>384x288</td><td>77.4</td><td>77.0(+0.4) &amp; extra data</td></tr><tr><td>DARK</td><td>HRNet-W48</td><td>384x288</td><td>78.9</td><td>extra data &amp; ensemble</td></tr><tr><td>UDP</td><td>HRNet-W48</td><td>384x288</td><td>76.5</td><td>75.5(+1.0)</td></tr><tr><td>PoseFix</td><td>HRNet-W48</td><td>384x288</td><td>76.7</td><td>75.5(+1.2)</td></tr><tr><td>PoseFix</td><td>EvoPose2D-L</td><td>512x384</td><td>76.8</td><td>75.7(+1.1)</td></tr></tbody></table></div><ul><li><code>COCO-val</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>backbone</th><th>input-size</th><th>AP</th><th>remark</th></tr></thead><tbody><tr><td><strong>bottomup</strong></td></tr><tr><td>OpenPose</td><td>-</td><td></td><td>58.4</td><td></td></tr><tr><td>OpenPose</td><td>-</td><td></td><td>61.0</td><td>CPM refine</td></tr><tr><td>HigherHRNet</td><td>HRNet-W32</td><td>512x512</td><td>67.1</td><td></td></tr><tr><td>HigherHRNet</td><td>HRNet-W32</td><td>640x640</td><td>68.5</td><td></td></tr><tr><td>HigherHRNet</td><td>HRNet-W48</td><td>640x640</td><td>69.9</td><td></td></tr><tr><td>UDP</td><td>HigherHRNet-W32</td><td>512x512</td><td>67.8</td><td>67.1(+0.7)</td></tr><tr><td>UDP</td><td>HigherHRNet-W48</td><td>640x640</td><td>69.9</td><td>69.9(+0.0)</td></tr><tr><td><strong>topdown</strong></td></tr><tr><td>CPN</td><td>ResNet-Inception</td><td></td><td>72.7</td><td></td></tr><tr><td>CPN</td><td>ResNet-Inception</td><td></td><td>74.5</td><td>ensemble</td></tr><tr><td>MSPN</td><td>4×Res-50</td><td>384x288</td><td>76.4</td><td>extra data &amp; ensemble</td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td>384x288</td><td>75.8</td><td></td></tr><tr><td>HRNet-W48</td><td>HRNet-W48</td><td>384x288</td><td>76.3</td><td></td></tr><tr><td>EvoPose2D-L</td><td></td><td>512x384</td><td>76.6</td><td></td></tr><tr><td>DARK</td><td>HRNet-W32</td><td>384x288</td><td>76.6</td><td>75.8(+0.8)</td></tr><tr><td>PoseFix</td><td>HRNet-W48</td><td>384x288</td><td>77.3</td><td>76.3(+1.0)</td></tr><tr><td>UDP</td><td>HRNet-W48</td><td>384x288</td><td>77.8</td><td>77.1(+0.7) <strong>???</strong></td></tr></tbody></table></div><ul><li><code>MPII test</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>backbone</th><th>input-size</th><th>AP</th><th>remark</th></tr></thead><tbody><tr><td><strong>bottomup</strong></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>topdown</strong></td></tr><tr><td>Stack Hourglass</td><td></td><td></td><td>90.9</td><td></td></tr><tr><td>SimpleBaseline</td><td>ResNet-152</td><td></td><td>91.5</td><td></td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td></td><td>92.3</td></tr></tbody></table></div><ul><li><code>MPII multi-person test</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>backbone</th><th>input-size</th><th>AP</th><th>remark</th></tr></thead><tbody><tr><td><strong>bottomup</strong></td></tr><tr><td>OpenPose</td><td>-</td><td></td><td>72.5</td><td></td></tr><tr><td>OpenPose</td><td>-</td><td></td><td>75.6</td><td>multi-scale</td></tr><tr><td>AE</td><td>StackHourglass</td><td></td><td>77.5</td><td></td></tr><tr><td><strong>topdown</strong></td></tr><tr><td>RMPE</td><td>PyraNet</td><td>320x256</td><td>82.1</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>benchmark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EvoPose2D-Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution</title>
    <link href="/2021/01/12/Paper-Notes-Pose-Estimation-20210112-EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution/"/>
    <url>/2021/01/12/Paper-Notes-Pose-Estimation-20210112-EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution/</url>
    
    <content type="html"><![CDATA[<h1 id="EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution"><a href="#EvoPose2D-Pushing-the-Boundaries-of-2D-Human-Pose-Estimation-using-Neuroevolution" class="headerlink" title="EvoPose2D-Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution"></a>EvoPose2D-Pushing the Boundaries of 2D Human Pose Estimation using Neuroevolution</h1><blockquote><ol><li>本文使用 神经网络进化（neuroevolution） 的方法搜索最优的 2D Human Pose Estimation 模型。文章提出 weight transfer scheme 来加速神经网络进化的速度。</li><li>通过神经网络进化生成的模型，可以比现有的 SOTA 具有更优的效果。其中最优的模型 <code>EvoPose2D-L</code> 相比于 HRNet-W48 只有 1/2.0 的 operations， 1/4.3 的 parameters。</li><li><code>EvoPose2D-L</code> 在 COCO test-dev 数据集上准确率为 75.7%<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>Weight transfer<br>文章提出 Weight transfer 来加速神经网络进化的过程。<br>为了保证突变（mutated）后的 child 网络能够快速收敛，很自然的想法就是使用 parent 的参数对其进行初始化。<br>作者根据 child 和 parent 的 kernel-size 和 channels 大小，定义了四种情况，具有 transfer 的方法参见论文公式。  </li><li>Search space<br>作者根据网络的各种超参，定义了 10e14大小的搜索空间，具有定义方法见论文描述。</li><li>Fitness<br>神经网络进化中，需要考虑 performance 和 effciency 的平衡，文章使用 Pareto optimizations 的优化方法，最小化适应函数 <code>fitness function</code>。函数表达式见论文。</li><li>Evolutionary strategy<br>神经网络进化的过程类似生物进化的流程。从 0 号祖先开始，生成若干个 child，然后找到 m 个 fit 比较好的 child，成为新的祖先，完成一次进化，重复多轮进化，直到 适应函数 收敛时，手动结束网络进化流程。  </li><li>Large-batch training<br>为了使神经网路进化的进程尽量快，文章使用了 large-batchsize 训练，训练过程中参照之前研究的 large-batchsize 训练方法，保证模型可以收敛。  </li><li>Compound scaling<br>同样根据现有的研究结果，网络的 resolution, channels, depth 同步的缩放可以使神经网络进化过程更加高效，作者根据搜索空间中的 resolution 直接计算出 channels 和 depth，实现 Compound scaling。  </li></ul><h2 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h2><ul><li><code>COCO val</code></li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Pretrain</th><th>Input size</th><th>Params (M)</th><th>FLOPs (G)</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td>CPN [9]</td><td>ResNet-50</td><td>Y</td><td>256 × 192</td><td>27.0</td><td>6.20</td><td>68.6</td><td>−</td><td>−</td><td>−</td><td>−</td><td>−</td></tr><tr><td>SimpleBaseline [49]</td><td>ResNet-50</td><td>Y</td><td>256 × 192</td><td>34.0</td><td>5.21†</td><td>70.4</td><td>88.6</td><td>78.3</td><td>67.1</td><td>77.2</td><td>76.3</td></tr><tr><td>SimpleBaseline [49]</td><td>ResNet-101</td><td>Y</td><td>256 × 192</td><td>53.0</td><td>8.84†</td><td>71.4</td><td>89.3</td><td>79.3</td><td>68.1</td><td>78.1</td><td>77.1</td></tr><tr><td>SimpleBaseline [49]</td><td>ResNet-152</td><td>Y</td><td>256 × 192</td><td>68.6</td><td>12.5†</td><td>72.0</td><td>89.3</td><td>79.8</td><td>68.7</td><td>78.9</td><td>77.8</td></tr><tr><td>HRNet-W32 [40]</td><td>-</td><td>N</td><td>256 × 192</td><td>28.5</td><td>7.65†</td><td>73.4</td><td>89.5</td><td>80.7</td><td>70.2</td><td>80.1</td><td>78.9</td></tr><tr><td>HRNet-W32 [40]</td><td>-</td><td>Y</td><td>256 × 192</td><td>28.5</td><td>7.65†</td><td>74.4</td><td>90.5</td><td>81.9</td><td>70.8</td><td>81.0</td><td>79.8</td></tr><tr><td>HRNet-W48 [40]</td><td>-</td><td>Y</td><td>256 × 192</td><td>63.6</td><td>15.7†</td><td>75.1</td><td>90.6</td><td>82.2</td><td>71.5</td><td>81.8</td><td>80.4</td></tr><tr><td>MSPN [25]</td><td>4xResNet-50</td><td>Y</td><td>256 × 192</td><td>120</td><td>19.9</td><td>75.9</td><td>−</td><td>−</td><td>−</td><td>−</td><td>−</td></tr><tr><td>SimpleBaseline [49]</td><td>ResNet-152</td><td>Y</td><td>384 × 288</td><td>68.6</td><td>28.1†</td><td>74.3</td><td>89.6</td><td>81.1</td><td>70.5</td><td>79.7</td><td>79.7</td></tr><tr><td>HRNet-W32 [40]</td><td>-</td><td>Y</td><td>384 × 288</td><td>28.5</td><td>16.0†</td><td>75.8</td><td>90.6</td><td>82.7</td><td>71.9</td><td>82.8</td><td>81.0</td></tr><tr><td>HRNet-W48 [40]</td><td>-</td><td>Y</td><td>384 × 288</td><td>63.6</td><td>35.3†</td><td>76.3</td><td>90.8</td><td>82.9</td><td>72.3</td><td>83.4</td><td>81.2</td></tr><tr><td>HRNet-W48 + PF [33]</td><td>-</td><td>Y</td><td>384 × 288</td><td>63.6</td><td>35.3†</td><td>77.3</td><td>90.9</td><td>83.5</td><td>73.5</td><td>84.4</td><td>82.0</td></tr><tr><td>SimpleBaseline</td><td>ResNet-50</td><td>N</td><td>256 × 192</td><td>34.1</td><td>5.21</td><td>70.6</td><td>89.0</td><td>78.4</td><td>66.9</td><td>77.1</td><td>77.3</td></tr><tr><td>SimpleBaseline</td><td>ResNet-50</td><td>Y</td><td>256 × 192</td><td>34.1</td><td>5.21</td><td>71.0</td><td>89.2</td><td>78.5</td><td>67.4</td><td>77.4</td><td>78.0</td></tr><tr><td>HRNet-W32</td><td>-</td><td>N</td><td>256 × 192</td><td>28.6</td><td>7.65</td><td>73.6</td><td>89.9</td><td>80.5</td><td>70.1</td><td>80.0</td><td>80.0</td></tr><tr><td>EvoPose2D-XS</td><td>-</td><td>N</td><td>256 × 192</td><td>2.53</td><td>0.47</td><td>67.7</td><td>87.8</td><td>75.8</td><td>64.5</td><td>73.7</td><td>74.7</td></tr><tr><td>EvoPose2D-XS</td><td>-</td><td>WT</td><td>256 × 192</td><td>2.53</td><td>0.47</td><td>68.0</td><td>87.9</td><td>76.1</td><td>64.5</td><td>74.3</td><td>75.0</td></tr><tr><td>EvoPose2D-S</td><td>-</td><td>N</td><td>256 × 192</td><td>2.53</td><td>1.07</td><td>69.8</td><td>88.6</td><td>77.3</td><td>66.3</td><td>76.2</td><td>76.4</td></tr><tr><td>EvoPose2D-S</td><td>-</td><td>WT</td><td>256 × 192</td><td>2.53</td><td>1.07</td><td>70.2</td><td>88.9</td><td>77.8</td><td>66.5</td><td>76.8</td><td>76.9</td></tr><tr><td>EvoPose2D-M</td><td>-</td><td>N</td><td>384 × 288</td><td>7.34</td><td>5.59</td><td>75.1</td><td>90.2</td><td>81.9</td><td>71.5</td><td>81.7</td><td>81.0</td></tr><tr><td>EvoPose2D-L</td><td>-</td><td>N</td><td>512 × 384</td><td>14.7</td><td>17.7</td><td>76.6</td><td>90.5</td><td>83.0</td><td>72.7</td><td>83.4</td><td>82.3</td></tr><tr><td>EvoPose2D-L + PF</td><td>-</td><td>N</td><td>512 × 384</td><td>14.7</td><td>17.7</td><td>77.5</td><td>90.9</td><td>83.6</td><td>74.0</td><td>84.2</td><td>82.5</td></tr></tbody></table></div><ul><li><code>COCO test-dev</code></li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Pretrain</th><th>Input size</th><th>Params (M)</th><th>FLOPs (G)</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td>CPN [9]</td><td>Res-Inception</td><td>Y</td><td>384 × 288</td><td>-</td><td>-</td><td>72.1</td><td>91.4</td><td>80.0</td><td>68.7</td><td>77.2</td><td>78.5</td></tr><tr><td>SimpleBaseline [49]</td><td>ResNet-152</td><td>Y</td><td>384 × 288</td><td>68.6</td><td>35.6</td><td>73.7</td><td>91.9</td><td>81.1</td><td>70.3</td><td>80.0</td><td>79.0</td></tr><tr><td>HRNet-W48 [40]</td><td>-</td><td>Y</td><td>384 × 288</td><td>63.6</td><td>32.9</td><td>75.5</td><td>92.5</td><td>83.3</td><td>71.9</td><td>81.5</td><td>80.5</td></tr><tr><td>HRNet-W48 + PF [33]</td><td>-</td><td>Y</td><td>384 × 288</td><td>63.6</td><td>32.9</td><td>76.7</td><td>92.6</td><td>84.1</td><td>73.1</td><td>82.6</td><td>81.5</td></tr><tr><td>EvoPose2D-L</td><td>-</td><td>N</td><td>512 × 384</td><td>14.7</td><td>17.7</td><td>75.7</td><td>91.9</td><td>83.1</td><td>72.2</td><td>81.5</td><td>81.7</td></tr><tr><td>EvoPose2D-L + PF</td><td>-</td><td>N</td><td>512 × 384</td><td>14.7</td><td>17.7</td><td>76.8</td><td>92.5</td><td>84.3</td><td>73.5</td><td>82.5</td><td>81.7</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>topdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep High-Resolution Representation Learning for Human Pose Estimation</title>
    <link href="/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/"/>
    <url>/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation"><a href="#Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation" class="headerlink" title="Deep High-Resolution Representation Learning for Human Pose Estimation"></a><a href="https://arxiv.org/pdf/1902.09212.pdf">Deep High-Resolution Representation Learning for Human Pose Estimation</a></h1><blockquote><ol><li>本文提出一种新型的网络结构 HRNet，与以往的网络结构不同，HRNet 中 high-to-low resolution subnetworks 是平行排列的，不同分辨率的特征通过 downsample 和 upsample 的形式相互融合。</li><li>HRNet 在 COCO test-dev 数据集上取得 75.5% 的准确率，在加入额外训练数据后准确率为 77.0%</li></ol></blockquote><a id="more"></a><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>Sequential multi-resolution subnetworks<br>现有网络的 backbone 大多为 Sequential 结构，每个 subnetwork 被称为一个 stage，相邻的 stage 通过 downsample 连接，网络的分辨率逐层减半。  <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-symbol">N11</span> → <span class="hljs-symbol">N22</span> → <span class="hljs-symbol">N33</span> → <span class="hljs-symbol">N44</span><br></code></pre></td></tr></table></figure></li><li>Parallel multi-resolution subnetworks<br>HRNet 第一个 stage 为 high-resolution subnetwork，然后以并行的方法，逐渐添加 high-to-low resolution subnetworks。因此，后续添加的 stage 既包含之前 stage 的 high resolution 信息，又包含 low resolution 信息。  <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-symbol">N11</span> → <span class="hljs-symbol">N21</span> → <span class="hljs-symbol">N31</span> → <span class="hljs-symbol">N41</span><br>      <span class="hljs-symbol">N22</span> → <span class="hljs-symbol">N32</span> → <span class="hljs-symbol">N42</span><br>            <span class="hljs-symbol">N33</span> → <span class="hljs-symbol">N43</span><br>                  <span class="hljs-symbol">N44</span><br></code></pre></td></tr></table></figure></li><li>Repeated multi-scale fusion<br>HRNet 中加入 <em>exchange units</em> 来交换不同 subnetworks 的信息。<br>downsample 过程通过 3x3 的卷积实现，卷积的 stride 为 2，图像分辨率下降一半；<br>upsample 过程分为两步，第一步是 nearest neighbor sampling，第二步是 1x1 卷积，用于改变通道数。</li><li>Heatmap estimation<br>作者在最后一层 <em>exchange unit</em> 输出的 特征图上，预测最终的 heatmap。  </li></ul><h2 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h2><ul><li><code>COCO val</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Pretrain</th><th>Input size</th><th>#Params</th><th>GFLOPs</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td>8-stage Hourglass [40]</td><td>8-stage Hourglass</td><td>N</td><td>256 × 192</td><td>25.1M</td><td>14.3</td><td>66.9</td><td>−</td><td>−</td><td>−</td><td>−</td><td>−</td></tr><tr><td>CPN [11]</td><td>ResNet-50</td><td>Y</td><td>256 × 192</td><td>27.0M</td><td>6.20</td><td>68.6</td><td>−</td><td>−</td><td>−</td><td>−</td><td>−</td></tr><tr><td>CPN + OHKM [11]</td><td>ResNet-50</td><td>Y</td><td>256 × 192</td><td>27.0M</td><td>6.20</td><td>69.4</td><td>−</td><td>−</td><td>−</td><td>−</td><td>−</td></tr><tr><td>SimpleBaseline [72]</td><td>ResNet-50</td><td>Y</td><td>256 × 192</td><td>34.0M</td><td>8.90</td><td>70.4</td><td>88.6</td><td>78.3</td><td>67.1</td><td>77.2</td><td>76.3</td></tr><tr><td>SimpleBaseline [72]</td><td>ResNet-101</td><td>Y</td><td>256 × 192</td><td>53.0M</td><td>12.4</td><td>71.4</td><td>89.3</td><td>79.3</td><td>68.1</td><td>78.1</td><td>77.1</td></tr><tr><td>SimpleBaseline [72]</td><td>ResNet-152</td><td>Y</td><td>256 × 192</td><td>68.6M</td><td>15.7</td><td>72.0</td><td>89.3</td><td>79.8</td><td>68.7</td><td>78.9</td><td>77.8</td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td>N</td><td>256 × 192</td><td>28.5M</td><td>7.10</td><td>73.4</td><td>89.5</td><td>80.7</td><td>70.2</td><td>80.1</td><td>78.9</td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td>Y</td><td>256 × 192</td><td>28.5M</td><td>7.10</td><td>74.4</td><td>90.5</td><td>81.9</td><td>70.8</td><td>81.0</td><td>79.8</td></tr><tr><td>HRNet-W48</td><td>HRNet-W48</td><td>Y</td><td>256 × 192</td><td>63.6M</td><td>14.6</td><td>75.1</td><td>90.6</td><td>82.2</td><td>71.5</td><td>81.8</td><td>80.4</td></tr><tr><td>SimpleBaseline [72]</td><td>ResNet-152</td><td>Y</td><td>384 × 288</td><td>68.6M</td><td>35.6</td><td>74.3</td><td>89.6</td><td>81.1</td><td>70.5</td><td>79.7</td><td>79.7</td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td>Y</td><td>384 × 288</td><td>28.5M</td><td>16.0</td><td>75.8</td><td>90.6</td><td>82.7</td><td>71.9</td><td>82.8</td><td>81.0</td></tr><tr><td><strong>HRNet-W48</strong></td><td>HRNet-W48</td><td>Y</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>76.3</td><td>90.8</td><td>82.9</td><td>72.3</td><td>83.4</td><td>81.2</td></tr></tbody></table></div><ul><li><code>COCO test-dev</code>  </li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input size</th><th>#Params</th><th>GFLOPs</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td>Mask-RCNN [21]</td><td>ResNet-50-FPN</td><td></td><td></td><td></td><td>63.1</td><td>87.3</td><td>68.7</td><td>57.8</td><td>71.4</td><td></td></tr><tr><td>G-RMI [47]</td><td>ResNet-101</td><td>353 × 257</td><td>42.6M</td><td>57.0</td><td>64.9</td><td>85.5</td><td>71.3</td><td>62.3</td><td>70.0</td><td>69.7</td></tr><tr><td>Integral Pose Regression [60]</td><td>ResNet-101</td><td>256 × 256</td><td>45.0M</td><td>11.0</td><td>67.8</td><td>88.2</td><td>74.8</td><td>63.9</td><td>74.0</td><td></td></tr><tr><td>G-RMI + extra data [47]</td><td>ResNet-101</td><td>353 × 257</td><td>42.6M</td><td>57.0</td><td>68.5</td><td>87.1</td><td>75.5</td><td>65.8</td><td>73.3</td><td>73.3</td></tr><tr><td>CPN [11]</td><td>ResNet-Inception</td><td>384 × 288</td><td></td><td></td><td>72.1</td><td>91.4</td><td>80.0</td><td>68.7</td><td>77.2</td><td>78.5</td></tr><tr><td>RMPE [17]</td><td>PyraNet [77]</td><td>320 × 256</td><td>28.1M</td><td>26.7</td><td>72.3</td><td>89.2</td><td>79.1</td><td>68.0</td><td>78.6</td><td></td></tr><tr><td>CFN [25]</td><td></td><td></td><td></td><td></td><td>72.6</td><td>86.1</td><td>69.7</td><td>78.3</td><td>64.1</td><td></td></tr><tr><td>CPN (ensemble) [11]</td><td>ResNet-Inception</td><td>384 × 288</td><td></td><td></td><td>73.0</td><td>91.7</td><td>80.9</td><td>69.5</td><td>78.1</td><td>79.0</td></tr><tr><td>SimpleBaseline [72]</td><td>ResNet-152</td><td>384 × 288</td><td>68.6M</td><td>35.6</td><td>73.7</td><td>91.9</td><td>81.1</td><td>70.3</td><td>80.0</td><td>79.0</td></tr><tr><td>HRNet-W32</td><td>HRNet-W32</td><td>384 × 288</td><td>28.5M</td><td>16.0</td><td>74.9</td><td>92.5</td><td>82.8</td><td>71.3</td><td>80.9</td><td>80.1</td></tr><tr><td>HRNet-W48</td><td>HRNet-W48</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>75.5</td><td>92.5</td><td>83.3</td><td>71.9</td><td>81.5</td><td>80.5</td></tr><tr><td><strong>HRNet-W48 + extra data</strong></td><td>HRNet-W48</td><td>384 × 288</td><td>63.6M</td><td>32.9</td><td>77.0</td><td>92.7</td><td>84.5</td><td>73.4</td><td>83.1</td><td>82.0</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>topdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</title>
    <link href="/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation/"/>
    <url>/2021/01/12/Paper-Notes-Pose-Estimation-20210112-Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation"><a href="#Scale-Aware-Representation-Learning-for-Bottom-Up-Human-Pose-Estimation" class="headerlink" title="Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation"></a><a href="https://arxiv.org/pdf/1908.10357.pdf">Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</a></h1><blockquote><ol><li>本文以多人姿态估计中的尺度差异为研究点，提出 HigherHRNet 结构，生成不同尺度的特征图，同时在多尺度的特征图上进行监督学习，实现自底向上的多人姿态估计。</li><li>文章提出的方法在 COCO-test-dev 数据集上取得 70.5% 的 mAP<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li><p>HigherHRNet 网络结构<br>HRNet 生成原图 1/4 大小的特征图，然后经过一层卷积生成 K 通道的 heatmap。以此为基础，HigherHRNet 网络结构进行下面一系列改动：</p><ul><li><strong><em>multi-resolution supervision (MRS)</em></strong>  </li></ul><ol><li>将 <code>1/4特征图</code> 经过 <code>deconv</code> 模块，生成 <code>1/2特征图</code>；  </li><li><code>1/2特征图</code> 进过一层卷积得到 K 通道 <code>1/2heatmap</code>；  </li><li>同时在 <code>1/4heatmap</code> 和 <code>1/2heatmap</code> 两个特征图上进行监督学习；  </li><li>预测时使用 <code>1/2heatmap</code> 进行预测。  </li></ol><ul><li><strong><em>feature concat</em></strong><br>步骤1 中的 <code>1/4特征图</code> 先与 <code>1/4heatmap</code> concat，再进行 deconv 操作。  </li><li><strong><em>heatmap aggregation</em></strong><br>步骤4 时，同时使用 <code>1/4heatmap</code> 和 <code>1/2heatmap</code> 进行预测。  </li><li><strong><em>extra res. blocks</em></strong><br>步骤2 中，<code>1/2特征图</code> 经过 4 个 residual block 之后生成 <code>1/2heatmap</code></li></ul><p>COCO-val ablation 实验结果<br>| Network | w/ MRS | feature concat. | w/ heatmap aggregation | extra res. blocks | AP | APM | APL |<br>| - | - | - | - | - | - | - | - |<br>| HRNet       |   |   |   |   | 64.4 | 57.1 | 75.6 |<br>| HigherHRNet | y |   |   |   | 66.0 | 60.7 | 74.2 |<br>| HigherHRNet | y | y |   |   | 66.3 | 60.8 | 74.0 |<br>| HigherHRNet | y | y | y |   | 66.9 | 61.0 | 75.7 |<br>| HigherHRNet | y | y | y | y | 67.1 | 61.5 | 76.1 |</p></li><li><p>Grouping<br>文章使用了 associative embedding 实现 grouping，将 tags 相近的关键点连接成单人姿态。</p></li></ul><h2 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h2><p>  <code>COCO test-dev</code> 实验结果</p><div class="table-container"><table><thead><tr><th>Method</th><th>AP</th><th>AP50</th><th>AP75</th><th>APM</th><th>APL</th><th>AR</th></tr></thead><tbody><tr><td>OpenPose∗ [3]</td><td>61.8</td><td>84.9</td><td>67.5</td><td>57.1</td><td>68.2</td><td>66.5</td></tr><tr><td>Hourglass∗+ [30]</td><td>65.5</td><td>86.8</td><td>72.3</td><td>60.6</td><td>72.6</td><td>70.2</td></tr><tr><td>PifPaf [22]</td><td>66.7</td><td>-</td><td>-</td><td>62.4</td><td>72.9</td><td>-</td></tr><tr><td>SPM [32]</td><td>66.9</td><td>88.5</td><td>72.9</td><td>62.6</td><td>73.1</td><td>-</td></tr><tr><td>PersonLab+ [33]</td><td>68.7</td><td>89.0</td><td>75.4</td><td>64.1</td><td>75.5</td><td>75.4</td></tr><tr><td><strong>HigherHRNet-W48+</strong></td><td>70.5</td><td>89.3</td><td>77.2</td><td>66.6</td><td>75.8</td><td>74.9</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>bottomup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Associative Embedding  End-to-End Learning for Joint Detection and Grouping</title>
    <link href="/2021/01/11/Paper-Notes-Pose-Estimation-20210111-Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/"/>
    <url>/2021/01/11/Paper-Notes-Pose-Estimation-20210111-Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/</url>
    
    <content type="html"><![CDATA[<h1 id="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping"><a href="#Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping" class="headerlink" title="Associative Embedding  End-to-End Learning for Joint Detection and Grouping"></a><a href="https://proceedings.neurips.cc/paper/2017/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf">Associative Embedding  End-to-End Learning for Joint Detection and Grouping</a></h1><blockquote><ol><li>这篇文章提出利用 Associative Embedding 实现 关键点分组 的方法，作者认为计算机视觉中的很多任务可以看做 Detection + Grouping 的操作，首先检测出关键点，然后给关键点打上 tag ，根据 tag 对关键点进行分组。作者用这种思路实现了一种多人姿态估计的算法。</li><li>文章提出的方法在 COCO-test-dev 数据集上取得 65.5% 的 mAP，在 MPII 数据集上取得 77.5 的 mAP</li></ol></blockquote><a id="more"></a><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li><p>Network Architecture<br>作者使用 StackHourglass 的网络结构，在最终预测时，同时预测 detection heatmap 和 associative embedding，associative embedding 是一个长度为 1 的向量。<br>预测得到关键点的最强响应之后，将 associative embedding 相近的关键点连接起来，就得到了最终的多人姿态估计结果。<br>作者对 StackHourglass 进行了一点小改动，提高了 resolution 部分的 特征图通道数，为了提高网络的参数量，使其能够学习到 associative embedding 信息。</p></li><li><p>Detection and Grouping<br>Detection 部分，本文采用和 Hourglass 相同的方法，使用 Heatmap 进行监督，利用 MSE Loss 学习关节点的位置。<br>Grouping 部分，作者定义 grouping loss，定义图像中 N 个 Person， K 个 Keypoint，缩小相同 Person 关键点的 embedding 距离，增大不同 Person 关键点的 embedding 距离。具体定义见论文公式。  </p></li><li><p>Multiscale Evaluation<br>作者使用 Multiscale Evaluation 的方式进行测试。假设使用 4 种 scale 的图像作为输入，得到 4 种 heatmap 和 embedding，那么 新的 heatmap<br>由 4 个 heatmap 进行平均得到，embedding vector 由 4 个 embedding concat 而成，成为长度为 4 的 vector。<br><code>COCO-test-dev</code> 实验结果<br>| | AP | AP50 | AP75 | APM | APL |<br>|- |- |- |- |- |- |<br>| single scale          | 0.566 | 0.818 | 0.618 | 0.498 | 0.670 |<br>| single scale + refine | 0.628 | 0.846 | 0.692 | 0.575 | 0.706 |<br>| multi scale           | 0.650 | 0.867 | 0.713 | 0.597 | 0.725 |<br>| multi scale + refine  | 0.655 | 0.868 | 0.723 | 0.606 | 0.726 |</p></li></ul><h1 id="2-Result"><a href="#2-Result" class="headerlink" title="2. Result"></a>2. Result</h1><ul><li><code>COCO-test-dev</code>  </li></ul><div class="table-container"><table><thead><tr><th>method</th><th>AP</th></tr></thead><tbody><tr><td>OpenPose</td><td>0.618</td></tr><tr><td>Mask RCNN</td><td>0.627</td></tr><tr><td>G-RMI</td><td>0.649</td></tr><tr><td><strong>AE</strong></td><td><strong>0.655</strong></td></tr></tbody></table></div><ul><li><code>COCO-test-std</code>  </li></ul><div class="table-container"><table><thead><tr><th>method</th><th>AP</th></tr></thead><tbody><tr><td>OpenPose</td><td>0.611</td></tr><tr><td>G-RMI</td><td>0.643</td></tr><tr><td><strong>AE</strong></td><td><strong>0.663</strong></td></tr></tbody></table></div><ul><li><code>MPII</code>  </li></ul><div class="table-container"><table><thead><tr><th>method</th><th>AP</th></tr></thead><tbody><tr><td>OpenPose</td><td>0.756</td></tr><tr><td>AlphaPose</td><td>0.767</td></tr><tr><td><strong>AE</strong></td><td><strong>0.775</strong></td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>bottomup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Simple Baselines for Human Pose Estimation and Tracking</title>
    <link href="/2021/01/07/Paper-Notes-Pose-Estimation-20210107-Simple-Baselines-for-Human-Pose-Estimation-and-Tracking/"/>
    <url>/2021/01/07/Paper-Notes-Pose-Estimation-20210107-Simple-Baselines-for-Human-Pose-Estimation-and-Tracking/</url>
    
    <content type="html"><![CDATA[<h1 id="Simple-Baselines-for-Human-Pose-Estimation-and-Tracking"><a href="#Simple-Baselines-for-Human-Pose-Estimation-and-Tracking" class="headerlink" title="Simple Baselines for Human Pose Estimation and Tracking"></a><a href="https://arxiv.org/pdf/1804.06208.pdf">Simple Baselines for Human Pose Estimation and Tracking</a></h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction:"></a>1. Introduction:</h2><ol><li>提出姿态估计 和 姿态跟踪 的一个简单 baseline<a id="more"></a></li></ol><h2 id="2-Pose-Estimation-Using-A-Deconvolution-Head-Network"><a href="#2-Pose-Estimation-Using-A-Deconvolution-Head-Network" class="headerlink" title="2. Pose Estimation Using A Deconvolution Head Network"></a>2. Pose Estimation Using A Deconvolution Head Network</h2><p>作者提出一个简单的网络结构作为 Baseline，网络结构如下：<br>  参考Fig1</p><p>作者对比了之前的两种方法 Stacked hourglass 和 CPN，两者都是对特征图进行上采样，生成 heatmap，而作者提出的网络结构，直接通过三层反卷积生成了heatmap，结构十分简单。</p><h3 id="2-1-Pose-Tracking-Based-on-Optical-Flow"><a href="#2-1-Pose-Tracking-Based-on-Optical-Flow" class="headerlink" title="2.1 Pose Tracking Based on Optical Flow"></a>2.1 Pose Tracking Based on Optical Flow</h3><p>作者对 ICCV’17 PoseTrack Challenge 的方法（<em>Detect-and-Track</em>）进行改进，提出 多人姿态跟踪 的 Baseline。作者的改进点有一下两个：  </p><ul><li><strong>Joint Propagation using Optical Flow</strong><br>利用前一帧的关键点预测 J(k) 已经两帧的光流 F(k, k+1)，投影出当前帧的关键点 J(k+1)，计算关键点的包围框并扩大一定比例，投影出当前帧的包围框。<br>这样可以解决由于 <strong>模糊、遮挡</strong> 造成的误检</li><li><strong>Flow-based Pose Similarity</strong><br>Pose Track 的时候需要计算两帧之间的 similarity，作者认为，用 包围框的 IOU 作为 metric 不能处理运动太快的情况；直接用 OKS 作为 metric 不能处理人物动作幅度比较大的情况。因此，作者首先利用光流投影出上一帧的关键点在当前帧的位置，然后再计算 OKS。</li></ul><h3 id="2-2-Flow-based-Pose-Tracking-Algorithm"><a href="#2-2-Flow-based-Pose-Tracking-Algorithm" class="headerlink" title="2.2 Flow-based Pose Tracking Algorithm"></a>2.2 Flow-based Pose Tracking Algorithm</h3><p>  参考Fig2</p><h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="3-1-Pose-Estimation-on-COCO"><a href="#3-1-Pose-Estimation-on-COCO" class="headerlink" title="3.1 Pose Estimation on COCO"></a>3.1 Pose Estimation on COCO</h3><ul><li>Train：<ul><li>pretrain on ImageNet</li><li>将 GT 框扩展成 4:3</li><li>crop 出 GT 框并 resize 成固定大小 256x192</li><li>数据增强： scale（0.7~1.3）， rotation （-40~40），flip</li></ul></li><li>Test：  <ul><li>faster-rcnn 检测</li><li>origin + flip</li><li>quarter offset from highest response to the second highest response</li></ul></li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Input Size</th><th>OHKM</th><th>AP</th></tr></thead><tbody><tr><td>8-stage Hourglass</td><td>-</td><td>256x192</td><td>n</td><td>66.9</td></tr><tr><td>8-stage Hourglass</td><td>-</td><td>256x256</td><td>n</td><td>67.1</td></tr><tr><td>CPN</td><td>ResNet-50</td><td>256x192</td><td>n</td><td>68.6</td></tr><tr><td>CPN</td><td>ResNet-50</td><td>384x288</td><td>n</td><td>70.6</td></tr><tr><td>CPN</td><td>ResNet-50</td><td>256x192</td><td>y</td><td>69.4</td></tr><tr><td>CPN</td><td>ResNet-50</td><td>384x288</td><td>y</td><td>71.6</td></tr><tr><td>Ours</td><td>ResNet-50</td><td>256x192</td><td>n</td><td>70.4</td></tr><tr><td>Ours</td><td>ResNet-50</td><td>384x288</td><td>n</td><td>72.2</td></tr><tr><td>Ours</td><td>ResNet-101</td><td>256x192</td><td>n</td><td>71.4</td></tr><tr><td>Ours</td><td>ResNet-152</td><td>256x192</td><td>n</td><td>72.0</td></tr></tbody></table></div><h3 id="3-2-Pose-Estimation-and-Tracking-on-PoseTrack"><a href="#3-2-Pose-Estimation-and-Tracking-on-PoseTrack" class="headerlink" title="3.2 Pose Estimation and Tracking on PoseTrack"></a>3.2 Pose Estimation and Tracking on PoseTrack</h3><ul><li>Train:  <ul><li>pretrain on COCO</li><li>根据 keypoint 画出 bbox，外扩 15%</li><li>数据增强</li></ul></li><li>Test：  <ul><li>检测框（ R-FCN、 FPN-DCN ）</li><li>光流估计 （ FlowNet2S )</li><li>Propagation </li><li>丢弃低置信度框（&lt;0.5）</li><li>丢弃低置信度关键点 （&lt;0.4）</li></ul></li></ul><div class="table-container"><table><thead><tr><th>Method</th><th>Backbone</th><th>Detector</th><th>Joint Propagation</th><th>Similarity Metric</th><th>mAP</th><th>MOTA</th></tr></thead><tbody><tr><td>a6</td><td>ResNet-50</td><td>R-FCN</td><td>y</td><td>SMulti_flow</td><td>70.3</td><td>62.2</td></tr><tr><td>b1</td><td>ResNet-50</td><td>FPN-DCN</td><td>n</td><td>SBbox</td><td>69.3</td><td>59.8</td></tr><tr><td>b2</td><td>ResNet-50</td><td>FPN-DCN</td><td>n</td><td>SPose</td><td>69.3</td><td>59.7</td></tr><tr><td>b3</td><td>ResNet-50</td><td>FPN-DCN</td><td>y</td><td>SBbox</td><td>72.4</td><td>62.1</td></tr><tr><td>b4</td><td>ResNet-50</td><td>FPN-DCN</td><td>y</td><td>SPose</td><td>72.4</td><td>61.8</td></tr><tr><td>b5</td><td>ResNet-50</td><td>FPN-DCN</td><td>y</td><td>SFlow</td><td>72.4</td><td>62.4</td></tr><tr><td>b6</td><td>ResNet-50</td><td>FPN-DCN</td><td>y</td><td>SMulti_Flow</td><td>72.4</td><td>62.9</td></tr><tr><td>c6</td><td>ResNet-152</td><td>FPN-DCN</td><td>y</td><td>SMulti_Flow</td><td>76.7</td><td>65.4</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>topdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RMPE  Regional Multi-Person Pose Estimation</title>
    <link href="/2021/01/07/Paper-Notes-Pose-Estimation-20210107-RMPE-Regional-Multi-Person-Pose-Estimation/"/>
    <url>/2021/01/07/Paper-Notes-Pose-Estimation-20210107-RMPE-Regional-Multi-Person-Pose-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="RMPE-Regional-Multi-Person-Pose-Estimation"><a href="#RMPE-Regional-Multi-Person-Pose-Estimation" class="headerlink" title="RMPE  Regional Multi-Person Pose Estimation"></a><a href="https://arxiv.org/pdf/1612.00137.pdf">RMPE  Regional Multi-Person Pose Estimation</a></h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ol><li>top-down 的 muti-person PE 方法有两个缺点：a 有的框(IOU&gt;0.5)实际上框的不准，b 有一些冗余框</li><li>提出 Regional Muti-person estimation 框架。</li><li>提出 symmetric spatial transformer network (SSTN), 对 SPPE 方法进行改进，在不准确的 bbox 上仍然能够准确地预测姿态。</li><li>提出 parametric pose NMS (PP-NMS), 解决冗余框的问题</li><li>提出 pose-guided human proposal generator (PGPG), 对训练数据进行数据增强<a id="more"></a></li></ol><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><ul><li>Single Person PE：<ul><li>tree models</li><li>random forrest models</li><li>CNN based models</li></ul></li><li>Muti person PE:<ul><li>part based framework<ul><li>一大堆没听过的算法</li><li>缺点是只利用了局部区域的信息，part 的检测效果可能不好</li></ul></li><li>two-step framework<ul><li>又是一大堆没听过的算法</li><li>本文的框架基于 two-step framework，解决人体 bbox 检测不准的问题。</li></ul></li></ul></li></ul><h2 id="3-Regional-Multi-person-Pose-Estimation"><a href="#3-Regional-Multi-person-Pose-Estimation" class="headerlink" title="3. Regional Multi-person Pose Estimation"></a>3. Regional Multi-person Pose Estimation</h2><p>fig3</p><ul><li>STN &amp; SDTN （SSTN）：<br>作者认为，目标检测得到的框很有可能是不准确的，这是制约 top-down 方法效果的一个关键点。因此，作者将 detection 的结果输入 SPPE 模块之前，会先经过一个 STN 模块，得到 PE 结果后，再经过一个对称的 SDTN 模块。STN 的目标是从 detection 结果中 warp 出更精确的框，STDN 的目标则相反。STN 和 STDN 都包含 3 个参数，通过推到可以得到两者的关系（参考论文）。</li><li>Parallel SPPE：<br>为了使 STN 更准确地定位到 detection proposal 的 domainate person，作者还加入了一个 Parallel SPPE，这个模块是一个固定参数的 SPPE 模块，输入由 STN 生成的 grid 之后，使用预训练好的 SPPE 对 grid 中的 person 进行预测，然后与 groundtruth 进行比较，这里的 groundtruth 是对原始 groundtruth 进行 center-located 操作之后得到的。  <blockquote><p>我的理解是，假设图片中有 A,B 两个人，目标检测得到一个不太准的框 bbox(A)，Parallel SPPE 的 groundtruth 就是以 A 的 pose groundtruth 为中心的 图片（SPPE 的 gt 就是直接将 gt 坐标转化到 bbox(A) 里面图片），如果 STN 生成的 grid 离 A 很远，Parallel SPPE 的误差就会非常大。</p></blockquote></li><li>Parametric Pose NMS：<br>为了消除冗余，作者提出了对 Pose 进行 NMS 的方法，NMS 的 metric 是 两个 Pose 的距离，作者提出了两种度量 Pose distance 的方法，NMS 的时候会取两种距离的加权和。<ul><li>H_sim<br>公式参照论文，这个距离很好理解，就是两个 pose 各个关键点的 L2 距离</li><li>K_sim<br>公式参照论文，这个距离的含义是，如果两个 Pose 的某个关键点重合（重合的概念被定义为 一个关键点处于另一个关键点为中心的矩形区域中），就会计算两个关键点置信度的乘积，因此，重叠的关键点越多，并且置信度越高的情况下，计算的结果越大。<blockquote><p>这个地方有个很奇怪的点。按照公式，越相似的 Pose 计算出来的 distance 会越大，但 NMS 公式里面却是距离小才会被抑制，不知道哪里看错了。</p></blockquote></li></ul></li><li>Poseguided Proposals Generator：<br>为了让 STN+SPPE 模块适应多样的 bbox proposal 输入，作者提出了一种数据增广的方案。作者会统计 detection 的 bbox 和 gt 的偏移量的分布。训练的时候，根据偏移量的分布，随机对 detection 的结果进行 shift。<br>由于不同的 Pose 对应的偏移量分布可能各不相同，作者定义了 4 个 原子姿态，首先对图片的 Pose 用 kmeans 进行聚类，每种原子姿态会对应不同的偏移量分布。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>topdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cascaded Pyramid Network for Multi-Person Pose Estimation</title>
    <link href="/2021/01/07/Paper-Notes-Pose-Estimation-20210107-Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation/"/>
    <url>/2021/01/07/Paper-Notes-Pose-Estimation-20210107-Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation"><a href="#Cascaded-Pyramid-Network-for-Multi-Person-Pose-Estimation" class="headerlink" title="Cascaded Pyramid Network for Multi-Person Pose Estimation"></a><a href="https://arxiv.org/abs/1711.07319">Cascaded Pyramid Network for Multi-Person Pose Estimation</a></h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction:"></a>1. Introduction:</h2><ol><li>提出 <code>cascaded pyramid network（CPN）</code>，包含 <code>global pyramid network</code> 和 <code>pyramid refined network</code></li><li>探讨 多人目标检测中各种因素的影响</li><li>COCO 上 <strong>test-dev AP： 73.0; test-challenge AP：72.1</strong><a id="more"></a></li></ol><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work:"></a>2. Related work:</h2><ol><li><p>muti-pose:</p><ul><li>bottom-up: DeepCut, OpenPose  </li><li>top-down: Mask-RCNN  </li></ul></li><li><p>single pose:</p><ul><li>CPM</li><li>hourglass</li><li>…</li></ul></li><li><p>detection:</p><ul><li>FPN</li><li>mask-RCNN</li></ul></li></ol><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><ul><li><p>network structure:<br><a href="https://arxiv.org/pdf/1711.07319.pdf">Fig-1</a></p></li><li><p>Detector:<br>这篇文章使用的 detetor 配置如下， FPN(res101) + ROIAlign + softNMS. AP = 0.411, AP(Person) = 0.533</p></li><li><p>GlobalNet<br>basemodel 提取不同 stage 的特征，底层的特征直接用插值的方法 upsample，然后通过 <code>1*1</code> 卷积将通道统一为 256，对特征做 <code>element-wise sum</code> 得到 feature pyramid，金字塔上每一层特征通过一个 1*1 conv + 3*3*17 conv 得到 globalnet 的输出（ heatmap * 17）</p></li><li><p>RefineNet<br>使用 bottleneck 结构对特征进行 upsample，特征图越小，通过的 bootleneck 越多，upsample 倍率越大，最后所有特征都具有相同的大小。将 4 个 stage 的特征在 channel wise concat 起来。</p></li><li><p>ohkm<br>根据 loss 判断出 hard keypoint，然后只对部分关键点的 loss 进行反向传播。</p></li></ul><h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><ul><li>Cropping Strategy<br><code>bbox --(extend)--&gt; 256:192  --(crop)--&gt; block --(resize)--&gt; 256:192</code></li><li>Data Augmentation<br>random flip<br>random rotation<br>random scale</li><li>Test:<br><strong>Apply gussian filter to heatmap</strong><br><strong>Also test flipped image</strong><br><strong>最高的响应 R1，第二高的响应为 R2，最终结果 = R1+0.25*R2</strong><br><strong>Rescore: S(pose) = S(bbox) * avg(S(key-point))</strong></li></ul><h1 id="5-Result"><a href="#5-Result" class="headerlink" title="5. Result"></a>5. Result</h1><p><strong>Ablation experiment</strong></p><div class="table-container"><table><thead><tr><th></th><th>AP</th><th>FLOPs</th><th>Params</th></tr></thead><tbody><tr><td>ResNet-50 + dilation(res4-5)</td><td>66.5</td><td>17.71G</td><td>92M</td></tr><tr><td>GlobalNet only</td><td>66.6</td><td>3.90G</td><td>94M</td></tr><tr><td>CPN w/o ohkm</td><td>68.6</td><td>6.20G</td><td>102M</td></tr><tr><td>CPN</td><td>69.4</td><td>6.20G</td><td>102M</td></tr><tr><td>GlobalNet + Concat</td><td>68.5</td><td>5.87G</td><td>-</td></tr><tr><td>GlobalNet + 1 bottleneck + Concat</td><td>69.2</td><td>6.92G</td><td>-</td></tr><tr><td>CPN (384 × 288)</td><td>71.6</td><td>13.9G</td><td>-</td></tr></tbody></table></div><p>在 <strong>refineNet</strong> 中使用不同的 layer</p><div class="table-container"><table><thead><tr><th></th><th>AP</th><th>FLOPs</th></tr></thead><tbody><tr><td>C2</td><td>68.3</td><td>5.02G</td></tr><tr><td>C2-3</td><td>68.4</td><td>5.50G</td></tr><tr><td>C2-4</td><td>69.1</td><td>5.88G</td></tr><tr><td>C2-5</td><td>69.4</td><td>6.20G</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>Pose Estimation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>topdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 编写 C 扩展</title>
    <link href="/2021/01/07/Python-20210107-Python-%E7%BC%96%E5%86%99-C-%E6%89%A9%E5%B1%95/"/>
    <url>/2021/01/07/Python-20210107-Python-%E7%BC%96%E5%86%99-C-%E6%89%A9%E5%B1%95/</url>
    
    <content type="html"><![CDATA[<h1 id="Python-编写-C-C-扩展"><a href="#Python-编写-C-C-扩展" class="headerlink" title="Python 编写 C/C++ 扩展"></a>Python 编写 C/C++ 扩展</h1><p>Python 的特性使得其在编写大型项目的时候并不占优势，目前主流的项目代码还是用C/C++或Java编写的，为了让我们写的Python代码能够嵌入到项目中，一般有两种方式，<br>1）让Python操作数据库，相当于通过读数据库完成传参，写数据库实现结果返回；<br>2）使用Python嵌入式编程，<br><strong>操作数据库</strong>的方式相对简单，但是存在一些问题，例如，主程序需要监听Python代码的返回值、数据库的权限需要暴露给模块、每一个模块都需要变为一个单独的服务常驻进程，相比之下，<strong>嵌入式编程</strong>的方式更加优雅，只需要将Python代码编写成 DLL/.so 文件提供给主程序，主程序直接调用就可以，这种过程的一个副产物就是可以保护源代码。<br>下面记录一次在Windows上使用Python嵌入式编程的实战过程，主要包含 <strong>工具准备</strong>，<strong>创建DLL</strong>，<strong>调用DLL</strong>，<strong>变量类型优化</strong>，<strong>依赖解决</strong>五个部分</p><a id="more"></a><h2 id="工具准备"><a href="#工具准备" class="headerlink" title="工具准备"></a>工具准备</h2><h3 id="1-MinGW"><a href="#1-MinGW" class="headerlink" title="1. MinGW"></a>1. MinGW</h3><p>网上的很多教程使用 <code>VS</code> 编写 DLL，鉴于 <code>VS</code> 安装过程太麻烦，并且还不能跨平台，这里选择的编译器是 <code>GCC</code>，直接在 MinGW-W64 <a href="http://www.mingw-w64.org/doku.php">官网</a>上下载安装程序就可以了（不要在 minGW 官网上下载，在线安装过程会不断 fail），安装完了之后需要做两件事：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 将 MinGW 的 bin 目录加入环境变量<br><span class="hljs-bullet">2.</span> 在 bin 目录下建立软链接，make -&gt; mingw32-make<br></code></pre></td></tr></table></figure><br>安装成功之后可以在命令行使用<code>gcc, make</code>这两个命令</p><h3 id="2-Cython"><a href="#2-Cython" class="headerlink" title="2. Cython"></a>2. Cython</h3><p>这个很简单，只需要 <code>pip install cython</code> 就安装好了，安装成功可以在命令行使用 <code>cython</code>命令</p><h2 id="创建-DLL"><a href="#创建-DLL" class="headerlink" title="创建 DLL"></a>创建 DLL</h2><h3 id="1-Python-源文件"><a href="#1-Python-源文件" class="headerlink" title="1. Python 源文件"></a>1. Python 源文件</h3><p>这次封装的 Python 代码源文件如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Y_interpoldate.py</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.interpolate <span class="hljs-keyword">import</span> griddata<br><br><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&#x27;PROJ_LIB&#x27;</span>] = <span class="hljs-string">r&#x27;D:\Softwares\Anaconda3\pkgs\proj4-4.9.3-hcf24537_7\Library\share&#x27;</span><br><span class="hljs-keyword">from</span> mpl_toolkits.basemap <span class="hljs-keyword">import</span> Basemap<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span>(<span class="hljs-params">csv_name</span>):</span><br>    <span class="hljs-keyword">return</span> pd.read_csv(csv_name, header=<span class="hljs-literal">None</span>, names=[<span class="hljs-string">&#x27;lons&#x27;</span>,<span class="hljs-string">&#x27;lats&#x27;</span>,<span class="hljs-string">&#x27;values_&#x27;</span>])<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">interpolate</span>():</span><br>    data = load_data(<span class="hljs-string">&#x27;HG.csv&#x27;</span>)<br>    basemap = Basemap(projection=<span class="hljs-string">&#x27;ortho&#x27;</span>,lat_0=<span class="hljs-number">30</span>,lon_0=<span class="hljs-number">120</span>)<br>    x, y = basemap(data.lons.values, data.lats.values)<br><br>    xi = np.linspace(<span class="hljs-built_in">min</span>(x)*<span class="hljs-number">0.8</span>, <span class="hljs-built_in">max</span>(x)*<span class="hljs-number">1.2</span>, <span class="hljs-number">300</span>)<br>    yi = np.linspace(<span class="hljs-built_in">min</span>(y)*<span class="hljs-number">0.8</span>, <span class="hljs-built_in">max</span>(y)*<span class="hljs-number">1.2</span>, <span class="hljs-number">300</span>)<br>    X, Y = np.meshgrid(xi, yi)<br>    Z = griddata( (x,y), data.values_, (X,Y), method=<span class="hljs-string">&#x27;linear&#x27;</span> )<br>    print(X.shape, Y.shape, Z.shape)<br></code></pre></td></tr></table></figure><br>需要封装的函数是 <code>interpolate</code>，为了简单起见，函数没有设置入参和返回值，在<strong>动态变量类型优化</strong>部分会深入讨论</p><h3 id="2-Cython-编译"><a href="#2-Cython-编译" class="headerlink" title="2. Cython 编译"></a>2. Cython 编译</h3><p>第一步用 Cython 编译源文件，Cython 编译的源文件是 <code>.pyx</code> 格式，Cython中定义导出函数的格式如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python">cdef public PyObject* func(PyObject* argv, ...)<br><br><span class="hljs-comment"># cdef： 是 Cython 定义函数的关键字；  </span><br><span class="hljs-comment"># public： 代表函数是要被导出的；  </span><br><span class="hljs-comment"># void： 指定函数的返回类型。 </span><br></code></pre></td></tr></table></figure></p><p>因此将源文件需要导出的函数进行下面的改动并另存为 <code>.pyx</code> 文件<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta"># _Y_interpolate.pyx</span><br>cdef <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> _interpolate():<br>    ...<br></code></pre></td></tr></table></figure><br>使用如下命令编译 <code>.pyx</code>文件生成 <code>.c, .h</code> 文件<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">cython <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">_Y_interpolate</span>.</span></span>pyx<br></code></pre></td></tr></table></figure><br>在生成的头文件中可以找到两个函数：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__PYX_EXTERN_C <span class="hljs-keyword">void</span> _interpolate(<span class="hljs-keyword">void</span>);<br><br><span class="hljs-function">PyMODINIT_FUNC <span class="hljs-title">init_Y_interpolate</span><span class="hljs-params">(<span class="hljs-keyword">void</span>)</span></span>;     <span class="hljs-comment">//for py2</span><br><span class="hljs-function">PyMODINIT_FUNC <span class="hljs-title">PyInit__Y_interpolate</span><span class="hljs-params">(<span class="hljs-keyword">void</span>)</span></span>;  <span class="hljs-comment">//for py3</span><br></code></pre></td></tr></table></figure><br>注意这里另存的 <code>.pyx</code> 文件名和 <code>函数名</code> 前面都添加了一个 <code>_</code>，在之后会解释。</p><h3 id="2-5-生成-exe-文件"><a href="#2-5-生成-exe-文件" class="headerlink" title="2.5 生成 exe 文件"></a>2.5 生成 exe 文件</h3><p>生成 <code>_Y_interpolate.c</code> 和 <code>_Y_interpolate.h</code> 文件后，就可以直接利用这两个源文件编写 C 程序了<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// cppmain.c</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;Python.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;_Y_interpolate.h&quot;</span></span><br> <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span> *argv[])</span></span><br><span class="hljs-function"></span>&#123;<br>    Py_Initialize();<br>    PyInit__Y_interpolate();<br>    _interpolate();<br>    Py_Finalize();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><br>编译命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcc">gcc _Y_interpolate.c cppmain.c -m64 -mthreads -Wall -O3 \<br>-I. -ID:\Softwares\Anaconda3\include \<br>-LD:\Softwares\Anaconda3\Lib -LD:\Softwares\Anaconda3\ <br>-lpython36 -o cppmain<br></code></pre></td></tr></table></figure><br>可以看见直接调用的程序还比较麻烦，需要使用 <code>Py_Initialize()</code> 和 <code>Py_Finalize()</code> 包裹代码，而且使用函数之前还需要一次初始化(注意这里的初始化函数 python2 和 python3 有区别，例子是 py3 的写法)。下面看看 DLL 封装是怎么做的吧。</p><h3 id="3-生成-DLL-文件"><a href="#3-生成-DLL-文件" class="headerlink" title="3 生成 DLL 文件"></a>3 生成 DLL 文件</h3><p>编写下面的 DLL 主程序代码<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// Y_interpolate.h</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">ifndef</span> Y_interpolate_H</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> Y_interpolate_H</span><br><br><span class="hljs-meta">#<span class="hljs-meta-keyword">ifdef</span> BUILD_DLL</span><br>    __declspec(dllexport) <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">interpolate</span><span class="hljs-params">(<span class="hljs-keyword">void</span>)</span></span>;<br><span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span><br>    __declspec(dllimport) <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">interpolate</span><span class="hljs-params">(<span class="hljs-keyword">void</span>)</span></span>;<br><span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span><br><br><span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span><br></code></pre></td></tr></table></figure><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">//Y_interplate.c</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;Python.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;Windows.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;Y_interpolate.h&quot;</span></span><br><br><span class="hljs-comment">// 定义DLL导出函数 interpolate()，如果没有这一步，同样可以生成 dll，但是符号表中找不到 _interpolate()</span><br>__declspec(dllexport) <span class="hljs-function"><span class="hljs-keyword">void</span> __stdcall <span class="hljs-title">interpolate</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> _interpolate();<br>&#125;<br><br><span class="hljs-comment">//将  Python 初始化的代码封装到 DLL 的主函数中</span><br><span class="hljs-function">BOOL WINAPI <span class="hljs-title">DllMain</span><span class="hljs-params">(HINSTANCE hinstDLL,DWORD fdwReason,LPVOID lpReserved)</span> </span>&#123;<br>    <span class="hljs-keyword">switch</span>( fdwReason ) &#123; <br>        <span class="hljs-keyword">case</span> DLL_PROCESS_ATTACH:<br>            Py_Initialize();<br>            PyInit_Y_interpolate();<br>            <span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> DLL_PROCESS_DETACH:<br>            Py_Finalize();<br>            <span class="hljs-keyword">break</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> TRUE;<br>&#125;<br></code></pre></td></tr></table></figure><br>编译命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcc">gcc _Y_interpolate.c Y_interpolate.c Y_interpolate.h \<br>-DBUILD_DLL -shared -m64 -Wall -O3 \<br>-I. -ID:\Softwares\Anaconda3\include \<br>-LD:\Softwares\Anaconda3\Lib -LD:\Softwares\Anaconda3\ \<br>-lpython36 -o Y_interpolate.dll -Wl,--output-def,Y_interpolate.def,--out-implib,Y_interpolate.a<br></code></pre></td></tr></table></figure><br>可以看见，<code>Y_interplate.c</code> 中的代码就是将 Cython 生成的 <code>_Y_interplate.c</code> 进行了一次封装，第一，将函数接口封装成 DLL导出函数；第二，将 Python 初始化相关的代码封装到DLL的加载主函数中（这就是之前 Cython 生成 .c 文件时最好修改一下文件名和函数接口的原因）。<br>编译的时候 <code>-Wl</code> 选项后面的内容指定编译器同时生成了 <code>def</code> 文件和 <code>.a</code> 文件，这两个文件一般是用来存放代码中的函数表的，实际上，目前调用的时候并不需要这两个文件</p><h2 id="调用-DLL"><a href="#调用-DLL" class="headerlink" title="调用 DLL"></a>调用 DLL</h2><p>生成好 DLL 后，就可以编写 C 程序来调用了，看一下调用的代码<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-comment">//dllcall.c</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;Y_interpolate.h&quot;</span></span><br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    interpolate();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><br>编译命令：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">gcc</span> dllcall.c -m<span class="hljs-number">64</span> -mthreads -Wall -O<span class="hljs-number">3</span> -I. -L. -lY_interpolate -o dllcall<br></code></pre></td></tr></table></figure><br>不需要解释，无论是程序代码编写还是编译命令都非常优雅简洁，没有任何冗余。<br>只需要一个 <code>.h</code> 文件和 <code>dll</code> 文件，就可以让其他程序调用 <code>interpolate</code> 这个接口（实际上，即使没有头文件，程序也可以编译成功，只是会出现 warning）。</p><h2 id="变量类型优化"><a href="#变量类型优化" class="headerlink" title="变量类型优化"></a>变量类型优化</h2><p>实际项目中，混合编程大多数都是需要传递参数的，由于这一部分比较复杂，因此在上面的例子中导出函数入参和返回值都设计为 void，下面详细讲解 Python 混合编程的时候怎样传参。<br>众所周知，Python 中的变量是动态变量，不需要显式声明就可以直接使用，而 C/C++ 中的变量都是静态变量，因此传参的过程必然需要转换，转换例子可以参看 Python 官方的例子<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// 1.1 转换入参 C-&gt;PyObject，构造包含3个元素的数组</span><br>PyObject* args = <span class="hljs-constructor">PyTuple_New(3)</span>;<br>PyObject* arg1 = <span class="hljs-constructor">Py_BuildValue(<span class="hljs-string">&quot;i&quot;</span>, 100)</span>; <span class="hljs-comment">// 整数参数</span><br>PyObject* arg2 = <span class="hljs-constructor">Py_BuildValue(<span class="hljs-string">&quot;f&quot;</span>, 3.14)</span>; <span class="hljs-comment">// 浮点数参数</span><br>PyObject* arg3 = <span class="hljs-constructor">Py_BuildValue(<span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;hello&quot;</span>)</span>; <span class="hljs-comment">// 字符串参数</span><br><span class="hljs-constructor">PyTuple_SetItem(<span class="hljs-params">args</span>, 0, <span class="hljs-params">arg1</span>)</span>;<br><span class="hljs-constructor">PyTuple_SetItem(<span class="hljs-params">args</span>, 1, <span class="hljs-params">arg2</span>)</span>;<br><span class="hljs-constructor">PyTuple_SetItem(<span class="hljs-params">args</span>, 2, <span class="hljs-params">arg3</span>)</span>;<br><br><span class="hljs-comment">// 1.2 与上面等价</span><br>PyObject* args = <span class="hljs-constructor">Py_BuildValue(<span class="hljs-string">&quot;(ifs)&quot;</span>, 100, 3.14, <span class="hljs-string">&quot;hello&quot;</span>)</span>;<br><br><span class="hljs-comment">// 2 调用函数</span><br>PyObject* pRet = <span class="hljs-constructor">PyObject_CallObject(<span class="hljs-params">pv</span>, <span class="hljs-params">args</span>)</span>;<br><br><span class="hljs-comment">// 3. 转换返回值 PyObject-&gt;C，类似函数还有 PyFloat_AsDouble,PyString_AsString,PyArg_ParseTuple 等</span><br>long res = <span class="hljs-constructor">PyInt_AsLong(<span class="hljs-params">pRet</span>)</span>;<br></code></pre></td></tr></table></figure><br>例程中的转换分别使用了 <code>Py_BuildValue</code>，<code>PyInt_AsLong</code>两个接口，这都是 <code>Python.h</code> 中定义的函数，用这些操作去构造数组无疑是繁琐且效率低下的，那么可不可以避免这些操作呢。答案是可以的，我们可以在 Cython 编译 <code>pyx</code>文件的时候告诉它入参和返回值的类型，这样调用方就可以像调用C语言的函数一样调用导出的函数了，由于使用了静态类型，这样做的一个副产物就是可以提高Python代码的效率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># _Y_interpolate.pyx</span><br>cdef public void _interpolate(<span class="hljs-built_in">float</span> in_arr[][<span class="hljs-number">3</span>], <span class="hljs-built_in">float</span> out_arr[][<span class="hljs-number">3</span>], <span class="hljs-built_in">int</span> in_size, <span class="hljs-built_in">int</span> out_size):<br>    cdef np.npy_intp in_dims[<span class="hljs-number">2</span>]<br>    cdef np.npy_intp out_dims[<span class="hljs-number">2</span>]<br>    in_dims = &#123;in_size,<span class="hljs-number">3</span>&#125;<br>    py_in_arr = np.PyArray_SimpleNewFromData(<span class="hljs-number">2</span>, in_dims, np.NPY_FLOAT, &lt;void*&gt; in_arr)<br>    out_dims = &#123;out_size,<span class="hljs-number">3</span>&#125;<br>    py_out_arr = np.PyArray_SimpleNewFromData(<span class="hljs-number">2</span>, out_dims, np.NPY_FLOAT, &lt;void*&gt; out_arr)<br>    py_interpolate(py_in_arr, py_out_arr, out_size)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">py_interpolate</span>(<span class="hljs-params"> np.ndarray[<span class="hljs-built_in">float</span>, ndim=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&quot;c&quot;</span>] in_arr <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                    np.ndarray[<span class="hljs-built_in">float</span>, ndim=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&quot;c&quot;</span>] out_arr <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                    <span class="hljs-built_in">int</span> out_size</span>):</span><br>    data = pd.DataFrame(in_arr, columns=[<span class="hljs-string">&#x27;lons&#x27;</span>,<span class="hljs-string">&#x27;lats&#x27;</span>,<span class="hljs-string">&#x27;values_&#x27;</span>])<br>    _min, _max = data.values_.<span class="hljs-built_in">min</span>(), data.values_.<span class="hljs-built_in">max</span>()<br>    data[<span class="hljs-string">&#x27;norm_values_&#x27;</span>] = data.values_.apply(<span class="hljs-keyword">lambda</span> x: x-_min/_max)<br>    basemap = Basemap(projection=<span class="hljs-string">&#x27;ortho&#x27;</span>,lat_0=<span class="hljs-number">30</span>,lon_0=<span class="hljs-number">120</span>)<br>    x, y = basemap(data.lons.values, data.lats.values)<br><br>    xi = np.linspace(<span class="hljs-built_in">min</span>(x)*<span class="hljs-number">0.8</span>, <span class="hljs-built_in">max</span>(x)*<span class="hljs-number">1.2</span>, <span class="hljs-number">300</span>)<br>    yi = np.linspace(<span class="hljs-built_in">min</span>(y)*<span class="hljs-number">0.8</span>, <span class="hljs-built_in">max</span>(y)*<span class="hljs-number">1.2</span>, <span class="hljs-number">300</span>)<br>    X, Y = np.meshgrid(xi, yi)<br>    Z = griddata( (x,y), data.norm_values_, (X,Y), method=<span class="hljs-string">&#x27;linear&#x27;</span>)<br><br>    res = np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=<span class="hljs-number">1</span>)<br>    res = np.delete(res, np.where(np.isnan(res))[<span class="hljs-number">0</span>], axis=<span class="hljs-number">0</span>)<br>    out_arr[...] = res[:out_size,:]<br></code></pre></td></tr></table></figure><br>这里我们使用了 numpy 的 <code>PyArray_SimpleNewFromData()</code> 接口将 C 语言的数组转换为 <code>numpy.ndarray</code>, 相反的操作可以通过 <code>PyArray_DATA()</code>返回数组指针，但是由于二维数组的指针的特殊性，本例没有选择使用指针作为返回值。<br>利用 cython 编译 <code>pyx</code> 文件后，生成 <code>.c</code>,<code>.h</code>文件，同样可以直接使用源文件或者编译成动态链接库，这里给出通过源文件使用接口的示例<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;Python.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;_Y_interpolate.h&quot;</span></span><br><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;string.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;malloc.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;math.h&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">read_csv</span><span class="hljs-params">(<span class="hljs-keyword">float</span> arr[][<span class="hljs-number">3</span>], <span class="hljs-keyword">int</span> line_cnt)</span></span>;<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span> *argv[])</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">int</span> in_size = <span class="hljs-number">48000</span>, out_size = <span class="hljs-number">48000</span>;<br>    <span class="hljs-keyword">float</span> in_arr[in_size][<span class="hljs-number">3</span>], out_arr[out_size][<span class="hljs-number">3</span>];<br>    read_csv(in_arr, in_size);<br>    Py_Initialize();<br>    PyInit__Y_interpolate();<br>    _interpolate(in_arr, out_arr, in_size, out_size);<br>    Py_Finalize();<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f, %f, %f&quot;</span>, out_arr[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], out_arr[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], out_arr[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>]);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><br>编译命令为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">gcc _Y_interpolate.c cppmain.c -m64 -mthreads -Wall -O3 \<br>-I. -ID:\Softwares\Anaconda3\include -ID:\Softwares\Anaconda3\lib\site-packages\numpy\core\include \<br>-LD:\Softwares\Anaconda3\ -LD:\Softwares\Anaconda3\Lib \<br>-lpython36 -o examples\cppmain<br><span class="hljs-comment"># 注意需要引用 numpy 的头文件</span><br></code></pre></td></tr></table></figure><br>至此，在 Cython 的帮助下，我们终于完成了一个 Python 代码的封装，熟悉这个步骤，就可以在实际项目中很轻松地利用 Python 进行开发了，利用运行效率换取开发效率。</p><h2 id="依赖解决"><a href="#依赖解决" class="headerlink" title="依赖解决"></a>依赖解决</h2><h3 id="0-依赖问题"><a href="#0-依赖问题" class="headerlink" title="0. 依赖问题"></a>0. 依赖问题</h3><p>试试运行按照上面步骤生成的 exe 文件，可能会出现提示 <code>找不到 Y_interpolate.dll</code>，将生成的 <code>Y_interpolate.dll</code> 文件复制到当前文件夹下，继续报错 <code>找不到 python36.dll</code>，同样将 <code>python36.dll</code> 复制到当前文件夹下。同时发布这三个文件，程序可以启动，但是并不一定能运行，如果电脑上的 <code>Anaconda</code> 被卸载，程序启动后会提示错误<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">Fatal Python error: Py_Initialize: unable <span class="hljs-built_in">to</span> <span class="hljs-built_in">load</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">system</span> codec<br>ModuleNotFoundError: No module named <span class="hljs-string">&#x27;encodings&#x27;</span><br></code></pre></td></tr></table></figure><br>显然这是 Python 的各种包引发的问题，那么怎么把程序中用到的 Python 包都打包到新的环境中呢？</p><h3 id="0-5-用户安装依赖"><a href="#0-5-用户安装依赖" class="headerlink" title="0.5 用户安装依赖"></a>0.5 用户安装依赖</h3><p>最简单解决依赖的方法就是在用户的计算机上安装 <code>Python</code>，然后通过 <code>pip</code> 安装需要的依赖，这种方法只需要建立虚拟环境，然后使用 <code>pip freeze</code>, <code>conda env export</code> 等命令就可以完成。<br>这种方法虽然简单，但是对于用户来说也更容易出错，有没有什么办法可以让开发者把所有依赖都打包都一个文件夹或打包成一个文件的方法呢。下面介绍两种可行的方法。</p><h3 id="1-Pyinstaller"><a href="#1-Pyinstaller" class="headerlink" title="1. Pyinstaller"></a>1. Pyinstaller</h3><p><code>Pyinstaller</code> 是一个将 Python 代码编译成 EXE 文件的工具，类似的工具还有 <code>cx_freeze</code>,<code>py2exe</code>, 这些工具生成 exe 时，会同时将 exe 所需要的依赖打包到一个文件夹中，将这个文件夹与生成的 DLL 一起发布，就可以解决依赖问题。</p><blockquote><p>尝试到一半，打包的体积有点儿大，网上有很多相关讨论，时间问题没有继续试验</p></blockquote><h3 id="2-虚拟环境"><a href="#2-虚拟环境" class="headerlink" title="2. 虚拟环境"></a>2. 虚拟环境</h3><p>这个方法与 <code>0.5</code> 中介绍的方法有些相似，只是不需要用户自己安装了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先创建虚拟环境</span><br>conda create -n dependency python=<span class="hljs-number">3.6</span><br><br>conda activate dependency<br>pip install numpy pandas scipy matplotlib ...<br></code></pre></td></tr></table></figure><br>安装完毕后，用户需要的依赖就全部都在<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs taggerscript">D:<span class="hljs-symbol">\S</span>oftwares<span class="hljs-symbol">\A</span>naconda3<span class="hljs-symbol">\e</span>nvs<span class="hljs-symbol">\d</span>ependency<span class="hljs-symbol">\D</span>LLs\<br>D:<span class="hljs-symbol">\S</span>oftwares<span class="hljs-symbol">\A</span>naconda3<span class="hljs-symbol">\e</span>nvs<span class="hljs-symbol">\d</span>ependency<span class="hljs-symbol">\L</span>ib\<br>D:<span class="hljs-symbol">\S</span>oftwares<span class="hljs-symbol">\A</span>naconda3<span class="hljs-symbol">\e</span>nvs<span class="hljs-symbol">\d</span>ependency<span class="hljs-symbol">\L</span>ib<span class="hljs-symbol">\s</span>ite-packages\<br></code></pre></td></tr></table></figure><br>这个三个目录里面了,运行时候将 <code>PYTHONPATH</code> 指向这三个子目录 或者 <code>set PYTHONHOME=D:\Softwares\Anaconda3\envs\dependency</code> 就可以成功运行程序了。<br>因此我们可以在 C 程序中加入如下代码<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-keyword">wchar_t</span>* py_home = Py_GetPythonHome();<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%S&quot;</span>, py_home);<br><span class="hljs-keyword">if</span>(py_home==<span class="hljs-literal">NULL</span>)&#123;<br>    Py_SetPythonHome(<span class="hljs-string">L&quot;./dependency&quot;</span>);<br>&#125;<br>Py_Initialize();<br>......<br></code></pre></td></tr></table></figure><br>这样就可以将 Python 所需要的依赖打包给用户，用户用 <code>PYTHONHOME</code> 环境变量或者 .pth 文件指定依赖的位置，或者直接放在 exe 所在目录即可。（注意路径中不要出现中文字符）</p><h2 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h2><ul><li>pyinstaller 打包 pandas</li><li>cython 编译 pyd</li><li>boost-python</li><li>swig</li><li>protocol buffer</li></ul>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>动态链接库</tag>
      
      <tag>Python</tag>
      
      <tag>混合编程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>argparser 模块</title>
    <link href="/2021/01/07/Python-20210107-argparser-%E6%A8%A1%E5%9D%97/"/>
    <url>/2021/01/07/Python-20210107-argparser-%E6%A8%A1%E5%9D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="Command-line-parse"><a href="#Command-line-parse" class="headerlink" title="Command line parse"></a>Command line parse</h1><p>argpaser 是 Python 官方推荐的 命令行参数解析器，习惯使用它可以极大地方便 Python程序的交互。<br><a id="more"></a></p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import argparse<br><br>args = argparse.<span class="hljs-constructor">ArgumentParser(<span class="hljs-params">descroption</span>=&#x27;Argpase <span class="hljs-params">example</span>&#x27;)</span><br>args.add<span class="hljs-constructor">_argument(&#x27;<span class="hljs-params">first</span>&#x27;, <span class="hljs-params">type</span>=<span class="hljs-params">str</span>, <span class="hljs-params">help</span>=&#x27;<span class="hljs-params">first</span> <span class="hljs-params">argument</span>&#x27;)</span><br><br>args = args.parse<span class="hljs-constructor">_args()</span><br>print(args.fisrt)<br></code></pre></td></tr></table></figure><h2 id="必选参数"><a href="#必选参数" class="headerlink" title="必选参数"></a>必选参数</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">args.add<span class="hljs-constructor">_argument(&#x27;<span class="hljs-params">first</span>&#x27;, <span class="hljs-params">type</span>=<span class="hljs-params">str</span>, <span class="hljs-params">help</span>=&#x27;<span class="hljs-params">first</span> <span class="hljs-params">argument</span>&#x27;)</span><br></code></pre></td></tr></table></figure><h2 id="可选参数"><a href="#可选参数" class="headerlink" title="可选参数"></a>可选参数</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">args.add<span class="hljs-constructor">_argument(&#x27;--<span class="hljs-params">optional</span>&#x27;, <span class="hljs-params">type</span>=<span class="hljs-params">str</span>, <span class="hljs-params">help</span>=&#x27;<span class="hljs-params">first</span> <span class="hljs-params">argument</span>&#x27;)</span><br></code></pre></td></tr></table></figure><h2 id="action"><a href="#action" class="headerlink" title="action"></a>action</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">args.add<span class="hljs-constructor">_argument(&#x27;--<span class="hljs-params">action_1</span>&#x27;, <span class="hljs-params">action</span>=&#x27;<span class="hljs-params">store_true</span>&#x27;, <span class="hljs-params">help</span>=&#x27;<span class="hljs-params">action_1</span> <span class="hljs-params">is</span> True&#x27;)</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>argparser</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 多线程</title>
    <link href="/2021/01/07/Python-20210107-Python-%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    <url>/2021/01/07/Python-20210107-Python-%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="0-Intro"><a href="#0-Intro" class="headerlink" title="0. Intro"></a>0. Intro</h3><p>多线程/多进程 在执行 <em>IO密集型</em> 或者 <em>CPU密集型</em> 的任务时，能够大大提高效率。Python 实现 多线程/多进程 的方式有很多，这里列举几个我见过的。</p><ul><li>thread/threading/Queue ： Python 官方库，接口比较简单</li><li>multiprocessing</li><li>concurrent ：对 Threading 进一步封装</li><li>joblib</li></ul><a id="more"></a><p>多线程是一个看起来很复杂，但是实际学习使用很简单的一个东西，所以当你有并发运算的需求的时候，一定要勤于动手，使用多线程/多进程去实现，这样会节省很多时间。<br>关于多线程与多进程的选择，一般的原则是 <strong><em>IO 密集型任务使用多线程， CPU密集型任务使用多进程</em></strong>，Python3 中好像还有 <strong>协程</strong> 的概念，属于比线程更加细粒度的并发，暂时没有研究过。<br>这篇文章主要记录 <strong><em>concurrent</em></strong> 库的简单使用。 Python2 中需要安装 future 库，Python3 的官方库中已经包含。</p><h3 id="concurrent"><a href="#concurrent" class="headerlink" title="concurrent"></a>concurrent</h3><p>concurrent 的接口非常简单：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_job</span>(<span class="hljs-params">obj</span>):</span><br>    print(obj)<br>    <span class="hljs-keyword">return</span> obj<br>    <br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor, ProcessPoolExecutor, Executor<br><span class="hljs-keyword">import</span> time<br><span class="hljs-comment"># 多线程</span><br>s = time.time()<br><span class="hljs-keyword">with</span> ThreadPoolExecutor(max_workers=<span class="hljs-number">8</span>) <span class="hljs-keyword">as</span> pool:<br>    results = <span class="hljs-built_in">list</span>( pool.<span class="hljs-built_in">map</span>(print_job, <span class="hljs-built_in">range</span>(<span class="hljs-number">100000</span>)) )<br>t1 = (time.time()-s)<br><br><span class="hljs-comment"># 多进程</span><br>s = time.time()<br><span class="hljs-keyword">with</span> ProcessPoolExecutor(max_workers=<span class="hljs-number">8</span>) <span class="hljs-keyword">as</span> pool:<br>    results = <span class="hljs-built_in">list</span>( pool.<span class="hljs-built_in">map</span>(print_job, <span class="hljs-built_in">range</span>(<span class="hljs-number">100000</span>)) )<br>t2 = (time.time()-s)<br><br><span class="hljs-comment"># 循环</span><br>s = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100000</span>):<br>    print_job(i)<br>t3 = (time.time()-s)<br><br>print(t1, t2, t3)<br><span class="hljs-number">8.88823914527893</span><br><span class="hljs-number">7.626605272293091</span><br><span class="hljs-number">2.69059157371521</span><br></code></pre></td></tr></table></figure><br>由于举了一个 非IO密集型，非 CPU 密集型 的例子，所以结果非常尴尬。总之，Python 的并发编程接口就是这样的。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>多线程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 中的 configure 设计</title>
    <link href="/2021/01/07/Python-20210107-Python-%E4%B8%AD%E7%9A%84-configure-%E8%AE%BE%E8%AE%A1/"/>
    <url>/2021/01/07/Python-20210107-Python-%E4%B8%AD%E7%9A%84-configure-%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="Build-configure-file-for-you-project"><a href="#Build-configure-file-for-you-project" class="headerlink" title="Build configure file for you project"></a>Build configure file for you project</h1><p>作为程序员，配置文件的作用和必要性无需赘言，很多优秀的开源项目也会配备一个 config 文件，下面介绍一些常见的 config 方法<br><a id="more"></a></p><hr><h2 id="Python-脚本"><a href="#Python-脚本" class="headerlink" title="Python 脚本"></a>Python 脚本</h2><p>Python 脚本是一种简单的配置方法。这种方法适用于一些自己写的小工程，一来它安全性不高，二来违背了配置与代码解耦的原则。  </p><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">############################################</span><br><span class="hljs-comment">#  databaseconfig.py</span><br><span class="hljs-comment">############################################</span><br><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-keyword">import</span> preprocessing<br>mysql = &#123;<span class="hljs-string">&#x27;host&#x27;</span>: <span class="hljs-string">&#x27;localhost&#x27;</span>,<br>        <span class="hljs-string">&#x27;user&#x27;</span>: <span class="hljs-string">&#x27;root&#x27;</span>,<br>        <span class="hljs-string">&#x27;passwd&#x27;</span>: <span class="hljs-string">&#x27;my secret password&#x27;</span>,<br>        <span class="hljs-string">&#x27;db&#x27;</span>: <span class="hljs-string">&#x27;write-math&#x27;</span>&#125;<br>preprocessing_queue = [preprocessing.scale_and_center,<br>                    preprocessing.dot_reduction,<br>                    preprocessing.connect_lines]<br>use_anonymous = <span class="hljs-literal">True</span><br><br><span class="hljs-comment">############################################</span><br><span class="hljs-comment">#  main.py</span><br><span class="hljs-comment">############################################</span><br><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-keyword">import</span> databaseconfig <span class="hljs-keyword">as</span> cfg<br>connect(cfg.mysql[<span class="hljs-string">&#x27;host&#x27;</span>], cfg.mysql[<span class="hljs-string">&#x27;user&#x27;</span>], cfg.mysql[<span class="hljs-string">&#x27;password&#x27;</span>])<br></code></pre></td></tr></table></figure></code></pre><hr><h2 id="YAML-文件"><a href="#YAML-文件" class="headerlink" title="YAML 文件"></a>YAML 文件</h2><ul><li>yaml 文件也是一种很流行的配置方法，相比于使用 Python 脚本，部署的时候， yaml 文件可以做到与代码解耦，因此更友好。相比于 json 文件，yaml 文件更易于阅读修改，除非有特殊需求，一般情况使用 yaml 文件配置工程是很好的选择。  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">######################</span><br><span class="hljs-comment">#  databaseconfig.yml</span><br><span class="hljs-comment">######################</span><br>mysql:<br>    host: localhost<br>    user: root<br>    passwd: my secret password<br>    db: write-math<br>other:<br>    preprocessing_queue:<br>        - preprocessing.scale_and_center<br>        - preprocessing.dot_reduction<br>        - preprocessing.connect_lines<br>    use_anonymous: yes<br><br><br><span class="hljs-comment">######################</span><br><span class="hljs-comment">#  main.py</span><br><span class="hljs-comment">######################</span><br><span class="hljs-keyword">import</span> yaml<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;config.yml&quot;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> ymlfile:<br>    cfg = yaml.load(ymlfile)<br><br><span class="hljs-keyword">for</span> section <span class="hljs-keyword">in</span> cfg:<br>    print(section)<br>print(cfg[<span class="hljs-string">&#x27;mysql&#x27;</span>])<br>print(cfg[<span class="hljs-string">&#x27;other&#x27;</span>])<br><br></code></pre></td></tr></table></figure></li><li>yaml 文件里面还可以指定 item 的数据类型<br><a href="http://huangro.iteye.com/blog/372976">数据类型对应表</a>  <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">video_size:</span> <span class="hljs-type">!!python/tuple</span> [<span class="hljs-number">1280</span>, <span class="hljs-number">720</span>]  <span class="hljs-comment"># 注意后面是中括号</span><br><span class="hljs-attr">stride:</span> <span class="hljs-number">300</span><br><span class="hljs-attr">history:</span> <span class="hljs-number">10</span><br><span class="hljs-attr">mask_n:</span> <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="configparser"><a href="#configparser" class="headerlink" title="configparser"></a>configparser</h2><p>configparser 是 Python 的一个模块。</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>namedtuple 模块</title>
    <link href="/2021/01/07/Python-20210107-namedtuple-%E6%A8%A1%E5%9D%97/"/>
    <url>/2021/01/07/Python-20210107-namedtuple-%E6%A8%A1%E5%9D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="Namedtuple-A-useful-data-structure-for-explicit-coding"><a href="#Namedtuple-A-useful-data-structure-for-explicit-coding" class="headerlink" title="Namedtuple: A useful data structure for explicit coding"></a>Namedtuple: A useful data structure for explicit coding</h2><p>namedtuple 是 collections 包中的一个数据结构，可以很方便的存储从 csv, 数据库 中读取到的数据，<strong>这种数据的特点是，数据分为多条记录，每一条记录包含若干字段。</strong></p><a id="more"></a><p>下面以一个班级学生的成绩表为例讲解一下，成绩表中包含 学号，姓名，成绩 三个字段</p><h3 id="list-dict-存储"><a href="#list-dict-存储" class="headerlink" title="list/dict 存储"></a>list/dict 存储</h3><p>如果对 Python 的各种数据类型接触不多，很容易会选择使用 list 或者 dict 去存储。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python">raw_data = [<br>    [ 01, <span class="hljs-string">&#x27;jack&#x27;</span>, <span class="hljs-string">&#x27;80&#x27;</span>],<br>    [ 02, <span class="hljs-string">&#x27;mike&#x27;</span>, <span class="hljs-string">&#x27;82&#x27;</span>],<br>    [ 03, <span class="hljs-string">&#x27;mary&#x27;</span>, <span class="hljs-string">&#x27;96&#x27;</span>],<br>]<br><br>list_sheet = []<br><span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> raw_data:<br>    list_sheet.append(record)<br><span class="hljs-comment"># 这种方法的缺点是，每一条记录的各个字段具有什么含义并不清晰</span><br><br><span class="hljs-comment"># 列表嵌字典</span><br>dict_sheet = []<br>dict_record = &#123;<br>    <span class="hljs-string">&#x27;stu_id&#x27;</span>: <span class="hljs-literal">None</span>,<br>    <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-literal">None</span>,<br>    <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-literal">None</span><br>&#125;<br><span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> raw_data:<br>    _id, _name, _score = record<br>    dict_record[<span class="hljs-string">&#x27;stu_id&#x27;</span>] = _id<br>    dict_record[<span class="hljs-string">&#x27;name&#x27;</span>] = _name<br>    dict_record[<span class="hljs-string">&#x27;score&#x27;</span>] = _score<br>    dict_sheet.append(dict_record)<br>   <br><span class="hljs-comment"># 字典嵌列表 </span><br>dict_sheet2 = &#123;<br>    <span class="hljs-string">&#x27;stu_id&#x27;</span>: [],<br>    <span class="hljs-string">&#x27;name&#x27;</span>: [],<br>    <span class="hljs-string">&#x27;score&#x27;</span>: [],<br>&#125;<br><span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> raw_data:<br>    _id, _name, _score = record<br>    dict_sheet2[<span class="hljs-string">&#x27;stu_id&#x27;</span>].append(_id)<br>    dict_sheet2[<span class="hljs-string">&#x27;name&#x27;</span>].append(_name)<br>    dict_sheet2[<span class="hljs-string">&#x27;score&#x27;</span>].append(_score)<br><span class="hljs-comment"># 这两种方法代码写起来比较麻烦，而且如果字典的 value 是 tuple/list 类型理解起来会有一些困难</span><br></code></pre></td></tr></table></figure></p><h3 id="class-方法"><a href="#class-方法" class="headerlink" title="class 方法"></a>class 方法</h3><p>熟练使用 C/C++ 的人，比较容易想到用 class 来存储数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Record</span>():</span><br>    stu_id = <span class="hljs-literal">None</span><br>    name = <span class="hljs-literal">None</span><br>    score = <span class="hljs-literal">None</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, _id, _name, _score</span>):</span><br>        self.stu_id = _id<br>        self.name = _name<br>        self.score = _score<br>    <br>sheet = []<br><span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> raw_data:<br>    sheet.append(Record(**record))<br><span class="hljs-comment"># 这种方法需要定义出来一个 class，代码同样显得比较麻烦</span><br></code></pre></td></tr></table></figure></p><h3 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple"></a>namedtuple</h3><p>实际上，这种场景最合适的数据结构就是 namedtuple，namedtuple 可以理解为用最简单的代码去定义一个 Record <code>Class</code><br><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs delphi">from collections import namedtuple<br><br><span class="hljs-keyword">Record</span> = namedtuple(<span class="hljs-string">&#x27;Record&#x27;</span>, [<span class="hljs-string">&#x27;stu_id&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>])<br><br>sheet = []<br><span class="hljs-keyword">for</span> <span class="hljs-keyword">record</span> <span class="hljs-keyword">in</span> raw_data:<br>    sheet.append(<span class="hljs-keyword">Record</span>._make(<span class="hljs-keyword">record</span>))<br>    <br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> sheet:<br>    print(r.stu_id, r.<span class="hljs-keyword">name</span>, r.score)<br># 相比之下，这种方法显得非常 explicit，习惯使用这种数据结构，处理 csv 数据的时候可以大大提高代码可读性。<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 中的 import 机制</title>
    <link href="/2021/01/07/Python-20210107-Python-%E4%B8%AD%E7%9A%84-import-%E6%9C%BA%E5%88%B6/"/>
    <url>/2021/01/07/Python-20210107-Python-%E4%B8%AD%E7%9A%84-import-%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<ul><li><p>module import </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">+ A/<br>    - a.py<br></code></pre></td></tr></table></figure><pre><code>  import m  from m import *  <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>m.py<br></code></pre></td></tr></table></figure>  # empty  <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">        </span><br>&#x27;&#x27;&#x27;<br>最简单的 import 情况<br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure></code></pre><a id="more"></a></li><li><p>package import</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">+ A/<br>    - a.py<br></code></pre></td></tr></table></figure><pre><code>  import m  from m import *  import C  from C import *  <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>m.py<br></code></pre></td></tr></table></figure>  import C  from C import *  <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">+</span> C/<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">__init__</span>.py<br></code></pre></td></tr></table></figure>      # empty      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">        </span><br>&#x27;&#x27;&#x27;<br>package 的 import<br><br><span class="hljs-code">  Q - ImportError: No module named C</span><br><span class="hljs-code">  A : python2 中，package 文件夹下必须有 __init__.py</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure></code></pre></li><li><p>relative import</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">##################################################################################</span><br>    <span class="hljs-string">&#x27;import 关键字的 相对导入&#x27;</span><br>+ X/<br>    + A/<br>        - __init__.py<br>        - a.py<br></code></pre></td></tr></table></figure><pre><code>      import m      from m import *      import C      from C import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>m.py<br></code></pre></td></tr></table></figure>      import C      from C import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">&#x27;&#x27;&#x27;<br>在 A 的 父级目录执行 python -c &quot;import A.a&quot;<br><br><span class="hljs-code">    a.py:1 --&gt; import m</span><br><span class="hljs-code">    Q - ModuleNotFoundError: No module named &#x27;m&#x27;</span><br><span class="hljs-code">    A : import &lt;pkg/module&gt;， pkg/module 必须在 PYTHONAPTH 路径下，所以这里有两种解决方案，</span><br><span class="hljs-code">        一种是将目录 A 的路径加入 PYTHONPATH；一种是改成 import A.m。  </span><br><span class="hljs-code">        很明显后一种改法固定了调用脚本与 packageA 的相对路径关系，因此只有在 A 作为 X 的一个 subpackage 才可以这么写；而 PYTHONPATH 又跟代码所在的路径相关，因此也不太实用。</span><br><span class="hljs-code">        所以这种情况下就需要用 from &lt;&gt; import &lt;&gt; 语法，这种语法支持 relative import</span><br>&#x27;&#x27;&#x27;<br><br><br>###################################################################################<br><span class="hljs-code">    &#x27;from ... import ... 的相对导入&#x27;</span><br>+ X/<br><span class="hljs-code">    + A/</span><br><span class="hljs-code">        - __init__.py</span><br><span class="hljs-code">        - a.py</span><br></code></pre></td></tr></table></figure>      # import A.m      from m import *      # import A.C      from C import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>m.py<br></code></pre></td></tr></table></figure>      # import A.C      from C import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">        + C/</span><br>&#x27;&#x27;&#x27;<br>修正好 import m/C 的错误之后，继续执行 python -c &quot;import A.a&quot;<br><br><span class="hljs-code">    a.py:1 --&gt; import A.m</span><br><span class="hljs-code">    m.py:2 --&gt;  from C import *</span><br><span class="hljs-code">    Q - ModuleNotFoundError: No module named &#x27;m&#x27;</span><br><span class="hljs-code">    A : 这里可以通过改 PYPATHPATH 消除这个错误；</span><br><span class="hljs-code">        如果不改环境变量的话，from ... import ... 语法支持相对导入，改成 </span><br><span class="hljs-code">            from .C import *    # 这里的 .C 是相对于 __package__ 变量的, 这一句等价于下面</span><br><span class="hljs-code">            from __package__.C    # 由于执行的语句是 import A.m， 所以这里的 __package__=&#x27;A&#x27;，所以等价于下面</span><br><span class="hljs-code">            from A.C import *</span><br>&#x27;&#x27;&#x27;<br><br><br>##################################################################################<br><span class="hljs-code">    &#x27;from ... import ... 的导入(2)&#x27;</span><br>+ X/<br><span class="hljs-code">    + A/</span><br><span class="hljs-code">        - __init__.py</span><br><span class="hljs-code">        - a.py</span><br></code></pre></td></tr></table></figure>      # import A.m      from .m import *      # import A.C      from .C import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>m.py<br></code></pre></td></tr></table></figure>      # import A.C      from .C import *      <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">    +</span> C/<br><span class="hljs-bullet">+</span> B/<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">__init__</span>.py<br><span class="hljs-bullet">    -</span> b.py<br></code></pre></td></tr></table></figure>      from ..A import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">&#x27;&#x27;&#x27;<br>将 A 中的导入全部都变成相对导入之后，试一下在 B 里面导入 package A，<br>执行 python -c &quot;import B.b&quot;<br><br><span class="hljs-code">    --- b.py:1 --&gt; from ..A import *</span><br><span class="hljs-code">    Q - ValueError: attempted relative import beyond top-level package</span><br><span class="hljs-code">    A : 这是由于 b.py 中的 import 语句使用了上一级的 package，而调用语句是 import B.b，只有一层 package，</span><br><span class="hljs-code">        要避免这个错误，需要将 X 作为一个 package 运行： </span><br><span class="hljs-code">            python -c &quot;import X.B.b&quot;</span><br>&#x27;&#x27;&#x27;<br><br><br>###################################################################################<br><span class="hljs-code">    &#x27;module 的执行&#x27;</span><br>+ X/<br><span class="hljs-code">    + A/</span><br><span class="hljs-code">        - __init__.py</span><br><span class="hljs-code">        - a.py</span><br></code></pre></td></tr></table></figure>      # import A.m      from .m import *      # import A.C      from .C import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>m.py<br></code></pre></td></tr></table></figure>      # import A.C      from .C import *      <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">    +</span> C/<br><span class="hljs-bullet">+</span> B/<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">__init__</span>.py<br><span class="hljs-bullet">    -</span> b.py<br></code></pre></td></tr></table></figure>      from ..A import *      <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">&#x27;&#x27;&#x27;<br>先总结一下，当你实现一个模块 a.py 后，如果你想在其他路径下写代码调用这个模块，就会用到形如 import A.a 这样的语句，<br>但是这种情况下，a.py 当中的 import 语句可能会报错。<br>因为 import 语句只支持绝对导入，所以这种情况要用 from .. import ..，并且将其改成相对导入的形式。<br><br>但是这种情况下，你再回到文件夹 A，却无法执行 python a.py 了<br><br><span class="hljs-code">    --- a.py:2 --&gt; from .m import *</span><br><span class="hljs-code">    Q - ModuleNotFoundError: No module named &#x27;__main__.m&#x27;; &#x27;__main__&#x27; is not a package</span><br><span class="hljs-code">    A : 执行 import A.a 的时候，a 作为一个 module，from 语句从 __package__ 变量相对的路径下去 import</span><br><span class="hljs-code">        执行 python A/a.py 的时候， a 作为一个 脚本，__package__=None，from 语句从 __name__ 的相对路径下去 import</span><br><span class="hljs-code">    </span><br>这时正确的做法是将 a.py 作为一个 module 执行。<br><span class="hljs-code">    python [-i] -m A.a    # 用了这么久才知道 python 有个 -i 选项 !!!!!!!!</span><br>&#x27;&#x27;&#x27;<br><br></code></pre></td></tr></table></figure></code></pre></li></ul>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python tips</title>
    <link href="/2021/01/07/Python-20210107-Python-tips/"/>
    <url>/2021/01/07/Python-20210107-Python-tips/</url>
    
    <content type="html"><![CDATA[<p>收集一些短小的 Python 代码段<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 查看磁盘占用</span><br><span class="hljs-keyword">import</span> os<br>vfs = os.statvfs(<span class="hljs-string">&#x27;/&#x27;</span>)<br>vfs<br>total = vfs.f_blocks * statvfs.f_bsize<br>used = vfs.f_bsize * (vfs.f_blocks - vfs.f_bfree)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 输出保留 2 位小数</span><br>a = <span class="hljs-number">3.1415926</span><br>print( <span class="hljs-built_in">round</span>(a,<span class="hljs-number">2</span>) )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 查看文件类型</span><br>improt filetype<br><br>f = <span class="hljs-string">&#x27;1.txt&#x27;</span><br>kind = filetype.guess(f)<br>print(kind.mime)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux .so 文件编译</title>
    <link href="/2021/01/07/C-20210107-Linux-so-%E6%96%87%E4%BB%B6%E7%BC%96%E8%AF%91/"/>
    <url>/2021/01/07/C-20210107-Linux-so-%E6%96%87%E4%BB%B6%E7%BC%96%E8%AF%91/</url>
    
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/tzhangofseu/archive/2011/11/15/2249585.html">http://www.cnblogs.com/tzhangofseu/archive/2011/11/15/2249585.html</a></p><a id="more"></a><hr><h1 id="so生成"><a href="#so生成" class="headerlink" title=".so生成"></a>.so生成</h1><ul><li>在 soDemo.cpp文件中实现函数<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;iostrem&gt;</span></span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">fun</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span>&lt;&lt;<span class="hljs-string">&quot;hello&quot;</span>&lt;&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li>使用 g++ 编译动态链接库<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">g++ soDemo.cpp -shared -fPIC -o libsoDemo.so<br></code></pre></td></tr></table></figure></li></ul><hr><h1 id="so使用"><a href="#so使用" class="headerlink" title=".so使用"></a>.so使用</h1><ul><li>在 socallDemo.cpp文件中直接调用库函数<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">fun</span><span class="hljs-params">()</span></span>;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    fun();<br>    retrun <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li>使用 g++ 编译程序，在命令行参数中指定 .so文件<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">g++ socallDemo.cpp -L. -lsoDemo -o socallDemo<br><span class="hljs-comment"># -L. 指定 .so 文件路径为当前目录</span><br><span class="hljs-comment"># -lsoDemo 指定 .so 文件的名称为 libsoDemo.so</span><br></code></pre></td></tr></table></figure></li><li>运行程序的时候需要让 loader 能够找到 .so文件<br><a href="http://blog.csdn.net/sahusoft/article/details/7388617">http://blog.csdn.net/sahusoft/article/details/7388617</a><br>方法1. 修改 /etc/ld.so.conf 文件，在其中加入 .so文件所在路径<br>方法2. 修改环境变量 LD_LIBRARY_PATH<br><code>export LD_LIBRARY_PATH= $path_to_lib:$LD_LIBRARY_PATH</code></li></ul>]]></content>
    
    
    <categories>
      
      <category>C++</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>动态链接库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DLL 编译</title>
    <link href="/2021/01/07/C-20210107-DLL-%E7%BC%96%E8%AF%91/"/>
    <url>/2021/01/07/C-20210107-DLL-%E7%BC%96%E8%AF%91/</url>
    
    <content type="html"><![CDATA[<p><a href="http://www.jellythink.com/archives/111">http://www.jellythink.com/archives/111</a><br><a href="http://www.cnblogs.com/fangyukuan/archive/2010/06/20/1761464.html">http://www.cnblogs.com/fangyukuan/archive/2010/06/20/1761464.html</a></p><a id="more"></a><hr><h1 id="dll生成"><a href="#dll生成" class="headerlink" title="dll生成"></a>dll生成</h1><ul><li>使用关键字声明导出函数<ul><li>新建 win32 项目，项目名为 <em>dllDemo</em></li><li>在 <em>dllDemo.cpp</em> 中编写代码</li><li>导出函数的代码用关键字 <em>__declspec(dllexport)</em> 声明<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-keyword">extern</span> <span class="hljs-string">&quot;C&quot;</span> __declspec(dllexport) <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">SayHello</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>  <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span>&lt;&lt;<span class="hljs-string">&quot;This function is exported from .dll&quot;</span>&lt;&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;<br>  ::MessageBoxW(<span class="hljs-literal">NULL</span>, <span class="hljs-string">L&quot;Hello&quot;</span>, <span class="hljs-string">L&quot;fangyukuan&quot;</span>, MB_OK);<br>&#125;<br></code></pre></td></tr></table></figure></li><li>编译 win32 项目</li></ul></li><li>使用.def文件声明导出函数<ul><li>新建 win32 项目，项目名为 <em>dllDemo</em></li><li>在 <em>dllDemo.cpp</em> 中编写代码</li><li>导出函数不再需要关键字声明<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">SayHello</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>  <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span>&lt;&lt;<span class="hljs-string">&quot;This function is exported from .dll&quot;</span>&lt;&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;<br>  ::MessageBoxW(<span class="hljs-literal">NULL</span>, <span class="hljs-string">L&quot;2_DLLDemo::Hello&quot;</span>, <span class="hljs-string">L&quot;fangyukuan&quot;</span>, MB_OK);<br>&#125;<br></code></pre></td></tr></table></figure></li><li><strong>新建 <em>.def</em> 文件</strong><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">LIBRARY</span>    <span class="hljs-string">&quot;dllDemo&quot;</span><br>EXPORTS<br>  SayHello<br></code></pre></td></tr></table></figure></li><li>编译 win32 项目</li></ul></li></ul><hr><h1 id="dll使用"><a href="#dll使用" class="headerlink" title="dll使用"></a>dll使用</h1><ul><li><p>隐式链接<br>隐式链接在<strong><em>项目配置</em></strong>中指定导入函数（即 .dll 文件中的导出函数），代码中只需要声明导入函数就可以调用</p><ul><li>建立 win32控制台项目</li><li>声明并调用导入函数<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">extern <span class="hljs-string">&quot;C&quot;</span> <span class="hljs-constructor">__declspec(<span class="hljs-params">dllimport</span>)</span> void <span class="hljs-constructor">SayHello(<span class="hljs-params">void</span>)</span>;<br><span class="hljs-built_in">int</span> <span class="hljs-constructor">_tmain(<span class="hljs-params">int</span> <span class="hljs-params">argc</span>, <span class="hljs-params">_TCHAR</span><span class="hljs-operator">*</span> <span class="hljs-params">argv</span>[])</span><br>&#123;<br>  <span class="hljs-constructor">SayHello()</span>;<br>  return <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li>在工程配置中指定 .lib文件的路径<br>方法1. 属性-&gt;链接器-&gt;常规-&gt;附加库目录：指向.lib文件的路径;   属性-&gt;链接器-&gt;输入-&gt;附加依赖项：.lib文件的文件名<br>方法2. 在代码中指定导入库文件.lib<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> comment(lib, <span class="hljs-meta-string">&quot;dllDemo.lib&quot;</span>)</span><br></code></pre></td></tr></table></figure></li><li>编译运行</li></ul></li><li><p>显式链接<br>显式链接在<strong><em>代码</em></strong>中加载.dll文件<br><strong>显示链接的时候不需要提供 .lib文件；但是，如果 .dll中存在依赖项，并且依赖项没有被导入，.dll会加载失败</strong></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">typedef void (*SayHello)<span class="hljs-literal">()</span>;<br><span class="hljs-built_in">int</span> <span class="hljs-constructor">_tmain(<span class="hljs-params">int</span> <span class="hljs-params">argc</span>, <span class="hljs-params">_TCHAR</span><span class="hljs-operator">*</span> <span class="hljs-params">argv</span>[])</span><br>&#123;<br>  HMODULE hDll = <span class="hljs-constructor">LoadLibrary(<span class="hljs-string">&quot;dllDemo.dll&quot;</span>)</span>;<br>  <span class="hljs-keyword">if</span> (hDll != NULL)<br>  &#123;<br>      SayHello  sayhello_proc = (SayHello)<span class="hljs-constructor">GetProcAddress(<span class="hljs-params">hDll</span>, <span class="hljs-string">&quot;SayHello&quot;</span>)</span>;<br>          <span class="hljs-keyword">if</span> (sayhello_proc != NULL)<br>          &#123;<br>              sayhello<span class="hljs-constructor">_proc()</span>;    <br>          &#125;<br>          <span class="hljs-constructor">FreeLibrary(<span class="hljs-params">hDll</span>)</span>;<br>    &#125;<br>  &#125;<br></code></pre></td></tr></table></figure></li></ul><h1 id="查看-dll-导出函数："><a href="#查看-dll-导出函数：" class="headerlink" title="查看 dll 导出函数："></a>查看 dll 导出函数：</h1><p><code>dumpbin -exports &lt;*.dll&gt;</code></p>]]></content>
    
    
    <categories>
      
      <category>C++</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>动态链接库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C3D User Guide</title>
    <link href="/2021/01/07/Caffe-20210107-C3D-User-Guide/"/>
    <url>/2021/01/07/Caffe-20210107-C3D-User-Guide/</url>
    
    <content type="html"><![CDATA[<p>C3D User Guide<br>Du Tran (Last modified Mar 20, 2017)</p><p>C3D-v1.1 is released with new models (Mar 01, 2017).</p><p>●     No documentation for v1.1 yet, but some examples for feature extraction, training, and fine-tuning are provided.</p><a id="more"></a><p>The below guide was written for C3D-v1.0<br>I. C3D Feature Extraction<br>If you have installed C3D successfully (same as install caffe and its dependences), following the following steps:</p><ul><li>Download pre-trained model and save it to YOUR_C3D_HOME/examples/c3d_feature_extraction</li><li>Change directory to YOUR_C3D_HOME/examples/c3d_feature_extraction</li><li>Run: sh c3d_sport1m_feature_extraction_frm.sh or sh c3d_sport1m_feature_extraction_video.sh</li></ul><p>If you can run the examples successfully, then you should find the extracted features in the output folders.</p><p>If you run to “out of memory” error, then you should consider to reduce the batch-size (see section I.B)</p><p>If you can run feature extraction with frames successfully, but fail with the video inputs. The cause may come from video codecs. Make sure you had compiled your OpenCV and Ffmpeg with shared-flags are on.</p><ul><li>make sure that ‘shuffle: false’ in your data layer when you use C3D as a feature extractor. This help us to keep the correspondences between the input clips and the output features.</li></ul><p>I.A Extract C3D features for your own videos or frames<br>Prepare your input files</p><ul><li><p>C3D allows you to use video inputs as sequences of frames or video files. In the case of video files (.mp4, .avi, .mov), make sure that your machine has codecs, opencv, and ffmpeg installed properly. In the case of using frames, C3D assumes that each video is a folder with frames which are numbered starting from 1 to N (number of frames). The frame names are formatted as “video_folder/%06d.jpg”.</p></li><li><p>Note that: frame numbers starting from 1 (e.g. 1..N) for using frame as inputs, and starting from 0 (e.g. [0..N-1]) for using video as inputs.</p></li></ul><p>Prepare your setting files</p><ul><li>There are two setting files you need to prepare: input-list and output prefix. In the provided example, they are: input_list_frm.txt, input_list_video.txt, and output_list_prefix.txt in YOUR_C3D_HOME/examples/c3d_feature_extraction/prototxt</li><li>The input list file is a text file where each line contain information for a clip that you are inputting into C3D for extracting features. Each line has the following format:</li></ul><p><string_path> <starting_frame> <label></label></starting_frame></string_path></p><p>where <label> is only used for training, testing, or fine-tuning, but NOT for extracting features, thus can be ignored (in the provided example, they are filled with 0s). For <string_path>, we have two cases. For the setting with video file inputs, <string_path> is the full path and filename of the video (e.g. input/avi/v_ApplyEyeMakeup_g01_c01.avi). For the setting with frame inputs, <string_path> is the full path to the folder containing frames of the video (e.g. input/frm/v_ApplyEyeMakeup_g01_c01/). Finally, the <starting_frame> is used to specify the starting frame of the clip. We note that C3D extract feature of 16-frame-long clips. For example, if starting frame is 1, then you are extracting features for the clip (from the video specified by <string_path>) from frame 1 to 16. If starting frame is 17, then the clip of interest is from frame 17 to 32. Note that in the provided examples, we have sampled clips from videos with step size (or stride) of 16 frames. You can use different sampling step-size: e.g. as dense as every 1 frame or sparser e.g. every 32 frames.</string_path></starting_frame></string_path></string_path></string_path></label></p><ul><li>The output prefix file is used to specify the locations for extracting features to be saved. Each line is formatted as</li></ul><output_prefix><p>Each line in the prefix file is corresponded to a line in the input list file (in the same order, e.g. line 1 in prefix file is the output prefix for the clip of line 1 in the list file). C3D will save features are output_prefix.[feature_name] (e.g. prefix.fc6). It is recommend that for each video, you should create an output folder and the prefix lines are formatted as sprintf(“output_folder/%06d”, starting_frame). That means each clip has its starting frame as identifier and file extensions are used for different features. Remember to create output folders, as C3D does not create them.</p><p>Extract C3D features</p><ul><li><p>Assume that you have prepared your setting files, then you need to modify the prototxt file to point to the input list file. In the prototxt file, looks for line:</p><p>source: “prototxt/input_list_frm.txt”</p></li></ul><p>Also remember set the use_image: true if you use images as inputs or false if use videos as inputs.</p><ul><li>Use extract_image_features tool to extract features. The arguments used by this tools is follow:</li></ul><p>extract_image_features.bin <feature_extractor_prototxt_file> <c3d_pre_trained_model> <gpu_id> <mini_batch_size> <number_of_mini_batches> <output_prefix_file> <feature_name1> <feature_name2> …</feature_name2></feature_name1></output_prefix_file></number_of_mini_batches></mini_batch_size></gpu_id></c3d_pre_trained_model></feature_extractor_prototxt_file></p><pre><code>   In which:</code></pre><ul><li><feature_extractor_prototxt_file>: is prototxt file (provided in example) which points to your input list file.</feature_extractor_prototxt_file></li><li><c3d_pre_trained_model>: is the C3D pre-trained model that you downloaded.</c3d_pre_trained_model></li><li><gpu_id>: GPU ID you would like to run (starting from 0), if this is set to -1, then it will use CPU.</gpu_id></li><li><mini_batch_size>: your mini batch size. Default is 50, but you can modify this number, depend on your GPU memory.</mini_batch_size></li><li><number_of_mini_batches>: Number of mini-batches you want to extract features. For examples, if you have 100 clips to extract features and you are using mini-batch size of 50, then this parameter should be set to 2. However, if you have 101 clips to be extracted features, then this number should be set to 3.</number_of_mini_batches></li><li><output_prefix_file>: Your output prefix file.</output_prefix_file></li><li><feature_name1>: You can list as many feature names as possible as long as they are in the names of the output blobs of the network (see prototxt file for all layers, but they look like fc6-1, fc7-1, fc8-1, pool5, conv5b, prob,…).</feature_name1></li></ul><p>You can find the following command line provided in the example as below:</p><p>GLOG_logtosterr=1 ../../build/tools/extract_image_features.bin prototxt/c3d_sport1m_feature_extractor_frm.prototxt conv3d_deepnetA_sport1m_iter_1900000 0 50 1 prototxt/output_list_prefix.txt fc7-1 fc6-1 prob</p><p>I.B Extract C3D features with smaller or larger batch-size</p><p>In case you have more or less memory, you can adjust the mini-batch size (larger or smaller than 50). To do that, you need to change this parameter in the prototxt file of the network (find line e.g. batch_size: 50). And you also need to input the newly-adjust parameters of <mini_batch_size> and <number_of_mini_batches> in the command line.</number_of_mini_batches></mini_batch_size></p><p>After extracted C3D features, you can use the provided MATLAB script (read_binary_blob.m ) to read the features for further analysis.</p><p>II. Train 3D ConvNet<br>A. Compute volume mean from list<br>This tool allows you to compute volume mean for you own dataset which can be useful for both training from scratch or fine-tuning C3D on your own dataset.</p><p>Usage:<br>GLOG_logtostderr=1 compute_volume_mean_from_list input_chunk_list length height width sampling_rate output_file [dropping rate]</p><p>Arguments:<br>input_chunk_list: the same as the list file used in feature extraction<br>length: the length of the clip used in training (e.g. 16)<br>height, width: size of frame e.g. 128, 171<br>sampling_rate: this is used to adjust the frame rate in you clip (e.g. clip length=16, sampling=1, then your clip is a 16-consecutive frame video chunk. Or if clip length=16, while sampling rate=2, then you clip is 32-frame long clips, but you sample 1 of every 2 frames)<br>output_file: the output mean file.<br>dropping_rate: In case you dataset is too large (e.g. 1M), you may want to compute the mean from a subset of your clips. Setting this to n, meaning the dropping rate is 1:n, choose 1 sample among every n clips for computing mean.</p><p>If you prefer to use mean_value, instead of volume_mean file, then you can set this mean_value field in your data layer. This is equivalent to the volume mean with all values are set to mean_value.</p><p>B. Train your own network from scratch<br>Assume you have your input_data_list, your train/test prototxt and your solver prototxt, you can use train_net to train the network.</p><p>C. An example of training from scratch on UCF101</p><ul><li>Change directory to YOUR_C3D_HOME/examples/c3d_train_ucf101/</li><li>run sh create_volume_mean.sh to compute the volume mean file</li><li>run sh train_ucf101.sh to train, expect a couple days to finish</li><li>run sh test_ucf101.sh to test, expect about 15’ to complete and you should have ~45% accuracy (this is clip accuracy)</li></ul><p>III. Fine-tune C3D<br>Assume you have download the C3D pre-trained model. You can try the fine-tuning example, by:</p><ul><li>Change directory to YOUR_C3D_HOME/examples/c3d_finetuning</li><li>Run: sh ucf101_finetuning.sh</li><li>When fine-tuning is done, you can test your fine-tuned model by running: sh ucf101_testing.sh</li><li>[Added 05/10/2016] In case you don’t have time to fine-tune C3D on UCF101 yourself, here we provide the C3D model fine-tuned on UCF101: <a href="https://www.dropbox.com/s/mkc9q7g4wnqnmcv/c3d_ucf101_finetune_whole_iter_20000">https://www.dropbox.com/s/mkc9q7g4wnqnmcv/c3d_ucf101_finetune_whole_iter_20000</a> Simply download this model to YOUR_C3D_HOME/examples/c3d_finetuning and run sh ucf101_testing.sh (assume that you have made sure your test_01.lst file points to your UCF101 frames). This will give an accuracy of 80.19% (clip accuracy). NOTE: this model is fine-tuned on UCF101 “train split 1”, thus it is only valid to test on “test split 1”.</li></ul><p>FAQs<br>●     Do we have MATLAB or Python wrappers for extracting C3D features?<br>       Unfortunately, we don’t have them yet.<br>●     Can I use C3D on a CPU?<br>This version of C3D is built on an old caffe  branch, so there is no ‘CPU_ONLY’ mode in<br>Makefile. But you can do that by the following:</p><ul><li>Compile C3D as normal (it requires CUDA driver to compile, but if you don’t have GPU, you still can run on a CPU).</li><li>To train using CPU, you can modify solver file to solver_mode: CPU (see here <a href="https://github.com/facebook/C3D/blob/master/examples/c3d_train_ucf101/conv3d_ucf101_solver.prototxt#L19">https://github.com/facebook/C3D/blob/master/examples/c3d_train_ucf101/conv3d_ucf101_solver.prototxt#L19</a>)</li><li>To test using CPU, in the command line use CPU instead of GPU (<a href="https://github.com/facebook/C3D/blob/master/examples/c3d_train_ucf101/test_ucf101.sh">https://github.com/facebook/C3D/blob/master/examples/c3d_train_ucf101/test_ucf101.sh</a>) and no GPU_ID is needed.</li><li>To extract features with CPU, use GPU_ID = -1, instead of 0 as in the example here (<a href="https://github.com/facebook/C3D/blob/master/examples/c3d_feature_extraction/c3d_sport1m_feature_extraction_frm.sh">https://github.com/facebook/C3D/blob/master/examples/c3d_feature_extraction/c3d_sport1m_feature_extraction_frm.sh</a>)</li></ul><p>●     Email me your questions? (trandu -at- fb.com) or post on github. This is more preferred because I sometime miss some emails. Github keeps tracks much better.</p></output_prefix>]]></content>
    
    
    <categories>
      
      <category>Caffe</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Caffe</tag>
      
      <tag>Action Recgnition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查看 caffemodel 内容</title>
    <link href="/2021/01/07/Caffe-20210107-%E6%9F%A5%E7%9C%8B-caffemodel-%E5%86%85%E5%AE%B9/"/>
    <url>/2021/01/07/Caffe-20210107-%E6%9F%A5%E7%9C%8B-caffemodel-%E5%86%85%E5%AE%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="使用-Python-接口，查看-caffemodel-文件内容"><a href="#使用-Python-接口，查看-caffemodel-文件内容" class="headerlink" title="使用 Python 接口，查看 .caffemodel 文件内容"></a>使用 Python 接口，查看 .caffemodel 文件内容</h2><p>训练好一个网络后，使用 Python 接口进行部署的时候，需要两个文件 <code>trian.prototxt</code> 和 <code>xx.caffemodel</code>。<br>初始化网络的时候，首先会通过 <code>train.prototxt</code> 生成网络的拓扑结构，我们可以在  <a href="https://ethereon.github.io/">netscope</a> 预览网络拓扑结构；<br>网络搭建好了之后，Caffe 会从预训练模型中载入参数，对网络参数进行初始化，这个时候有三种情况：</p><ul><li><code>prototxt</code> 中出现的层，<code>caffemodel</code> 中有对应的层：<br>这是最常见的情况，网络参数会使用 <code>caffemodel</code>  中存储的参数进行初始化。如果两者的结构参数不一样，就会抛出异常（shape mismatch）。比较典型的有：<code>conv层/pool层</code> 的 <code>kernel-size</code>，<code>channel</code> 不一样；<code>fc层</code> 的输入/输出 维度不一样。</li><li><code>prototxt</code> 中出现的层，<code>caffemodel</code> 中没有对应的层：<br>这种情况下，网络参数会使用 <code>prototxt</code> 中指定的 <code>filler</code> 方法进行初始化，等同于这一层 train from scratch；使用 ImageNet 预训练模型训练其他图片分类模型，修改最后一个 <code>fc层</code> 的名字，就是这种场景。</li><li><p><code>caffemodel</code> 中出现的层，<code>prototxt</code> 中没有对应的层：<br>这种情况下，<code>caffemodel</code> 中的参数就不会使用，在 Caffe 中会打印出 <code>Ignore layer **</code> 这样的 log 信息。</p><blockquote><p>举例来说：使用 下面的 pretrain.prototxt 训练出来 caffemodel，然后初始化 train.prototxt 的参数时：layer-A 会被 ignore；layer-B 会使用 pretrain-model 的参数；layer-C 会使用 高斯分布初始化   </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> pretrain.prototxt</span><br>layer&#123;<br>    name: layer-A   # param in this layer will be ignored when training train.prototxt<br>    ...<br>&#125;<br>layer&#123;<br>    name: layer-B   # param in this layer will be used to initalize training prototxt<br>    ...<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> train.prototxt</span><br>layer&#123;<br>    name: layer-B<br>    ...<br>&#125;<br>layer&#123;<br>    name: layer-C   # this layer does not have cordnate layer in pretrained model, will be initialize by weight-filler<br>    weiget-filler:&#123;<br>        type: guassian<br>        std: 0.1<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>下面言归正传，prototxt 是文本文件，很容易看到 待训练模型 的网络结构，那么，如何看到预训练模型的网络结构呢？直接上代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> caffe.proto.caffe_pb2 <span class="hljs-keyword">as</span> caffe_pb2<br>model_f = <span class="hljs-string">&#x27;pretrain.caffemodel&#x27;</span><br>model = caffe_pb2.NetParameter()<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(model_f, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    string = f.read()<br>    model.ParseFromString(string)  <span class="hljs-comment"># this instruction will cost much time</span><br>layers = model.layer<br><br>layers[<span class="hljs-number">0</span>]<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">name: &quot;input-data&quot;</span><br><span class="hljs-string">type: &quot;Python&quot;</span><br><span class="hljs-string">top: &quot;data&quot;</span><br><span class="hljs-string">top: &quot;im_info&quot;</span><br><span class="hljs-string">top: &quot;gt_boxes&quot;</span><br><span class="hljs-string">phase: TRAIN</span><br><span class="hljs-string">python_param &#123;</span><br><span class="hljs-string">module: &quot;roi_data_layer.layer&quot;</span><br><span class="hljs-string">layer: &quot;RoIDataLayer&quot;</span><br><span class="hljs-string">param_str: &quot;\&#x27;num_classes\&#x27;: 3&quot;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> layers:<br>    print(layer.name)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">input-data</span><br><span class="hljs-string">data_input-data_0_split</span><br><span class="hljs-string">im_info_input-data_1_split</span><br><span class="hljs-string">gt_boxes_input-data_2_split</span><br><span class="hljs-string">conv1_1</span><br><span class="hljs-string">relu1_1</span><br><span class="hljs-string">conv1_2</span><br><span class="hljs-string">relu1_2</span><br><span class="hljs-string">pool1</span><br><span class="hljs-string">conv2_1</span><br><span class="hljs-string">relu2_1</span><br><span class="hljs-string">conv2_2</span><br><span class="hljs-string">relu2_2</span><br><span class="hljs-string">pool2</span><br><span class="hljs-string">conv3_1</span><br><span class="hljs-string">relu3_1</span><br><span class="hljs-string">conv3_2</span><br><span class="hljs-string">relu3_2</span><br><span class="hljs-string">conv3_3</span><br><span class="hljs-string">relu3_3</span><br><span class="hljs-string">pool3</span><br><span class="hljs-string">conv4_1</span><br><span class="hljs-string">relu4_1</span><br><span class="hljs-string">conv4_2</span><br><span class="hljs-string">relu4_2</span><br><span class="hljs-string">conv4_3</span><br><span class="hljs-string">relu4_3</span><br><span class="hljs-string">pool4</span><br><span class="hljs-string">conv5_1</span><br><span class="hljs-string">relu5_1</span><br><span class="hljs-string">conv5_2</span><br><span class="hljs-string">relu5_2</span><br><span class="hljs-string">conv5_3</span><br><span class="hljs-string">relu5_3</span><br><span class="hljs-string">conv5_3_relu5_3_0_split</span><br><span class="hljs-string">rpn_conv/3x3</span><br><span class="hljs-string">rpn_relu/3x3</span><br><span class="hljs-string">rpn/output_rpn_relu/3x3_0_split</span><br><span class="hljs-string">rpn_cls_score</span><br><span class="hljs-string">rpn_cls_score_rpn_cls_score_0_split</span><br><span class="hljs-string">rpn_bbox_pred</span><br><span class="hljs-string">rpn_bbox_pred_rpn_bbox_pred_0_split</span><br><span class="hljs-string">rpn_cls_score_reshape</span><br><span class="hljs-string">rpn_cls_score_reshape_rpn_cls_score_reshape_0_split</span><br><span class="hljs-string">rpn-data</span><br><span class="hljs-string">rpn_loss_cls</span><br><span class="hljs-string">rpn_loss_bbox</span><br><span class="hljs-string">rpn_cls_prob</span><br><span class="hljs-string">rpn_cls_prob_reshape</span><br><span class="hljs-string">proposal</span><br><span class="hljs-string">roi-data</span><br><span class="hljs-string">roi_pool5</span><br><span class="hljs-string">fc6</span><br><span class="hljs-string">relu6</span><br><span class="hljs-string">drop6</span><br><span class="hljs-string">fc7</span><br><span class="hljs-string">relu7</span><br><span class="hljs-string">drop7</span><br><span class="hljs-string">fc7_drop7_0_split</span><br><span class="hljs-string">cls_score6</span><br><span class="hljs-string">bbox_pred6</span><br><span class="hljs-string">loss_cls</span><br><span class="hljs-string">loss_bbox</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="使用-Python-接口，修改-caffemodel-的内容"><a href="#使用-Python-接口，修改-caffemodel-的内容" class="headerlink" title="使用 Python 接口，修改 caffemodel 的内容"></a>使用 Python 接口，修改 caffemodel 的内容</h2><p>在上面的接口中，我们可以看到，读取 caffemodel 的过程实际上是 load 一串二进制方式存储的字符串，相对地，我们也可以将 字符串序列化成为二进制文件，成为一个 caffemodel，实际上，使用 Caffe 训练模型的时候，就是通过这样一个函数保存模型的，当保存路径不存在或没有权限的时候，这个接口还会抛出异常，我们调用这个函数的 Python 接口，就可以修改预训练 caffemodel 中的参数了。</p></blockquote></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>使用 Python 接口制作 lmdb</title>
    <link href="/2021/01/07/Caffe-20210107-%E4%BD%BF%E7%94%A8-Python-%E6%8E%A5%E5%8F%A3%E5%88%B6%E4%BD%9C-lmdb/"/>
    <url>/2021/01/07/Caffe-20210107-%E4%BD%BF%E7%94%A8-Python-%E6%8E%A5%E5%8F%A3%E5%88%B6%E4%BD%9C-lmdb/</url>
    
    <content type="html"><![CDATA[<h1 id="使用-lmdb-的-Python-接口制作数据集"><a href="#使用-lmdb-的-Python-接口制作数据集" class="headerlink" title="使用 lmdb 的 Python 接口制作数据集"></a>使用 lmdb 的 Python 接口制作数据集</h1><h2 id="Caffe-输入格式："><a href="#Caffe-输入格式：" class="headerlink" title="Caffe 输入格式："></a>Caffe 输入格式：</h2><p>一般来说，使用 Caffe 训练的过程，网络结构的第一层是都用来处理输入数据的，输入数据的形式大多数为图片（NLP 工作中，输入的一般是文本的特征，不过这种情况不太常见）。Caffe 的数据输入层有以下几种：<br><a id="more"></a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@p3d-container:/bigdata/dxx/pseudo-3d-residual-networks/caffe/src/caffe/layers# ll *data_layer.cpp<br>-rw-r--r-- 1 root root  4118 Mar 22 09:25 base_data_layer.cpp<br>-rw-r--r-- 1 root root  4313 Mar 22 09:25 data_layer.cpp<br>-rw-r--r-- 1 root root  4826 Mar 22 09:25 dummy_data_layer.cpp<br>-rw-r--r-- 1 root root  6136 Mar 22 09:25 hdf5_data_layer.cpp<br>-rw-r--r-- 1 root root  6946 Mar 22 09:25 image_data_layer.cpp<br>-rw-r--r-- 1 root root  4414 Mar 22 09:25 memory_data_layer.cpp<br>-rw-r--r-- 1 root root 17544 Mar 22 09:25 window_data_layer.cpp<br><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">###############################################################</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># 其中，base_data_layer.cpp 定义了数据层的基类；</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># data_layer.cpp 支持 **lmdb/leveldb** 格式的输入</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># hdf5_data_layer.cpp 支持 **hdf5** 格式的输入</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># image_data_layer.cpp 支持 **图片** 直接输入</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># 其他的三个 layer 我不太了解，感兴趣可以查阅相关资料</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">###############################################################</span></span><br></code></pre></td></tr></table></figure><p>我们平时最常见的情况就是使用 lmdb/leveldb 作为输入，Caffe 的 mnist example 就是使用的这两个格式。因为这样比直接使用 image 进行输入要更快一些（原因可能是因为 lmdb/leveldb 支持 prefetch 操作，可以节省 IO 时间），其中 lmdb 比 leveldb 更快，体积更小。<br>目前 Caffe 最常见的输入就是 lmdb 格式。那么怎么制作 lmdb 呢？ Caffe 提供了一个 convert_imageset 程序，按照指定的格式整理好 image 和 label，直接调用命令可以将图片转换为 lmdb。具体操作参考官网，这里不详细介绍。</p><h2 id="lmdb-的-Python-接口"><a href="#lmdb-的-Python-接口" class="headerlink" title="lmdb 的 Python 接口"></a>lmdb 的 Python 接口</h2><p>简单的情况下，我们可以直接利用官方提供的工具，将图片数据转换为 lmdb，然后利用 data_layer 的 transform_param 对数据进行简单的预处理或者数据增强操作。但是某些特殊情况下，transform_param 可能无法实现我们想要的预处理。这种情况下，我们可以修改 data_layer 的源码，实现想要的功能，当然这样就比较麻烦了，更为简单的方法就是，我们可以在制作 lmdb 之前先实现想要的预处理操作，然后将图片制作成 lmdb，这就是下面要介绍的内容。同理，如果想用这种方法实现比较复杂的数据增强功能也是可以的，不过，这种情况下，lmdb 占用的空间会相应的增加。<br>我这次的任务是让 Caffe 实现视频输入功能，因此需要将 16 个连续视频帧 stack 起来，制作成一个 clip，后续的网络会对 clip 进行卷积操作。<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> lmdb<br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> cv<span class="hljs-number">2</span><br><span class="hljs-attribute">import</span> caffe<br><span class="hljs-attribute">from</span> caffe.proto import caffe_pb<span class="hljs-number">2</span><br><br><span class="hljs-comment">#basic setting</span><br><span class="hljs-comment"># 这个设置用来存放lmdb数据的目录</span><br><span class="hljs-attribute">lmdb_file</span> = &#x27;lmdb_data&#x27;<br><span class="hljs-attribute">batch_size</span> = <span class="hljs-number">256</span><br><br><span class="hljs-comment"># create the lmdb file</span><br><span class="hljs-comment"># map_size指的是数据库的最大容量，根据需求设置</span><br><span class="hljs-attribute">lmdb_env</span> = lmdb.open(lmdb_file, map_size=int(<span class="hljs-number">1</span>e<span class="hljs-number">12</span>))<br><span class="hljs-attribute">lmdb_txn</span> = lmdb_env.begin(write=True)<br><span class="hljs-comment"># 因为caffe中经常采用datum这种数据结构存储数据</span><br><span class="hljs-attribute">datum</span> = caffe_pb<span class="hljs-number">2</span>.Datum()<br><br><span class="hljs-attribute">item_id</span> = -<span class="hljs-number">1</span><br><span class="hljs-attribute">for</span> x in range(<span class="hljs-number">1000</span>):<br>    <span class="hljs-attribute">item_id</span> += <span class="hljs-number">1</span><br><br>    <span class="hljs-comment">#prepare the data and label</span><br>    <br>    <span class="hljs-comment">#data = np.ones((3,64,64), np.uint8) * (item_id%128 + 64) #CxHxW array, uint8 or float</span><br>    <span class="hljs-comment"># pic_path设置成图像目录, 0表示读入灰度图</span><br>    <span class="hljs-attribute">data</span> = cv<span class="hljs-number">2</span>.imread(pic_path, <span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># label 设置图像的label就行</span><br>    <span class="hljs-attribute">label</span> = item_id%<span class="hljs-number">128</span> + <span class="hljs-number">64</span><br><br>    <span class="hljs-comment"># save in datum</span><br>    <span class="hljs-attribute">datum</span> = caffe.io.array_to_datum(data, label)<br>    <span class="hljs-attribute">keystr</span> = &#x27;&#123;:<span class="hljs-number">0</span>&gt;<span class="hljs-number">8</span>d&#125;&#x27;.format(item_id)<br>    <span class="hljs-attribute">lmdb_txn</span>.put( keystr, datum.SerializeToString() )<br><br>    <span class="hljs-comment"># write batch</span><br>    <span class="hljs-attribute">if</span>(item_id + <span class="hljs-number">1</span>) % batch_size == <span class="hljs-number">0</span>:<br>        <span class="hljs-attribute">lmdb_txn</span>.commit()<br>        <span class="hljs-attribute">lmdb_txn</span> = lmdb_env.begin(write=True)<br>        <span class="hljs-attribute">print</span> (item_id + <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># write last batch</span><br><span class="hljs-attribute">if</span> (item_id+<span class="hljs-number">1</span>) % batch_size != <span class="hljs-number">0</span>:<br>    <span class="hljs-attribute">lmdb_txn</span>.commit()<br>    <span class="hljs-attribute">print</span> &#x27;last batch&#x27;<br>    <span class="hljs-attribute">print</span> (item_id + <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Caffe</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Caffe</tag>
      
      <tag>LMDB</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Py-faster-rcnn</title>
    <link href="/2021/01/07/Caffe-20210107-Py-faster-rcnn/"/>
    <url>/2021/01/07/Caffe-20210107-Py-faster-rcnn/</url>
    
    <content type="html"><![CDATA[<h1 id="使用-faster-rcnn-检测游戏中的血条"><a href="#使用-faster-rcnn-检测游戏中的血条" class="headerlink" title="使用 faster-rcnn 检测游戏中的血条"></a>使用 faster-rcnn 检测游戏中的血条</h1><p><a href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a><br>这一周使用 rbg 大神的 py-faster-rcnn 训练王者荣耀视频中的人物血条，因为之前已经有不少经验，以为可以直接了当得到结果，但是事实却并不那么如意，现在将实验过程简单总结一下。</p><a id="more"></a><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>实验环境：<br>实验的环境配置是 <code>Python-2.7, OpenCV-2.4.9, Caffe-1.0, cudnn-v6</code></li><li>安装依赖项：<br>faster-rcnn 使用的是 Caffe 框架，首先需要安装 Caffe 的依赖，另外还需要一些 Python 环境，通过 pip 即可安装。<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> cpython easydict<br></code></pre></td></tr></table></figure></li></ul><h2 id="编译-faster-rcnn"><a href="#编译-faster-rcnn" class="headerlink" title="编译 faster-rcnn"></a>编译 faster-rcnn</h2><ul><li>从 github pull 代码  <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">git clone --recursive https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/rbgirshick/</span>py-faster-rcnn.git<br></code></pre></td></tr></table></figure></li><li>更新 Caffe<br><a href="http://blog.csdn.net/rzjmpb/article/details/52373012">http://blog.csdn.net/rzjmpb/article/details/52373012</a><br>github 中的代码使用的 Caffe 版本很古老，因此不支持 cudnnv5，为了使用 cudnn，可以升级 Caffe 或者安装低版本的 cudnn，这里选择升级 Caffe.<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd py-faster-rcnn<span class="hljs-regexp">/caffe-fast-rcnn/</span><br>git remote add caffe https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/BVLC/</span>caffe.git <br><span class="hljs-comment"># git config --list </span><br>git fetch caffe<br>git merge -X theirs caffe/master<br></code></pre></td></tr></table></figure><strong><em>merge 之后需要注释掉 python_layer.hpp 中的一行代码</em></strong><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> vi caffe-fast-rcnn/include/caffe/layers/python_layer.hpp +29</span><br>// self_.attr(&quot;phase&quot;) = static_cast&lt;int&gt;(this-&gt;phase_);              // &lt;================<br></code></pre></td></tr></table></figure></li></ul><ul><li><p>配置 Makefile.config<br>注意打开 <code>WITH_PYTHON_LAYER=1</code></p></li><li><p>编译Caffe 和 faster-rcnn</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">cd</span> <span class="hljs-keyword">py</span>-faster-rcnn/caffe-fater-rcnn/<br><span class="hljs-keyword">make</span> -<span class="hljs-keyword">j</span><br><span class="hljs-keyword">make</span> pycaffe<br><span class="hljs-keyword">cd</span> <span class="hljs-keyword">py</span>-faster-rcnn/lib/<br><span class="hljs-keyword">make</span><br></code></pre></td></tr></table></figure><blockquote><p>Caffe 默认使用的是 Python2, OpenCV2; faster-rcnn 使用系统环境变量中的 Python 解释器。一定要注意两者保持一致，特别是由于不可抗原因必须使用 Python3 的场景，确保 Caffe 和 faster-rcnn 的编译环境保持一致（<em>faster-rcnn 中有大量的 Python2 接口不被 Python3兼容，使用 Python3 需要做很多修改，或者在 Github 上寻找其他 fork 分支</em>）</p></blockquote></li></ul><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>一般按照 pascal_voc 的格式准备数据，需要准备三个文件夹  </p><ul><li><code>my_data/VOC2007/Annotations/</code><br><code>Annotation</code> 文件夹下面是 XML 文件，具体格式如下：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs HTML"><span class="hljs-tag">&lt;<span class="hljs-name">annotation</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">folder</span>&gt;</span>VOC2007<span class="hljs-tag">&lt;/<span class="hljs-name">folder</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">filename</span>&gt;</span>img.jpg<span class="hljs-tag">&lt;/<span class="hljs-name">filename</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">size</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">width</span>&gt;</span>1280<span class="hljs-tag">&lt;/<span class="hljs-name">width</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">height</span>&gt;</span>720<span class="hljs-tag">&lt;/<span class="hljs-name">height</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">depth</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">depth</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">size</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">object</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">bndbox</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">xmin</span>&gt;</span>548<span class="hljs-tag">&lt;/<span class="hljs-name">xmin</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">xmax</span>&gt;</span>711<span class="hljs-tag">&lt;/<span class="hljs-name">xmax</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">ymin</span>&gt;</span>255<span class="hljs-tag">&lt;/<span class="hljs-name">ymin</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">ymax</span>&gt;</span>287<span class="hljs-tag">&lt;/<span class="hljs-name">ymax</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">bndbox</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>health_blue<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">truncated</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">truncated</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">difficult</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">difficult</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">object</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">object</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">bndbox</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">xmin</span>&gt;</span>675<span class="hljs-tag">&lt;/<span class="hljs-name">xmin</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">xmax</span>&gt;</span>842<span class="hljs-tag">&lt;/<span class="hljs-name">xmax</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">ymin</span>&gt;</span>373<span class="hljs-tag">&lt;/<span class="hljs-name">ymin</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">ymax</span>&gt;</span>407<span class="hljs-tag">&lt;/<span class="hljs-name">ymax</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">bndbox</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>health_red<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">truncated</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">truncated</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">difficult</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">difficult</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">object</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">segmented</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">segmented</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">annotation</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><code>my_data/VOC2007/ImageSets/</code><br><code>ImageSets/Main</code> 文件夹下面是 TXT 文件，总共有 4 个，分别是 <code>trainval.txt</code>, <code>train.txt</code>, <code>val.txt</code>, <code>test.txt</code>， 文件中每一行代表一张图片；  </li><li><code>my_data/VOC2007/JPEGImages/</code><br><code>JPEGImages</code> 文件夹下面保存的是图片。  </li></ul><p><strong>这里有几个小细节：<br>（1）图片的文件名后缀是 .jpg；<br>（2）ImageSets 文件夹下面的 txt 是没有后缀的；<br>（3）xml 中矩形框字段的 tag 是 bndbox，不是常用的 bbox；<br>（4）cls 的名字最好全部使用小写英文字母和下划线，因为 faster-rcnn 会将字符全部转换为小写；<br>（5）注意检查 xml 中是否每一张图片都含有 object 字段，没有 object 字段的 xml 进入网络训练不会报错，但是对训练结果会有影响。</strong>  </p><ul><li>制作完数据集后，在 <code>data/</code> 目录下，建立软连接，指向 <code>my_data/</code> 文件夹<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">cd</span> <span class="hljs-keyword">py</span>-faster-rcnn/data/<br><span class="hljs-keyword">ln</span> -s my_data/ VOCdevkit2007<br></code></pre></td></tr></table></figure><h2 id="更改网络设置"><a href="#更改网络设置" class="headerlink" title="更改网络设置"></a>更改网络设置</h2>faster-rcnn 的网络结构由两部分组成，一部分在 <code>models/</code> 目录下，这里的主要包含 prototxt， 用来初始化 Solver 和 Network；另一部分在 <code>lib/</code> 目录下，这里包含 rpn, roi_layer, nms 等模块，以及用来 读取和解析 xml 文件的代码。  <ul><li><code>models/</code><br>py-faster-rcnn 提供了两种训练方法，一种是 alt-opt 训练，一种是 end2end 训练，关于两种训练的差别可以参考 Github 上的 readme，另外我们还可以选择使用三种不同的 Backbone Network： {VGG16, VGG-M, ZF}，这里我们选择性能最好的 VGG16，训练方式为 end2end 训练。对应目录为 <code>models/pascal_voc/VGG16/faster_rcnn_end2end/</code><br>— 修改 train.prototxt<figure class="highlight puppet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs puppet">$ vi models/pascal_voc/VGG16/faster_rcnn_end2end/train.prototxt<br><span class="hljs-comment"># input_data 层（第 11 行）， 修改 num_classes 为 cls+1</span><br><span class="hljs-keyword">layer</span> &#123;<br>    <span class="hljs-literal">name</span>: <span class="hljs-string">&#x27;input-data&#x27;</span><br>    <span class="hljs-built_in">type</span>: <span class="hljs-string">&#x27;Python&#x27;</span><br>    top: <span class="hljs-string">&#x27;data&#x27;</span><br>    top: <span class="hljs-string">&#x27;im_info&#x27;</span><br>    top: <span class="hljs-string">&#x27;gt_boxes&#x27;</span><br>    python_param &#123;<br>        module: <span class="hljs-string">&#x27;roi_data_layer.layer&#x27;</span><br>        layer: <span class="hljs-string">&#x27;RoIDataLayer&#x27;</span><br>        <span class="hljs-comment"># param_str: &quot;&#x27;num_classes&#x27;: 21&quot;</span><br>        param_str: <span class="hljs-string">&quot;&#x27;num_classes&#x27;: 3&quot;</span>              <span class="hljs-comment"># &lt;================</span><br>    &#125;<br>&#125;<br><span class="hljs-comment"># roi_data 层（第 530 行）， 修改 num_classes 为 cls+1</span><br><span class="hljs-keyword">layer</span> &#123;<br>    <span class="hljs-literal">name</span>: <span class="hljs-string">&#x27;roi-data&#x27;</span><br>    <span class="hljs-built_in">type</span>: <span class="hljs-string">&#x27;Python&#x27;</span><br>    bottom: <span class="hljs-string">&#x27;rpn_rois&#x27;</span><br>    bottom: <span class="hljs-string">&#x27;gt_boxes&#x27;</span><br>    top: <span class="hljs-string">&#x27;rois&#x27;</span><br>    top: <span class="hljs-string">&#x27;labels&#x27;</span><br>    top: <span class="hljs-string">&#x27;bbox_targets&#x27;</span><br>    top: <span class="hljs-string">&#x27;bbox_inside_weights&#x27;</span><br>    top: <span class="hljs-string">&#x27;bbox_outside_weights&#x27;</span><br>    python_param &#123;<br>        module: <span class="hljs-string">&#x27;rpn.proposal_target_layer&#x27;</span><br>        layer: <span class="hljs-string">&#x27;ProposalTargetLayer&#x27;</span><br>        <span class="hljs-comment"># param_str: &quot;&#x27;num_classes&#x27;: 21&quot;</span><br>        param_str: <span class="hljs-string">&quot;&#x27;num_classes&#x27;: 3&quot;</span>              <span class="hljs-comment"># &lt;================</span><br>    &#125;<br>&#125;<br><span class="hljs-comment"># cls_score 层（第 620 行）， 修改 num_output 为 cls+1</span><br><span class="hljs-keyword">layer</span> &#123;<br>    <span class="hljs-literal">name</span>: <span class="hljs-string">&quot;cls_score&quot;</span><br>    <span class="hljs-built_in">type</span>: <span class="hljs-string">&quot;InnerProduct&quot;</span><br>    bottom: <span class="hljs-string">&quot;fc7&quot;</span><br>    top: <span class="hljs-string">&quot;cls_score&quot;</span><br>    param &#123;<br>        lr_mult: <span class="hljs-number">1</span><br>    &#125;<br>    <span class="hljs-keyword">param</span> &#123;<br>        lr_mult: <span class="hljs-number">2</span><br>    &#125;<br>    <span class="hljs-keyword">inner_product_param</span> &#123;<br>        <span class="hljs-comment"># num_output: 21</span><br>        num_output: <span class="hljs-number">3</span>              <span class="hljs-comment"># &lt;================</span><br>        weight_filler &#123;<br>            <span class="hljs-built_in">type</span>: <span class="hljs-string">&quot;gaussian&quot;</span><br>            std: <span class="hljs-number">0</span>.<span class="hljs-number">01</span><br>        &#125;<br>        <span class="hljs-keyword">bias_filler</span> &#123;<br>            <span class="hljs-built_in">type</span>: <span class="hljs-string">&quot;constant&quot;</span><br>            value: <span class="hljs-number">0</span><br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-comment"># box_pred 层（第 643 行）， 修改 num_output 为 4*(cls+1)</span><br><span class="hljs-keyword">layer</span> &#123;<br>    <span class="hljs-literal">name</span>: <span class="hljs-string">&quot;bbox_pred&quot;</span><br>    <span class="hljs-built_in">type</span>: <span class="hljs-string">&quot;InnerProduct&quot;</span><br>    bottom: <span class="hljs-string">&quot;fc7&quot;</span><br>    top: <span class="hljs-string">&quot;bbox_pred&quot;</span><br>    param &#123;<br>        lr_mult: <span class="hljs-number">1</span><br>    &#125;<br>    <span class="hljs-keyword">param</span> &#123;<br>        lr_mult: <span class="hljs-number">2</span><br>    &#125;<br>    <span class="hljs-keyword">inner_product_param</span> &#123;<br>        <span class="hljs-comment"># num_output: 84</span><br>        num_output: <span class="hljs-number">12</span>              <span class="hljs-comment"># &lt;================</span><br>        weight_filler &#123;<br>            <span class="hljs-built_in">type</span>: <span class="hljs-string">&quot;gaussian&quot;</span><br>            std: <span class="hljs-number">0</span>.<span class="hljs-number">001</span><br>        &#125;<br>        <span class="hljs-keyword">bias_filler</span> &#123;<br>            <span class="hljs-built_in">type</span>: <span class="hljs-string">&quot;constant&quot;</span><br>            value: <span class="hljs-number">0</span><br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>— 修改 test.prototxt<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs dts">$ vi models/pascal_voc/VGG16/faster_rcnn_end2end/test.prototxt<br><span class="hljs-meta"># cls_score 层（第 567 行）， 修改 num_output 为 cls+1</span><br><span class="hljs-class">layer </span>&#123;<br><span class="hljs-symbol">    name:</span> <span class="hljs-string">&quot;cls_score&quot;</span><br><span class="hljs-symbol">    type:</span> <span class="hljs-string">&quot;InnerProduct&quot;</span><br><span class="hljs-symbol">    bottom:</span> <span class="hljs-string">&quot;fc7&quot;</span><br><span class="hljs-symbol">    top:</span> <span class="hljs-string">&quot;cls_score&quot;</span><br>    <span class="hljs-class">param </span>&#123;<br><span class="hljs-symbol">        lr_mult:</span> <span class="hljs-number">1</span><br>    &#125;<br>    <span class="hljs-class">param </span>&#123;<br><span class="hljs-symbol">        lr_mult:</span> <span class="hljs-number">2</span><br>    &#125;<br>    <span class="hljs-class">inner_product_param </span>&#123;<br>        <span class="hljs-meta"># num_output: 21</span><br><span class="hljs-symbol">        num_output:</span> <span class="hljs-number">3</span>              <span class="hljs-meta"># &lt;================</span><br>        <span class="hljs-class">weight_filler </span>&#123;<br><span class="hljs-symbol">            type:</span> <span class="hljs-string">&quot;gaussian&quot;</span><br><span class="hljs-symbol">            std:</span> <span class="hljs-number">0.01</span><br>        &#125;<br>        <span class="hljs-class">bias_filler </span>&#123;<br><span class="hljs-symbol">            type:</span> <span class="hljs-string">&quot;constant&quot;</span><br><span class="hljs-symbol">            value:</span> <span class="hljs-number">0</span><br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-meta"># box_pred 层（第 592 行）， 修改 num_output 为 4*(cls+1)</span><br><span class="hljs-class">layer </span>&#123;<br><span class="hljs-symbol">    name:</span> <span class="hljs-string">&quot;bbox_pred&quot;</span><br><span class="hljs-symbol">    type:</span> <span class="hljs-string">&quot;InnerProduct&quot;</span><br><span class="hljs-symbol">    bottom:</span> <span class="hljs-string">&quot;fc7&quot;</span><br><span class="hljs-symbol">    top:</span> <span class="hljs-string">&quot;bbox_pred&quot;</span><br>    <span class="hljs-class">param </span>&#123;<br><span class="hljs-symbol">        lr_mult:</span> <span class="hljs-number">1</span><br>    &#125;<br>    <span class="hljs-class">param </span>&#123;<br><span class="hljs-symbol">        lr_mult:</span> <span class="hljs-number">2</span><br>    &#125;<br>    <span class="hljs-class">inner_product_param </span>&#123;<br>        <span class="hljs-meta"># num_output: 84</span><br><span class="hljs-symbol">        num_output:</span> <span class="hljs-number">12</span>              <span class="hljs-meta"># &lt;================</span><br>        <span class="hljs-class">weight_filler </span>&#123;<br><span class="hljs-symbol">            type:</span> <span class="hljs-string">&quot;gaussian&quot;</span><br><span class="hljs-symbol">            std:</span> <span class="hljs-number">0.001</span><br>        &#125;<br>        <span class="hljs-class">bias_filler </span>&#123;<br><span class="hljs-symbol">            type:</span> <span class="hljs-string">&quot;constant&quot;</span><br><span class="hljs-symbol">            value:</span> <span class="hljs-number">0</span><br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>— 修改 solver.prototxt<br>修改 solver 中的参数，使之适合训练自己的数据；<br>faster-rcnn 的另外大多数网络参数在 <code>lib/fast-rcnn/config.py</code> 中，附有详细的注释，可以参考注释进行修改。</li></ul></li></ul><ul><li><code>lib/</code><br>  — 修改 pascal_voc.py<br>  由于自己训练数据的类别数与 pascal_voc 不同，因此需要稍微修改一下  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs Python">vi lib/datasets/pascal_voc.py<br><span class="hljs-comment"># 修改 self._classes 为自己的类别：</span><br><span class="hljs-comment"># self._classes = (&#x27;__background__&#x27;, # always index 0</span><br><span class="hljs-comment">#             &#x27;aeroplane&#x27;, &#x27;bicycle&#x27;, &#x27;bird&#x27;, &#x27;boat&#x27;,</span><br><span class="hljs-comment">#             &#x27;bottle&#x27;, &#x27;bus&#x27;, &#x27;car&#x27;, &#x27;cat&#x27;, &#x27;chair&#x27;,</span><br><span class="hljs-comment">#             &#x27;cow&#x27;, &#x27;diningtable&#x27;, &#x27;dog&#x27;, &#x27;horse&#x27;,</span><br><span class="hljs-comment">#             &#x27;motorbike&#x27;, &#x27;person&#x27;, &#x27;pottedplant&#x27;,</span><br><span class="hljs-comment">#             &#x27;sheep&#x27;, &#x27;sofa&#x27;, &#x27;train&#x27;, &#x27;tvmonitor&#x27;)</span><br>self._classes = (<span class="hljs-string">&#x27;__background__&#x27;</span>, <span class="hljs-comment"># always index 0</span><br>             <span class="hljs-string">&#x27;health_blue&#x27;</span>, <span class="hljs-string">&#x27;health_red&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h2 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h2><ol><li><p>建立测试结果的文件夹：  </p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p data/VOCdevkit2007/results/VOC2007/Main/<br></code></pre></td></tr></table></figure><p> 训练完成后，会进行一次测试，如果不手动新建文件夹，测试的时候会报错说找不到这个目录。  </p></li><li><p>直接使用 faster-rcnn 的 脚本进行训练：</p> <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">.<span class="hljs-regexp">/experiments/</span>scripts/faster_rcnn_end2end.sh <span class="hljs-number">0</span> VGG16 pascal_voc<br></code></pre></td></tr></table></figure><p> 最大迭代次数在 <code>./experiments/scripts/faster_rcnn_end2end.sh</code> 中进行设置；<br> 训练的 log 存储在 <code>experiments/logs/</code> 中， 也可以自己重定向出来。</p></li></ol><h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><ol><li><p>faster-rcnn 提供的训练脚本，在使用训练集验证集训练完成后，会在测试集上进行测试，输出模型的 AP 和 AR；  </p></li><li><p>如果想使用单张图片进行测试，并且看到可视化效果，可以参考 <code>tools/demo.py</code>， <code>demo.py</code> 使用 matplotlib 进行可视化，对脚本稍加修改，可以将测试结果保存成图片存储下来。  </p></li><li><p>修改脚本代码：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><br><span class="hljs-comment"># --------------------------------------------------------</span><br><span class="hljs-comment"># Faster R-CNN</span><br><span class="hljs-comment"># Copyright (c) 2015 Microsoft</span><br><span class="hljs-comment"># Licensed under The MIT License [see LICENSE for details]</span><br><span class="hljs-comment"># Written by Ross Girshick</span><br><span class="hljs-comment"># --------------------------------------------------------</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Demo script showing detections in sample images.</span><br><span class="hljs-string">See README.md for installation instructions before running.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> _init_paths<br><span class="hljs-keyword">from</span> fast_rcnn.config <span class="hljs-keyword">import</span> cfg<br><span class="hljs-keyword">from</span> fast_rcnn.test <span class="hljs-keyword">import</span> im_detect<br><span class="hljs-keyword">from</span> fast_rcnn.nms_wrapper <span class="hljs-keyword">import</span> nms<br><span class="hljs-keyword">from</span> utils.timer <span class="hljs-keyword">import</span> Timer<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> scipy.io <span class="hljs-keyword">as</span> sio<br><span class="hljs-keyword">import</span> caffe, os, sys, cv2<br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">import</span> glob<br><br>CLASSES=(<span class="hljs-string">&#x27;__background__&#x27;</span>,<span class="hljs-string">&#x27;health_blue&#x27;</span>,<span class="hljs-string">&#x27;health_red&#x27;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vis_detections</span>(<span class="hljs-params">im, class_name, dets, thresh=<span class="hljs-number">0.5</span></span>):</span><br>    inds = np.where(dets[:, -<span class="hljs-number">1</span>] &gt;= thresh)[<span class="hljs-number">0</span>]<br>    boxes, scores = ([], [])<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> inds:<br>        bbox = dets[i, :<span class="hljs-number">4</span>]<br>        score = dets[i, -<span class="hljs-number">1</span>]<br>        boxes.append( dets[i, :<span class="hljs-number">4</span>] )<br>        scores.append(score)<br>        print(bbox, score)<br>    <span class="hljs-keyword">return</span> boxes, scores<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span>(<span class="hljs-params">net, im, filename, out_dir</span>):</span><br>    timer = Timer()<br>    timer.tic()<br>    scores, boxes = im_detect(net, im)<br>    timer.toc()<br>    print(<span class="hljs-string">&#x27;-----------------------------------------------------&#x27;</span>)<br>    <span class="hljs-built_in">print</span> ((<span class="hljs-string">&#x27;Detection &#123;&#125; took &#123;:.3f&#125;s for &#123;:d&#125; object proposals&#x27;</span>).<span class="hljs-built_in">format</span>(filename, timer.total_time, boxes.shape[<span class="hljs-number">0</span>]))<br><br>    CONF_THRESH = <span class="hljs-number">0.8</span><br>    NMS_THRESH = <span class="hljs-number">0.1</span><br>    <span class="hljs-keyword">for</span> cls_ind, cls <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(CLASSES[<span class="hljs-number">1</span>:]):<br>        cls_ind += <span class="hljs-number">1</span> <span class="hljs-comment"># because we skipped background</span><br>        cls_boxes = boxes[:, <span class="hljs-number">4</span>*cls_ind:<span class="hljs-number">4</span>*(cls_ind + <span class="hljs-number">1</span>)]<br>        cls_scores = scores[:, cls_ind]<br>        dets = np.hstack((cls_boxes,<br>                        cls_scores[:, np.newaxis])).astype(np.float32)<br>        keep = nms(dets, NMS_THRESH)<br>        dets = dets[keep, :]<br><br>        _boxes, _scores = vis_detections(im, cls, dets, thresh=CONF_THRESH)<br>        <span class="hljs-keyword">for</span> box, score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>( _boxes, _scores):<br>            display_str = cls + <span class="hljs-string">&#x27; &#x27;</span> + <span class="hljs-built_in">str</span>(score)<br>            cv2.rectangle(im, <span class="hljs-built_in">tuple</span>(box[:<span class="hljs-number">2</span>]), <span class="hljs-built_in">tuple</span>(box[<span class="hljs-number">2</span>:]), (<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>), <span class="hljs-number">2</span>)<br>            cv2.putText(im, display_str, <span class="hljs-built_in">tuple</span>(box[:<span class="hljs-number">2</span>]), cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.7</span>, (<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>), <span class="hljs-number">2</span>)<br><br>    cv2.imwrite( os.path.join( out_dir, filename), im)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_args</span>():</span><br>    <span class="hljs-string">&quot;&quot;&quot;Parse input arguments.&quot;&quot;&quot;</span><br>    parser = argparse.ArgumentParser(description=<span class="hljs-string">&#x27;Faster R-CNN demo&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;model_name&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;model name&#x27;</span>,<br>                        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--gpu&#x27;</span>, dest=<span class="hljs-string">&#x27;gpu_id&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;GPU device id to use [0]&#x27;</span>,<br>                        default=<span class="hljs-number">1</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--cpu&#x27;</span>, dest=<span class="hljs-string">&#x27;cpu_mode&#x27;</span>,<br>                        <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Use CPU mode (overrides --gpu)&#x27;</span>,<br>                        action=<span class="hljs-string">&#x27;store_true&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--data_dir&#x27;</span>, dest=<span class="hljs-string">&#x27;data_dir&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;data path&#x27;</span>,default=<span class="hljs-string">&#x27;/bigdata/dxx/py-faster-rcnn/demo/&#x27;</span>,<br>                        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--video_name&#x27;</span>, dest=<span class="hljs-string">&#x27;video_name&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;video name&#x27;</span>,default=<span class="hljs-string">&#x27;test.mp4&#x27;</span>,<br>                        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--image_name&#x27;</span>, dest=<span class="hljs-string">&#x27;image_name&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;image name&#x27;</span>,default=<span class="hljs-string">&#x27;test.mp4&#x27;</span>,<br>                        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--model_dir&#x27;</span>, dest=<span class="hljs-string">&#x27;model_dir&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;model path&#x27;</span>,default=<span class="hljs-string">&#x27;/bigdata/dxx/py-faster-rcnn/models/&#x27;</span>,<br>                        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    args = parser.parse_args()<br>    <span class="hljs-keyword">return</span> args<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    cfg.TEST.HAS_RPN = <span class="hljs-literal">True</span>  <span class="hljs-comment"># Use RPN for proposals</span><br>    args = parse_args()<br><br>    out_dir = os.path.join(args.data_dir, args.model_name)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(out_dir):<br>        os.mkdir(out_dir)<br>        os.system(<span class="hljs-string">&#x27;&#123;&#125; &#123;&#125; &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">&#x27;chmod&#x27;</span>, <span class="hljs-string">&#x27;777&#x27;</span>, out_dir))<br>    print(<span class="hljs-string">&#x27;output images to &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(out_dir))<br>    prototxt = <span class="hljs-string">&#x27;models/pascal_voc/VGG16/faster_rcnn_end2end/test.prototxt&#x27;</span><br>    caffemodel = os.path.join(args.model_dir, args.model_name, <span class="hljs-string">&#x27;vgg16_faster_rcnn_final.caffemodel&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> args.cpu_mode:<br>        caffe.set_mode_cpu()<br>    <span class="hljs-keyword">else</span>:<br>        caffe.set_mode_gpu()<br>        caffe.set_device(args.gpu_id)<br>        cfg.GPU_ID = args.gpu_id<br><br>    net = caffe.Net(prototxt, caffemodel, caffe.TEST)<br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;\n\nLoaded network &#123;:s&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(caffemodel))<br><br>    image_f = os.path.join(args.data_dir, <span class="hljs-string">&#x27;image&#x27;</span>, args.image_name, <span class="hljs-string">&#x27;*.jpg&#x27;</span>)<br>    <span class="hljs-keyword">for</span> im_f <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(glob.glob(image_f)):<br>        im = cv2.imread(im_f)<br>        demo(net, im, os.path.basename(im_f), out_dir)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    video_f = os.path.join(args.data_dir, &#x27;video&#x27;, args.video_name)</span><br><span class="hljs-string">    video_cap = cv2.VideoCapture(video_f)</span><br><span class="hljs-string">    frame_cnt = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))</span><br><span class="hljs-string">    error_frame = max(20, frame_cnt / 50)</span><br><span class="hljs-string">    for i, frame_index in enumerate(range(0, frame_cnt, 10)):</span><br><span class="hljs-string">        video_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)</span><br><span class="hljs-string">        status, frame = video_cap.read()</span><br><span class="hljs-string">        if not status:</span><br><span class="hljs-string">                error_frame -= 1</span><br><span class="hljs-string">                if error_frame &gt; 0:</span><br><span class="hljs-string">                    continue</span><br><span class="hljs-string">                video_cap.release()</span><br><span class="hljs-string">                raise Exception(&#x27;Bad video file&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        demo(net, frame, frame_index+&#x27;.jpg&#x27;, out_dir)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure></li><li>测试图片的路径格式如下：   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">+ /bigdata/dxx/py-faster-rcnn/<br>    + models/<br>        + &lt;model_name&gt;/<br>            - vgg16_faster_rcnn_final.caffemodel<br>    + demo/<br>        + video/<br>            - test.mp4<br>        + image/<br>            + test.mp4/<br>                - 001.jpg<br>                - 002.jpg<br>        + &lt;model_name&gt;/  # detection results<br>            - 001.jpg<br>            - 002.jpg<br></code></pre></td></tr></table></figure></li><li><p><code>python tools/demo.py &lt;model_name&gt;</code></p></li><li><p>这次使用了迭代 5000 次的模型进行测试，在两个视频上进行了简单的测试，在测试的 100 张图片上，完全没有出现 漏检或错检，准确率达到 1.0</p></li></ol><h2 id="错误列表"><a href="#错误列表" class="headerlink" title="错误列表"></a>错误列表</h2><ul><li><p>注意事项：<br><strong><em>1. 编译过程中，每次更新环境，最好重新编译 Caffe、pycaffe、faster-rcnn/lib</em></strong><br><strong><em>2. 运行过程中，每次重新开始训练/测试，最好删除掉 <code>data/cache/</code> 和 <code>data/VOCdevkit2007/annotationcache/</code></em></strong></p></li><li><p><strong><code>AttributeError: can&#39;t set attribute</code></strong><br>  这是由于 git merge 之后没有修改 <code>caffe-fast-rcnn/include/caffe/layers/python_layer.hpp</code> 导致的。</p></li><li><p><strong><code>AttributeError: &#39;module&#39; object has no attribute &#39;text_format&#39;</code></strong><br>  这个和 protobuf 的版本有关</p>  <figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gradle">$ vi lib<span class="hljs-regexp">/fast-rcnn/</span>train.py<br>+ <span class="hljs-keyword">import</span> google.protobuf.text_format<br></code></pre></td></tr></table></figure></li><li><p><strong><code>TypeError: slice indices must be integers or None or have an __index__ method</code></strong>  </p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># $ vi lib/rpn/proposal_target_layer.py</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_bbox_regression_labels</span>(<span class="hljs-params">bbox_target_data, num_classes</span>):</span><br>    clss = bbox_target_data[:, <span class="hljs-number">0</span>]<br>    bbox_targets = np.zeros((clss.size, <span class="hljs-number">4</span> * num_classes), dtype=np.float32)<br>    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)<br>    inds = np.where(clss &gt; <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> ind <span class="hljs-keyword">in</span> inds:<br>        ind = <span class="hljs-built_in">int</span>(ind)                  &lt;=================              <br>        cls = clss[ind]<br>        start = <span class="hljs-built_in">int</span>(<span class="hljs-number">4</span> * cls)            &lt;=================<br>        end = <span class="hljs-built_in">int</span>(start + <span class="hljs-number">4</span>)            &lt;=================<br>        bbox_targets[ind, start:end] = bbox_target_data[ind, <span class="hljs-number">1</span>:]<br>        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS<br>    <span class="hljs-keyword">return</span> bbox_targets, bbox_inside_weights<br><br></code></pre></td></tr></table></figure></li><li><p><strong><code>TypeError: &#39;numpy.float64&#39; object cannot be interpreted as an index</code></strong><br>  <a href="https://github.com/rbgirshick/py-faster-rcnn/issues/481">https://github.com/rbgirshick/py-faster-rcnn/issues/481</a><br>  numpy1.12 版本以后，不再支持 1.0，2.0 这样的浮点数作为索引，因此需要安装低版本的numpy  </p>  <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install -U numpy==<span class="hljs-number">1</span>.<span class="hljs-number">11</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>  另外还可以找到所有出问题的地方，使用 astype() 函数进行类型转换  </p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python">$ vi lib/roi_data_layer/minibatch.py +<span class="hljs-number">26</span><br>fg_rois_per_image = np.<span class="hljs-built_in">round</span>(cfg.TRAIN.FG_FRACTION *rois_per_image).astype(np.<span class="hljs-built_in">int</span>)<br><br>$ vi lib/datasets/ds_utils.py +<span class="hljs-number">12</span><br>hashes = np.<span class="hljs-built_in">round</span>(boxes * scale).dot(v).astype(np.<span class="hljs-built_in">int</span>)<br><br>$ vi lib/fast_rcnn/test.py line +<span class="hljs-number">129</span><br>hashes = np.<span class="hljs-built_in">round</span>(blobs[<span class="hljs-string">&#x27;rois&#x27;</span>] * cfg.DEDUP_BOXES).dot(v).astype(np.<span class="hljs-built_in">int</span>)<br><br>$ vi lib/rpn/proposal_target_layer.py +<span class="hljs-number">60</span><br>fg_rois_per_image = np.<span class="hljs-built_in">round</span>(cfg.TRAIN.FG_FRACTION * rois_per_image).astype(np.<span class="hljs-built_in">int</span>)<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Caffe</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Caffe</tag>
      
      <tag>Object Detection</tag>
      
      <tag>Faster RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openpose_train</title>
    <link href="/2021/01/07/Caffe-20210107-openpose-train/"/>
    <url>/2021/01/07/Caffe-20210107-openpose-train/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a><br><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_train">https://github.com/CMU-Perceptual-Computing-Lab/openpose_train</a></p><a id="more"></a><hr><h3 id="1-拉取代码"><a href="#1-拉取代码" class="headerlink" title="1. 拉取代码"></a>1. 拉取代码</h3><h2 id><a href="#" class="headerlink" title></a><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">git clone https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/CMU-Perceptual-Computing-Lab/</span>openpose_train.git<br></code></pre></td></tr></table></figure></h2><h3 id="2-下载数据集"><a href="#2-下载数据集" class="headerlink" title="2. 下载数据集"></a>2. 下载数据集</h3><h4 id="2-1-cocoapi"><a href="#2-1-cocoapi" class="headerlink" title="2.1. cocoapi"></a>2.1. cocoapi</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs awk">mkdir -p openpose_train<span class="hljs-regexp">/dataset/</span><br>mkdir -p openpose_train<span class="hljs-regexp">/dataset/</span>COCO/<br>cd openpose_train<span class="hljs-regexp">/dataset/</span>COCO/<br>git clone --recurse https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/gineshidalgo99/</span>cocoapi.git<br><br><span class="hljs-comment"># 安装 mingw, matlab PCT</span><br><span class="hljs-comment"># 编译 gasonMex, maskApiMex</span><br>cd cocoapi<span class="hljs-regexp">/MatlabAPI/</span><br>mex(<span class="hljs-string">&#x27;CXXFLAGS=$CXXFLAGS -std=c++11 -Wall&#x27;</span>,<span class="hljs-string">&#x27;-largeArrayDims&#x27;</span>,<span class="hljs-string">&#x27;private/gasonMex.cpp&#x27;</span>,<span class="hljs-string">&#x27;../common/gason.cpp&#x27;</span>,<span class="hljs-string">&#x27;-I../common/&#x27;</span>,<span class="hljs-string">&#x27;-outdir&#x27;</span>,<span class="hljs-string">&#x27;private&#x27;</span>);<br>mex(<span class="hljs-string">&#x27;COMPFLAGS=\$CFLAGS -Wall -std=c++11&#x27;</span>,<span class="hljs-string">&#x27;-largeArrayDims&#x27;</span>,<span class="hljs-string">&#x27;private/maskApiMex.c&#x27;</span>,<span class="hljs-string">&#x27;../common/maskApi.c&#x27;</span>,<span class="hljs-string">&#x27;-I../common/&#x27;</span>,<span class="hljs-string">&#x27;-outdir&#x27;</span>,<span class="hljs-string">&#x27;private&#x27;</span>);<br></code></pre></td></tr></table></figure><h4 id="2-2-coco数据集"><a href="#2-2-coco数据集" class="headerlink" title="2.2. coco数据集"></a>2.2. coco数据集</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># annotations_trainval2017, image_info_test2017 --&gt; dataset/cocoapi/annotations/</span><br><span class="hljs-comment"># train2017, val2017, test2017                  --&gt; dataset/cocoapi/images/</span><br><br><br>http:<span class="hljs-regexp">//im</span>ages.cocodataset.org<span class="hljs-regexp">/zips/</span>train2017.zip<br>http:<span class="hljs-regexp">//im</span>ages.cocodataset.org<span class="hljs-regexp">/zips/</span>val2017.zip<br>http:<span class="hljs-regexp">//im</span>ages.cocodataset.org<span class="hljs-regexp">/zips/</span>test2017.zip<br>http:<span class="hljs-regexp">//im</span>ages.cocodataset.org<span class="hljs-regexp">/annotations/</span>annotations_trainval2017.zip<br>http:<span class="hljs-regexp">//im</span>ages.cocodataset.org<span class="hljs-regexp">/annotations/im</span>age_info_test2017.zip<br>http:<span class="hljs-regexp">//im</span>ages.cocodataset.org<span class="hljs-regexp">/annotations/</span>stuff_annotations_trainval2017.zip<br></code></pre></td></tr></table></figure><h4 id="2-3-生成LMDB"><a href="#2-3-生成LMDB" class="headerlink" title="2.3. 生成LMDB"></a>2.3. 生成LMDB</h4><h2 id="-1"><a href="#-1" class="headerlink" title></a><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta"># 生成 body keypoints LMDB</span><br><span class="hljs-meta"># 依次执行 a1_coco_jsonToNegativesJson, a2_coco_jsonToMat, a3_coco_matToMasks, a4_coco_matToRefinedJson</span><br>python c_generateLmdbs.py<br><br><span class="hljs-meta"># 生成 foot keypoints LMDB</span><br><span class="hljs-meta"># 依次执行 a2_coco_jsonToMat, a4_coco_matToRefinedJson</span><br>python c_generateLmdbs.py<br></code></pre></td></tr></table></figure></h2><h3 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3. 模型训练"></a>3. 模型训练</h3><h4 id="3-1-拉取-OpenPose-Caffe-和-预训练模型"><a href="#3-1-拉取-OpenPose-Caffe-和-预训练模型" class="headerlink" title="3.1. 拉取 OpenPose Caffe 和 预训练模型"></a>3.1. 拉取 OpenPose Caffe 和 预训练模型</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd <span class="hljs-regexp">/openpose_train/</span><br>git clone --recurse github.com<span class="hljs-regexp">/CMU-Perceptual-Computing-Lab/</span>openpose_caffe_train.git<br><br>cd <span class="hljs-regexp">/openpose_train/</span>dataset/<br>mkdir vgg<br>wget -c http:<span class="hljs-regexp">//</span>www.robots.ox.ac.uk<span class="hljs-regexp">/~vgg/</span>software<span class="hljs-regexp">/very_deep/</span>caffe/VGG_ILSVRC_19_layers.caffemodel<br>wget -c https:<span class="hljs-regexp">//gi</span>st.githubusercontent.com<span class="hljs-regexp">/ksimonyan/</span><span class="hljs-number">3785162</span>f95cd2d5fee77<span class="hljs-regexp">/raw/</span>bb2b4fe0a9bb0669211cf3d0bc949dfdda173e9e/VGG_ILSVRC_19_layers_deploy.prototxt<br></code></pre></td></tr></table></figure><h4 id="3-2-生成-prototxt"><a href="#3-2-生成-prototxt" class="headerlink" title="3.2. 生成 prototxt"></a>3.2. 生成 prototxt</h4><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/openpose_train/training</span><br>python d_<span class="hljs-keyword">set</span>Layers.py<br></code></pre></td></tr></table></figure><h4 id="3-3-训练"><a href="#3-3-训练" class="headerlink" title="3.3. 训练"></a>3.3. 训练</h4><h2 id="-2"><a href="#-2" class="headerlink" title></a><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd <span class="hljs-regexp">/openpose_train/</span>training_result<span class="hljs-regexp">/pose/</span><br>./train_pose.sh <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></h2><h3 id="4-测试"><a href="#4-测试" class="headerlink" title="4.测试"></a>4.测试</h3><h4 id="4-1-速度测试"><a href="#4-1-速度测试" class="headerlink" title="4.1. 速度测试"></a>4.1. 速度测试</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># Test using LMDB:</span><br><span class="hljs-regexp">/openpose_train/</span>openpose_caffe_train<span class="hljs-regexp">/build/</span>tools/caffe time -gpu <span class="hljs-number">0</span> -model pose_training.prototxt<br><br><span class="hljs-comment"># Test using input layer:  368*368</span><br><span class="hljs-regexp">/openpose_train/</span>openpose_caffe_train<span class="hljs-regexp">/build/</span>tools/caffe time -gpu <span class="hljs-number">0</span> -model pose_deploy.prototxt -phase TEST<br></code></pre></td></tr></table></figure><pre><code>| batchsize | 1     | 8     |16     | **deploy**||---        |---    |---    |---    |---        ||time(ms)   |19     |103    |199    |17.3       |</code></pre><h4 id="4-2-Evaluate"><a href="#4-2-Evaluate" class="headerlink" title="4.2. Evaluate"></a>4.2. Evaluate</h4><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs nix">cd /openpose<br>./scripts/tests/pose_accuracy_coco_val.sh <br><span class="hljs-comment"># https://github.com/gineshidalgo99/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb  --&gt; evaluate.py</span><br>python scripts/tests/evaluate.py<br><br><br>Body25 baseline:<br> <span class="hljs-number">5000</span>samples/<span class="hljs-number">119</span><span class="hljs-attr">s</span> = <span class="hljs-number">42</span>FPS<br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.523</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.763</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.568</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.466</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.604</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.576</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.790</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.613</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.483</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.709</span><br><br>COCO baseline:<br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.490</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.742</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.520</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.426</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.588</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.544</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.766</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.571</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.442</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.688</span><br><br>body23:<br> <span class="hljs-number">5000</span>samples/<span class="hljs-number">120</span><span class="hljs-attr">s</span> = <span class="hljs-number">41</span>FPS<br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.433</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.692</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.450</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.360</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.535</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.484</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.719</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.502</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.378</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.632</span><br> <br> <span class="hljs-number">90</span> degree<br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.324</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.602</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.304</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.259</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.416</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.376</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.633</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.366</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.283</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.505</span><br><br> <span class="hljs-number">60</span> degree<br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.345</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.630</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.327</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.290</span><br> Average Precision  (AP) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.425</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.399</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.662</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.75</span>      | <span class="hljs-attr">area=</span>   all | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.392</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=medium</span> | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.313</span><br> Average Recall     (AR) @[ <span class="hljs-attr">IoU=0.50:0.95</span> | <span class="hljs-attr">area=</span> large | <span class="hljs-attr">maxDets=</span> <span class="hljs-number">20</span> ] = <span class="hljs-number">0.520</span><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Caffe</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Caffe</tag>
      
      <tag>OpenPose</tag>
      
      <tag>Pose Estimation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow object-detecct-api</title>
    <link href="/2021/01/07/TensorFlow-20210107-tensorflow-object-detecct-api/"/>
    <url>/2021/01/07/TensorFlow-20210107-tensorflow-object-detecct-api/</url>
    
    <content type="html"><![CDATA[<h2 id="0-安装"><a href="#0-安装" class="headerlink" title="0. 安装"></a>0. 安装</h2><h2 id><a href="#" class="headerlink" title></a><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs elixir">apt-get install protobuf-compiler<br>pip install pillow<br>pip install lxml<br>pip install jupyter<br>pip install matplotlib<br><br><span class="hljs-comment"># From tensorflow/models/research/</span><br>protoc object_detection/protos/*.proto --python_out=.<br><br><span class="hljs-comment"># From tensorflow/models/research/</span><br>export PYTHONPATH=<span class="hljs-variable">$PYTHONPATH</span><span class="hljs-symbol">:`pwd`</span><span class="hljs-symbol">:`pwd`/slim</span><br></code></pre></td></tr></table></figure></h2><a id="more"></a><h2 id="1-训练命令"><a href="#1-训练命令" class="headerlink" title="1. 训练命令"></a>1. 训练命令</h2><p><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md</a><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> From the tensorflow/models/research/ directory</span><br>python object_detection/train.py \<br>    --logtostderr \<br>    --pipeline_config_path=$&#123;PATH_TO_YOUR_PIPELINE_CONFIG&#125; \<br>    --train_dir=$&#123;PATH_TO_TRAIN_DIR&#125;<br></code></pre></td></tr></table></figure><br><figure class="highlight plain"><figcaption><span>[pipeline_config.pbtxt]</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs PATH_TO_YOUR_PIPELINE_CONFIG```：网络设置文件">&#96;&#96;&#96;PATH_TO_TRAIN_DIR&#96;&#96;&#96;：训练模型保存位置<br><br>---<br>## 2. PIPELINE_CONFIG<br>&lt;https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;models&#x2F;blob&#x2F;master&#x2F;research&#x2F;object_detection&#x2F;g3doc&#x2F;configuring_jobs.md&gt;<br>&#96;&#96;&#96;proto<br>model &#123;<br>(... Add model config here...)<br>&#125;<br><br>train_config : &#123;<br>(... Add train_config here...)<br>&#125;<br><br>train_input_reader: &#123;<br>(... Add train_input configuration here...)<br>&#125;<br><br>eval_config: &#123;<br>&#125;<br><br>eval_input_reader: &#123;<br>(... Add eval_input configuration here...)<br>&#125;<br></code></pre></td></tr></table></figure><br>pipeline_config 文件分为以下五个部分</p><ul><li>model: <code>object_detection/samples/model_configs</code></li><li>train_config:   <figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">batch_size:</span> <span class="hljs-number">1</span><br><span class="hljs-class">optimizer </span>&#123;<br><span class="hljs-symbol">momentum_optimizer:</span> &#123;<br><span class="hljs-symbol">learning_rate:</span> &#123;<br>  <span class="hljs-class">manual_step_learning_rate </span>&#123;<br><span class="hljs-symbol">    initial_learning_rate:</span> <span class="hljs-number">0.0002</span><br>    <span class="hljs-class">schedule </span>&#123;<br><span class="hljs-symbol">      step:</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">      learning_rate:</span> <span class="hljs-number">.0002</span><br>    &#125;<br>    <span class="hljs-class">schedule </span>&#123;<br><span class="hljs-symbol">      step:</span> <span class="hljs-number">900000</span><br><span class="hljs-symbol">      learning_rate:</span> <span class="hljs-number">.00002</span><br>    &#125;<br>    <span class="hljs-class">schedule </span>&#123;<br><span class="hljs-symbol">      step:</span> <span class="hljs-number">1200000</span><br><span class="hljs-symbol">      learning_rate:</span> <span class="hljs-number">.000002</span><br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-symbol">momentum_optimizer_value:</span> <span class="hljs-number">0.9</span><br>&#125;<br><span class="hljs-symbol">use_moving_average:</span> false<br>&#125;<br><span class="hljs-symbol">fine_tune_checkpoint:</span> <span class="hljs-string">&quot;/usr/home/username/tmp/model.ckpt-#####&quot;</span><br><span class="hljs-symbol">from_detection_checkpoint:</span> true<br><span class="hljs-symbol">gradient_clipping_by_norm:</span> <span class="hljs-number">10.0</span><br><span class="hljs-class">data_augmentation_options </span>&#123;<br>    <span class="hljs-class">random_horizontal_flip </span>&#123;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li><li>train_input_reader:<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-class">tf_record_input_reader </span>&#123;<br>input_path&#123;<br>    <span class="hljs-string">&quot;/usr/home/username/data/train.record&quot;</span><br>&#125;<br><span class="hljs-symbol">label_map_path:</span> <span class="hljs-string">&quot;/usr/home/username/data/label_map.pbtxt&quot;</span><br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="3-TF-record"><a href="#3-TF-record" class="headerlink" title="3. TF-record"></a>3. TF-record</h2><p><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md</a></p><h2 id="-1"><a href="#-1" class="headerlink" title></a><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment"># From tensorflow/models/research/</span><br>wget http:<span class="hljs-string">//host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</span><br>tar -xvf VOCtrainval_11-May-2012.tar<br>python object_detection/create_pascal_tf_record.py \<br>    <span class="hljs-params">--label_map_path=object_detection/data/pascal_label_map</span>.pbtxt \<br>    <span class="hljs-params">--data_dir=VOCdevkit</span> <span class="hljs-params">--year=VOC2012</span> <span class="hljs-params">--set=train</span> \<br>    <span class="hljs-params">--output_path=pascal_train</span>.record<br>python object_detection/create_pascal_tf_record.py \<br>    <span class="hljs-params">--label_map_path=object_detection/data/pascal_label_map</span>.pbtxt \<br>    <span class="hljs-params">--data_dir=VOCdevkit</span> <span class="hljs-params">--year=VOC2012</span> <span class="hljs-params">--set=val</span> \<br>    <span class="hljs-params">--output_path=pascal_val</span>.record<br></code></pre></td></tr></table></figure></h2><h2 id="4-export-trained-model"><a href="#4-export-trained-model" class="headerlink" title="4. export trained model"></a>4. export trained model</h2><p><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md</a><br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># From tensorflow/models/research/</span><br>python object_detection/export_inference_graph.py \<br>    --input_type image_tensor \<br>    --pipeline_config_path  object_detection<span class="hljs-regexp">/dxx_models/i</span>nception_resnet_v2/pipeline_config.pbtxt \ <span class="hljs-comment">#$&#123;PIPELINE_CONFIG_PATH&#125; i</span><br>    --trained_checkpoint_prefix object_detection<span class="hljs-regexp">/dxx_models/i</span>nception_resnet_v2<span class="hljs-regexp">/train/m</span>odel.ckpt-<span class="hljs-number">381545</span> \ <span class="hljs-comment">#$&#123;MODEL_PATH&#125; </span><br>    --output_directory object_detection<span class="hljs-regexp">/dxx_models/i</span>nception_resnet_v2<span class="hljs-regexp">/train/</span>output_inference_graph.pb2<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>Object Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Non-local Neural Network</title>
    <link href="/2021/01/07/Caffe2-20210107-Non-local-Neural-Network/"/>
    <url>/2021/01/07/Caffe2-20210107-Non-local-Neural-Network/</url>
    
    <content type="html"><![CDATA[<p>Facebook 的 Non-local 开源了，上周简单试验了一下怎么用。记录一下步骤<br><a href="https://github.com/facebookresearch/video-nonlocal-net">https://github.com/facebookresearch/video-nonlocal-net</a></p><a id="more"></a><hr><h2 id="Caffe2-Install"><a href="#Caffe2-Install" class="headerlink" title="Caffe2 Install"></a>Caffe2 Install</h2><p><a href="https://github.com/facebookresearch/video-nonlocal-net/blob/master/INSTALL.md">https://github.com/facebookresearch/video-nonlocal-net/blob/master/INSTALL.md</a></p><ul><li>依赖库<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recrusive http://github.com/caffe2/caffe2.git<br><br>apt-get update<br>apt-get install -y --no-install-recommends \<br>    build-essential \<br>    cmake \<br>    git \<br>    libgoogle-glog-dev \<br>    libgtest-dev \<br>    libiomp-dev \<br>    libleveldb-dev \<br>    liblmdb-dev \<br>    libopencv-dev \<br>    libopenmpi-dev \<br>    libsnappy-dev \<br>    libprotobuf-dev \<br>    openmpi-bin \<br>    openmpi-doc \<br>    protobuf-compiler \<br>    python-dev \<br>    python-pip                          <br>pip install \<br>    future \<br>    numpy \<br>    protobuf<br></code></pre></td></tr></table></figure></li><li>中间可能需要安装 <code>setuptools</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt install wget<br>wget http://pypi.python.org/packages/<span class="hljs-built_in">source</span>/s/setuptools/setuptools-0.6c11.tar.gz<br>tar -xvf setuptools-0.6c11.tar.gz<br><span class="hljs-built_in">cd</span> setuptools-0.6c11<br>python setup.py build<br>python setup.py install<br></code></pre></td></tr></table></figure></li><li>Clone Non-local 代码<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> caffe2<br>git <span class="hljs-built_in">clone</span> --recursive https://github.com/facebookresearch/video-nonlocal-net.git<br>rm -rf caffe2/video/<br>cp -r video-nonlocal-net/caffe2_customized_ops/video/ caffe2/video/<br></code></pre></td></tr></table></figure></li><li>Make  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># use_ffmpeg ON</span><br>make -j<br>python caffe2_setup.py install      <span class="hljs-comment"># (not sure how it works)</span><br></code></pre></td></tr></table></figure></li><li>配置环境变量<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> PYTHONPATH=&lt;caffe2-dir&gt;/video-nonlocal-net/lib:<span class="hljs-variable">$PYTHONPATH</span><br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><p><a href="https://github.com/facebookresearch/video-nonlocal-net/blob/master/DATASET.md">https://github.com/facebookresearch/video-nonlocal-net/blob/master/DATASET.md</a></p><p>我是用 UCF-101 数据集进行 finetune 的，主要参照了官方的链接制作 LMDB</p><ul><li>图片准备<br>首先下载解压 UCF-101 数据集到 <code>/data</code> 目录下<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">root@dxx# ls /data/<br>UCF-101/  ucfTrainTestlist/<br></code></pre></td></tr></table></figure></li><li>生成 list 文件<br>官方文档里面使用 gen_py_list.py 生成 trainlist.txt<br>这里我们直接用 ucf101 提供的 trainlist01.txt<br>我们还需要做另外两个工作 （1）ucf101 的标签是从 1 开始，需要改成 0； （2）ucf101 中的 testlist01.txt 里面没有标签信息，需要自己加上<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> &lt;caffe2-dir&gt;/video-nonlocal-net/process_data/<br>cp -r kinetics/ ucf101/<br><span class="hljs-built_in">cd</span> ucf101/<br><br>rm -f *.txt<br>cp /data/ucfTrainTestlist/classInd.txt .<br>cp /data/ucfTrainTestlist/trainlist01.txt .<br>cp /data/ucfTrainTestlist/testlist01.txt .<br><br><br>python gen_py_list_football.py<br><span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;</span><br><span class="hljs-string">&gt;&gt;&gt;python</span><br><span class="hljs-string">d = &#123;&#125;</span><br><span class="hljs-string">for line in open(&#x27;</span>classInd.txt<span class="hljs-string">&#x27;):</span><br><span class="hljs-string">    ind, label = line.strip().split()</span><br><span class="hljs-string">    d[label] = ind</span><br><span class="hljs-string">    </span><br><span class="hljs-string">with open(&#x27;</span>trainlist.txt<span class="hljs-string">&#x27;, &#x27;</span>w<span class="hljs-string">&#x27;) as f:</span><br><span class="hljs-string">    for line in open(&#x27;</span>trainlist01.txt<span class="hljs-string">&#x27;):</span><br><span class="hljs-string">        img, ind = line.strip().split()</span><br><span class="hljs-string">        if ind == &#x27;</span>101<span class="hljs-string">&#x27;:</span><br><span class="hljs-string">            ind = &#x27;</span>0<span class="hljs-string">&#x27;</span><br><span class="hljs-string">        f.write(&#x27;</span>/data/UCF-101/<span class="hljs-string">&#x27; + img + &#x27;</span> <span class="hljs-string">&#x27; + ind + &#x27;</span>\n<span class="hljs-string">&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">with open(&#x27;</span>vallist.txt<span class="hljs-string">&#x27;, &#x27;</span>w<span class="hljs-string">&#x27;) as f:</span><br><span class="hljs-string">    for line in open(&#x27;</span>testlist01.txt<span class="hljs-string">&#x27;):</span><br><span class="hljs-string">        img = line.strip()</span><br><span class="hljs-string">        label = img.split(&#x27;</span>/<span class="hljs-string">&#x27;)[0]</span><br><span class="hljs-string">        ind = d[label]</span><br><span class="hljs-string">        if ind == &#x27;</span>101<span class="hljs-string">&#x27;:</span><br><span class="hljs-string">            ind = &#x27;</span>0<span class="hljs-string">&#x27;</span><br><span class="hljs-string">        f.write(&#x27;</span>/data/UCF-101/<span class="hljs-string">&#x27; + img + &#x27;</span> <span class="hljs-string">&#x27; + ind + &#x27;</span>\n<span class="hljs-string">&#x27;)</span><br><span class="hljs-string">&#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br></code></pre></td></tr></table></figure></li><li>resize 数据集<br>为了加快训练时候的 IO 速度，我们可以提前把数据集 resize 好，官方直接写好了一个脚本<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 修改 downscale_video_joblib.py:21,22</span><br>YOUR_DATASET_FOLDER/train/  --&gt; /data/UCF-101/<br>YOUR_DATASET_FOLDER/train_256/  --&gt; /data/UCF-101_s256/      <span class="hljs-comment"># 注意不能少了路径最后的 &#x27;/&#x27;</span><br><br>pip install joblib pandas<br>mkdir -p /data/UCF-101_s256/<br>python downscale_video_joblib.py<br><span class="hljs-comment"># 修改 downscale_video_joblib.py:20</span><br>trainlist.txt  --&gt;  vallist.txt<br>python downscale_video_joblib.py<br></code></pre></td></tr></table></figure></li><li>生成 LMDB<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed -i <span class="hljs-string">&#x27;s/UCF-101/UCF-101_s256/g&#x27;</span> trainlist.txt<br>sed -i <span class="hljs-string">&#x27;s/UCF-101/UCF-101_s256/g&#x27;</span> vallist.txt<br>python shuffle_list_rep.py trainlist_shuffle_rep.txt<br><br>pip install lmdb<br>bash run_createdb.sh<br>bash run_createdb_test.sh<br>bash run_createdb_test_multicrop.sh<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p><a href="https://github.com/facebookresearch/video-nonlocal-net/blob/master/README.md">https://github.com/facebookresearch/video-nonlocal-net/blob/master/README.md</a></p><p>训练过程可以有很多种设置，这里我选择的是 <code>run_i3d_nlnet_400k.sh</code> 这个脚本，具体区别看官方说明。<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs vala">cd &lt;caffe2-dir&gt;/video-nonlocal-net/scripts/<br>pip install networkx<br>apt install python-yaml<br><br><span class="hljs-meta"># 设置一下 yaml 文件:  ../configs/DBG_kinetics_resnet_8gpu_c2d_nonlocal_400k.yaml</span><br><span class="hljs-meta"># GPU_NUMS</span><br><span class="hljs-meta"># MODEL.NUM_CLASSES</span><br><span class="hljs-meta"># TRAIN.BATCH_SIZE</span><br><span class="hljs-meta"># TEST.BATCH_SIZE</span><br><span class="hljs-meta"># TEST.DATASET_SIZE</span><br><br><span class="hljs-meta">#  修改 solver 超参数</span><br><br><br><span class="hljs-meta"># 修改  run_i3d_nlnet_400k.sh</span><br><span class="hljs-meta"># 6: TRAIN.PARAMS_FILE ../data/pretrained_model/i3d_nonlocal_8x8_IN_pretrain_400k_clean.pkl \</span><br><span class="hljs-meta"># 14: FILENAME_GT ../process_data/ucf101/vallist.txt \</span><br><span class="hljs-meta">#</span><br><span class="hljs-meta">#</span><br>chmod <span class="hljs-number">777</span> run_i3d_nlnet_400k.sh<br>./run_i3d_nlnet_400k.sh<br></code></pre></td></tr></table></figure></p><blockquote><p>按照上面的步骤基本可以开始训练 ucf101了，这里的预训练模型有两种选择，一种是在 Kinetics 上训练时选择的 ImageNet 预训练模型，脚本里面名字是 <a href="https://s3.amazonaws.com/video-nonlocal/pretrained_model.tar.gz"><code>../data/pretrained_model/r50_pretrain_c2_model_iter450450_clean.pkl</code></a>; 一种是在 Kinetics 上训练好的模型 <a href="https://s3.amazonaws.com/video-nonlocal/i3d_nonlocal_8x8_IN_pretrain_400k.pkl"><code>i3d_nonlocal_8x8_IN_pretrain_400k</code></a>。<br>我选择的是后者，但是刚开始训练的时候就会认为已经进行到了 400k 次迭代，所以直接结束了训练。我最后在 <code>../tools/train_net_video.py:131</code> 加上 <code>start_model_iter = 0</code>训练了起来（小伙伴可以尝试其他更好的解决办法）。一旦网络保存过一次 checkpoint，下次训练就会从 checkpoint 里选择最新的模型继续训练，这个时候就可以删掉 <code>start_model_iter = 0</code> 了。</p></blockquote><hr><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><p>视频分类这一块的模型测试，通常都会有两层意思，<br>一个是对测试视频进行上述的数据准备操作，对于 C3D (caffe版) 来说，是生成 lst 文件；对于 Non-local I3D (caffe2版) 来说，是生成 lmdb 文件，这两个文件的作用都是定义测试 clip 在 video/image-squence 中对应的帧号，网络读取数据准备的结果文件后，会组织数据送入后面的卷积层。（当然，数据准备过程只与输入层的实现相关，与网络结构没有关系），这一部分工作与训练几乎没有区别，并且可以方便进行测试，因此很容易跑通。<br>另一个意思就是对视频输入进行分类测试，也就是落地部署，一般的开源项目只为了让人复现实验结果，并不会实现这一部分。视频输入 与 文本/lmdb 输入具有不小的 gap，为了能够直接在视频上测试，需要对网络的输入模块进行改动，下面是 caffe2 版 Non-local I3D 部署的步骤。</p><ul><li>Input layer modification<br><a href="https://github.com/clover978/video-nonlocal-net/blob/master/tools/deploy_net_video_local.py">https://github.com/clover978/video-nonlocal-net/blob/master/tools/deploy_net_video_local.py</a></li><li>Image propocess<br><code>caffe2_customized_ops/video/customized_video_input_op.h:200-221</code> summarize 了 Non-local I3D 的 video_input_op 对视频帧进行的预处理过程。更改后的网络结构需要实现对应的功能。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># customized_video_input_op.h:206,  scale</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_scale</span>(<span class="hljs-params">frame, size=(<span class="hljs-params"><span class="hljs-number">320</span>, <span class="hljs-number">256</span></span>)</span>):</span><br>    <span class="hljs-keyword">return</span> cv2.resize(frame, size)<br><br><span class="hljs-comment"># customized_video_input_op.h:208,210,  crop</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_crop</span>(<span class="hljs-params">frame, crop_size=<span class="hljs-number">224</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;random&#x27;</span></span>):</span><br>    <span class="hljs-comment"># type: &#123;&#x27;random&#x27;, &#x27;center&#x27;&#125;</span><br>    h, w  = frame.shape[:<span class="hljs-number">2</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;random&#x27;</span>:<br>        x = random.randint(crop_size//<span class="hljs-number">2</span>, w-crop_size//<span class="hljs-number">2</span>-<span class="hljs-number">1</span>)<br>        y = random.randint(crop_size//<span class="hljs-number">2</span>, h-crop_size//<span class="hljs-number">2</span>-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;center&#x27;</span>:<br>        x = w//<span class="hljs-number">2</span><br>        y = h//<span class="hljs-number">2</span><br>    <span class="hljs-keyword">else</span>:<br>        logger.warning(<span class="hljs-string">&#x27;unknow type, use center crop instead&#x27;</span>)<br>        x = w//<span class="hljs-number">2</span><br>        y = h//<span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> frame[y-crop_size//<span class="hljs-number">2</span>:y+crop_size//<span class="hljs-number">2</span>, x-crop_size//<span class="hljs-number">2</span>:x+crop_size//<span class="hljs-number">2</span>, :]<br><br><span class="hljs-comment"># customized_video_input_op.h:211,212,  sampling</span><br><span class="hljs-comment"># @sa: deploy_net_video_local:collect_clip()</span><br><br><span class="hljs-comment"># customized_video_input_op.h:213,  normalize</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_normalize</span>(<span class="hljs-params">clip, mean=<span class="hljs-number">128</span>, std=<span class="hljs-number">1</span></span>):</span><br>    <span class="hljs-comment"># just do it over clip</span><br>    <span class="hljs-keyword">return</span> (clip-mean)/std<br>    <br><span class="hljs-comment"># customized_video_input_op.h:219,   channel_swap</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_channel_swap</span>(<span class="hljs-params">frame, use_bgr=<span class="hljs-literal">True</span></span>):</span><br>    <span class="hljs-keyword">if</span> use_bgr:<br>        <span class="hljs-keyword">return</span> cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> frame<br><br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><ol><li><p>训练过程中，会周期性地进行 test，我在 test 的时候报错</p> <figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs groovy"><span class="hljs-attr">Traceback:</span><br>    File <span class="hljs-string">&quot;lib/utils/misc.py:200&quot;</span>:<br>        used_gpu_memory = out_dict[<span class="hljs-string">&#x27;Used GPU Memory&#x27;</span>]<br><span class="hljs-attr">KeyError:</span> Used GPU Memory<br></code></pre></td></tr></table></figure><p> 查了半天不知道正确的 key 是什么，直接注释掉了 <code>lib/utils/metrics.py:343</code> </p> <figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autoit"><span class="hljs-meta"># json_stats[<span class="hljs-string">&#x27;used_gpu_memory&#x27;</span>] = misc.get_gpu_stats()</span><br></code></pre></td></tr></table></figure></li><li><p>关于训练时使用 <code>i3d_nonlocal_8x8_IN_pretrain_400k</code> 会直接结束训练的问题，作者提供了一个新的脚本，重置了 lr, iteration, momentum 等信息。</p> <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd &lt;caffe2-dir&gt;<span class="hljs-regexp">/video-nonlocal-net/</span>process_data<span class="hljs-regexp">/convert_models/</span><br><span class="hljs-comment"># 修改 modify_blob_rm.py: 5,6 行</span><br>input_model_file  --&gt; ..<span class="hljs-regexp">/../</span>data<span class="hljs-regexp">/pretrained_model/i</span>3d_nonlocal_8x8_IN_pretrain_400k.pkl<br>output_model_file  --&gt;  ..<span class="hljs-regexp">/../</span>data<span class="hljs-regexp">/pretrianed_model/i</span>3d_nonlocal_8x8_IN_pretrain_400k_clean.pkl<br></code></pre></td></tr></table></figure></li><li><p>finetune 的时候，log 中显示的进度是根据 Kinetic 数据集的规模计算，有很大的出入，具体的设置在 <code>&lt;caffe2-dir&gt;/video-nonlocal-net/lib/core/config.py:78</code>, 配置 yaml 文件可以修改，添加 <code>TRAIN.DATASET_SIZE = 9537</code> </p></li><li><p>训练是输入的是视频，具体按照怎样的规则得到 clip ？<br>github issue 中有两个是与这个问题相关的，可以参考 <a href="https://github.com/facebookresearch/video-nonlocal-net/issues/12">#12</a> <a href="https://github.com/facebookresearch/video-nonlocal-net/issues/15">#15</a><br>简单来说，第一步会 decode 出输入视频的所有帧，此时如果遇到太短的视频会抛弃掉，从源代码看，最小是 32K 字节。<code>==&gt; video/customized_video_decoder.cc:79</code><br>第二步从解码出来的帧中，随机选择 clip_length 长度的图片进行训练。 <code>==&gt; video/customized_video_io.cc:674,687</code></p></li><li><p>训练和测试的时候视频经过怎样的预处理？<br><a href="https://github.com/facebookresearch/video-nonlocal-net/issues/16">#16</a><br><a href="https://github.com/facebookresearch/video-nonlocal-net/issues/10">#10</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Caffe2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Action Recgnition</tag>
      
      <tag>Caffe2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSN-Pytorch</title>
    <link href="/2021/01/07/Pytorch-20210107-TSN-Pytorch/"/>
    <url>/2021/01/07/Pytorch-20210107-TSN-Pytorch/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/yjxiong/tsn-pytorch">https://github.com/yjxiong/tsn-pytorch</a></p><hr><h3 id="0-Intro"><a href="#0-Intro" class="headerlink" title="0. Intro"></a>0. Intro</h3><p>TSN 是动作识别领域里面一个很老工作，也是一个很重要的工作。作者开源了代码，是用 Caffe 实现的，但是由于 Pytorch 在学界的流行，作者又重新实现了一个 Pytorch 版本的。代码写的很优雅，基本是可以直接用的，这个简单记录一下使用步骤。<br><strong>TSN 的训练模式有 3 种, (1) RGB, (2) Flow, (3) RGB+Flow, 这里只介绍第一种</strong><br><strong>下文需要用到的代码片段都已经上传到 fork 的 repo 中 <a href="https://github.com/clover978/tsn-pytorch">https://github.com/clover978/tsn-pytorch</a></strong></p><a id="more"></a><hr><h3 id="1-Data-Prepare"><a href="#1-Data-Prepare" class="headerlink" title="1. Data-Prepare"></a>1. Data-Prepare</h3><ul><li>抽帧：<br>首先生成视频文件列表：  <figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gradle">cd <span class="hljs-regexp">/home/</span>dxx<span class="hljs-regexp">/UCF101/U</span>CF101/<br><span class="hljs-keyword">find</span> . -type f &gt; trainvallist.txt<br></code></pre></td></tr></table></figure>然后抽帧：<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">cd</span> <span class="hljs-symbol">&lt;tsn-path&gt;</span><br><span class="hljs-keyword">python</span> tools/extract_frames.<span class="hljs-keyword">py</span><br></code></pre></td></tr></table></figure></li><li>生成 lst 文件：<br><code>tsn-pytorch</code> 用文本文件的形式作为输入，读取数据集。文本文件的格式如下：<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text"># 视频帧路径  视频总帧数  视频标签<br>&lt;image_dir_path&gt; &lt;frames_cnt&gt; &lt;label_index&gt;<br></code></pre></td></tr></table></figure>准备 <code>classInd.txt</code> 文件，用下面的脚本生成训练需要的文本文件：<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> gen_tsn_list.<span class="hljs-keyword">py</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="2-Train"><a href="#2-Train" class="headerlink" title="2. Train"></a>2. Train</h3><ul><li>ucf101 训练：<br>UCF101数据集 可以直接使用如下的命令训练：  <figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-comment"># tools/train_tsn.sh</span><br><br><span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span>,<span class="hljs-string">2</span>,<span class="hljs-string">3</span> \<br>        <span class="hljs-string">python</span> -<span class="hljs-string">u</span> <span class="hljs-string">main</span>.<span class="hljs-string">py</span> <span class="hljs-string">ucf101</span> <span class="hljs-string">RGB</span> <span class="hljs-string">train_tsn</span>.<span class="hljs-string">txt</span> <span class="hljs-string">val_tsn</span>.<span class="hljs-string">txt</span> \<br>        <span class="hljs-built_in">--arch</span> <span class="hljs-string">BNInception</span> \<br>        <span class="hljs-built_in">--num_segments</span> <span class="hljs-string">5</span> <span class="hljs-built_in">--gd</span> <span class="hljs-string">20</span> \<br>        <span class="hljs-built_in">--lr</span> <span class="hljs-string">0</span>.<span class="hljs-string">001</span> <span class="hljs-built_in">--lr_steps</span> <span class="hljs-string">70</span> <span class="hljs-string">130</span> <span class="hljs-built_in">--epochs</span> <span class="hljs-string">180</span> \<br>        -<span class="hljs-string">b</span> <span class="hljs-string">140</span> -<span class="hljs-string">j</span> <span class="hljs-string">8</span> <span class="hljs-built_in">--dropout</span> <span class="hljs-string">0</span>.<span class="hljs-string">5</span> <span class="hljs-built_in">--snapshot_pref</span> <span class="hljs-string">models</span>/<span class="hljs-string">meitu_bninception</span> \<br>        |&amp; <span class="hljs-string">tee</span> <span class="hljs-string">log</span>.<span class="hljs-string">txt</span><br></code></pre></td></tr></table></figure></li><li>自定义数据集训练：<br>自定义的数据集，可以使用同样的命令训练，训练之前只需要修改 <code>main.py</code>, 将 num_class 替换为自己数据集的类别数<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vim"># main.<span class="hljs-keyword">py</span>:<span class="hljs-number">25</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">args</span>.dataset == <span class="hljs-string">&#x27;ucf101&#x27;</span>:<br>    num_class = &lt;datasets classes <span class="hljs-keyword">number</span>&gt;<br></code></pre></td></tr></table></figure></li><li>使用 resume 参数从中间开始训练<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">--resume models<span class="hljs-regexp">/tsn-pytorch/m</span>eitu_bninception_rgb_1_checkpoint.pth.tar<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Action Recgnition</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep-high-resolution-net.pytorch</title>
    <link href="/2021/01/07/Pytorch-20210107-Deep-high-resolution-net-pytorch/"/>
    <url>/2021/01/07/Pytorch-20210107-Deep-high-resolution-net-pytorch/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a></p><a id="more"></a><h2 id="1-配置-HRNET-环境"><a href="#1-配置-HRNET-环境" class="headerlink" title="1. 配置 HRNET 环境"></a>1. 配置 HRNET 环境</h2><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">11.1</span>-cudnn8-devel-ubuntu18.<span class="hljs-number">04</span><br><br><span class="hljs-keyword">LABEL</span><span class="bash"> auther=clover978</span><br><br><span class="hljs-comment"># Basic toolchain</span><br><span class="hljs-keyword">RUN</span><span class="bash"> apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="bash">        build-essential \</span><br><span class="bash">        git \</span><br><span class="bash">        ffmpeg \</span><br><span class="bash">        python3-pip \</span><br><span class="bash">        python3-dev \</span><br><span class="bash">        libsm6 \</span><br><span class="bash">        libxext6 \</span><br><span class="bash">    &amp;&amp; <span class="hljs-built_in">cd</span> /usr/bin \</span><br><span class="bash">    &amp;&amp; ln -s /usr/bin/python3 python \</span><br><span class="bash">    &amp;&amp; pip3 install --upgrade pip \</span><br><span class="bash">    &amp;&amp; apt-get autoremove -y \</span><br><span class="bash">    &amp;&amp; rm -rf /var/lib/apt/lists/* </span><br><br><span class="hljs-comment"># clone deep-high-resolution-net</span><br><span class="hljs-keyword">ARG</span> POSE_ROOT=/workspace/deep-high-resolution-net<br><span class="hljs-keyword">RUN</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://gitee.com/clover978/deep-high-resolution-net.pytorch.git <span class="hljs-variable">$POSE_ROOT</span></span><br><span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">$POSE_ROOT</span></span><br><br><span class="hljs-keyword">RUN</span><span class="bash"> pip3 install --no-cache-dir -r requirements.txt &amp;&amp; \</span><br><span class="bash">    pip3 install --no-cache-dir pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors \</span><br><span class="bash">    pip3 install --no-cache-dir torch==1.7.0+cu110 torchvision==0.8.1+cu110 -f https://download.pytorch.org/whl/torch_stable.html </span><br><br><span class="hljs-comment"># build deep-high-resolution-net lib</span><br><span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">$POSE_ROOT</span>/lib</span><br><span class="hljs-keyword">RUN</span><span class="bash"> make</span><br><br><span class="hljs-comment"># install COCO API</span><br><span class="hljs-keyword">ARG</span> COCOAPI=/cocoapi<br><span class="hljs-keyword">RUN</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://gitee.com/clover978/cocoapi <span class="hljs-variable">$COCOAPI</span></span><br><span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">$COCOAPI</span>/PythonAPI</span><br><span class="hljs-keyword">RUN</span><span class="bash"> make install</span><br><br><span class="hljs-comment"># clone </span><br><span class="hljs-keyword">ARG</span> DET_ROOT=/workspace/Yet-Another-EfficientDet-Pytorch<br><span class="hljs-keyword">RUN</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://gitee.com/clover978/Yet-Another-EfficientDet-Pytorch <span class="hljs-variable">$DET_ROOT</span></span><br><br><span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">$POSE_ROOT</span></span><br></code></pre></td></tr></table></figure><blockquote><p>clover fork 的 HRNET(<a href="https://gitee.com/clover978/deep-high-resolution-net.pytorch.git">https://gitee.com/clover978/deep-high-resolution-net.pytorch.git</a>) 进行了一些改动：</p><ul><li>支持 QDTM 数据集，数据集包含 18 个关键点，不同于 COCO 的 17 个点；</li><li>添加代码保存中间模型，每 10 个 epoch 保存一次；</li><li>添加 inference 代码，可以对图片文件夹进行遍历，并将结果生成视频。</li></ul><p>clover fork 的 cocoapi(<a href="https://gitee.com/clover978/cocoapi">https://gitee.com/clover978/cocoapi</a>) 进行了一个改动：</p><ul><li>更改 PythonAPI/pycocotools/cocoeval.py:523，以适应 QDTM 数据集的 18 个关键点。 </li></ul></blockquote><h2 id="2-训练、测试"><a href="#2-训练、测试" class="headerlink" title="2. 训练、测试"></a>2. 训练、测试</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">docker run -itd --name hrnet --ipc host -v <span class="hljs-regexp">/home/</span>dxx:<span class="hljs-regexp">/home/</span>dxx -v <span class="hljs-regexp">/data:/</span>data --net host dxx/hrnet:v1<br><br>$ deep-high-resolution-net.pytorch<br>python tools<span class="hljs-regexp">/train.py --cfg experiments/</span>qdtm<span class="hljs-regexp">/hrnet/</span>w48_384x288_adam_lr1e-<span class="hljs-number">3</span>_vault_v3.yaml <br>python tools<span class="hljs-regexp">/inference.py --cfg experiments/</span>qdtm<span class="hljs-regexp">/hrnet/</span>w48_384x288_adam_lr1e-<span class="hljs-number">3</span>_vault_v3.yaml TEST.MODEL_FILE output<span class="hljs-regexp">/output/</span>qdtm<span class="hljs-regexp">/pose_hrnet/</span>w48_384x288_adam_lr1e-<span class="hljs-number">3</span>_vault_v3/model_best.pth<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>Pose Estimation</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git commitzen</title>
    <link href="/2021/01/06/others-20210106-Git-commitzen/"/>
    <url>/2021/01/06/others-20210106-Git-commitzen/</url>
    
    <content type="html"><![CDATA[<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h2 id="0-Intro"><a href="#0-Intro" class="headerlink" title="0. Intro"></a>0. Intro</h2><p><code>Git</code> 是一种非常常见的 <strong>分布式版本管理工具</strong>，学习使用 <code>git</code>应该算是程序员的一项基本功了，毕竟稍微有一些体量的公司，都会有自己的版本管理系统。<br><code>Git</code> 的子命令相当之多，比较复杂的那些，有相关需求的时候上网查就可以了，这篇笔记只记录最简单常用的命令。  </p><a id="more"></a><h2 id="1-Why-Git"><a href="#1-Why-Git" class="headerlink" title="1. Why Git"></a>1. Why Git</h2><p><strong><em>以下内容纯属个人理解，没有参考网上资料，不一定正确</em></strong><br>Git 中引入了几个重要的概念：</p><ul><li>分支：<br>一个<strong>代码仓库</strong>（repo）至少拥有一个 <strong>master</strong> 分支，还可以拥有多个其他分支，在分支上的开发各自分开，然后合并（merge）到主分支中。</li><li>暂存区：<br>在 <code>git</code> 中，磁盘上的代码可以叫做<strong>本地代码</strong>，被加入到 <code>git</code>仓库的代码处于<strong>工作区</strong>，工作区的代码的改动不会直接进入<strong>分支</strong>，而是被放在<strong>暂存区</strong>中。使用 <code>git diff</code> 命令，可以轻松查看开发迭代在上版本代码上的所有改动。<br>在实际使用过程中会对这些概念有更清晰的认识。</li><li>origin/master：<br><code>git</code> 的代码管理是分布式的，同样一份代码，可以存储在各个不同的地方。这需要依赖一个 git 服务器和一个 <strong>origin</strong> 分支。因此 <code>git</code>仓库 中的代码总共存在于 5 个地方：本地磁盘、工作区、暂存区、本地分支、云端分支。前四个在本地存储，最后一个在服务器存储，没有 git 服务器同样可以使用 <code>git</code> 做版本管理，只是不支持分布式，本地删掉之后代码就没有了。<br>其中 origin 就是远程仓库的名字，master 是远程分支的名字，当仓库只有一个分支的时候，origin/master 即代表了远端仓库的代码。</li></ul><h2 id="2-Git-in-action"><a href="#2-Git-in-action" class="headerlink" title="2. Git in action"></a>2. Git in action</h2><p>使用 <code>git</code>，首先需要配置一下开发者用户名<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git config --global user.name <span class="hljs-string">&quot;yourname&quot;</span><br>git config --global user.email <span class="hljs-string">&quot;your.email@gmail.com&quot;</span><br>git config --list<br></code></pre></td></tr></table></figure><br>下面用几个命令，简单介绍 <code>git</code> 的最常见用法<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建一个 git 仓库</span><br>git init    <span class="hljs-comment"># 一般这个命令会在空文件夹下执行，此时工作区为空</span><br><br><span class="hljs-comment"># 创建一个新文件</span><br>touch readme.txt    <span class="hljs-comment"># 此时磁盘代码发生了变动，工作区依然为空</span><br><br><span class="hljs-comment"># 查看分支状态, 将代码加入 git 仓库</span><br>    <span class="hljs-comment">## --&gt; 如果文件是 new file，使用 git add &lt;file&gt; 将其加入仓库，这样文件同时进入了工作区和暂存区  </span><br>    git status<br>    git add readme.txt<br>    <span class="hljs-comment">## --&gt; 如果文件是 modified，代表文件本来处于工作区，但是进行了改动，使用 git add &lt;file&gt; 将其加入暂存区，或者使用 git checkout -- &lt;file&gt; 撤销工作区改动。git diff &lt;file&gt; 可以查看文件的改动  </span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;new changes&quot;</span> &gt; readme.txt     <span class="hljs-comment"># 现在工作区和暂存区中的文件是一致的。对文件进行改动</span><br>    git status<br>    git diff<br>    git add readme.txt  |  git checkout -- readme.txt<br><br><span class="hljs-comment"># 提交改动</span><br><span class="hljs-comment"># 使用 git commit 将 暂存区 的修改提交到 git 分支上，每一次 commit 对应 git 分支上的 一个节点。</span><br>git commit -m <span class="hljs-string">&quot;&lt;massage&gt;&quot;</span><br>or<br>git commit  <span class="hljs-comment"># edit commit massage in vim</span><br><br><span class="hljs-comment"># 查看 log</span><br>git <span class="hljs-built_in">log</span><br><br><span class="hljs-comment"># 提交到远端仓库</span><br>    <span class="hljs-comment">## --&gt; 如果本地是一个新仓库，则需要配置远端仓库地址，如果本地仓库是从远端 clone/pull 下来的，则不需要配置</span><br>    git remote add origin &lt;url&gt;  <span class="hljs-comment"># url 示例 https://github.com/clover978/tsn-pytorch</span><br>git push origin master  <span class="hljs-comment"># 将本地分支 push 到远端的 master 分支 （origin 代表远端仓库的地址）</span><br><br><span class="hljs-comment"># 从远程仓库拉取。代码提交到远端之后，就可以在别的地方拉取代码了</span><br>    <span class="hljs-comment">## --&gt; 如果没有从远端拉取过代码，则使用 git clone 克隆一个新的代码仓库</span><br>    git <span class="hljs-built_in">clone</span> &lt;url&gt;<br>    or <br>    git remote add origin &lt;url&gt;  <span class="hljs-comment"># 配置远端仓库地址，然后拉取</span><br>git pull origin master  <span class="hljs-comment"># 拉取远端的 master 分支到本地</span><br></code></pre></td></tr></table></figure></p><h1 id="Git-commitzen"><a href="#Git-commitzen" class="headerlink" title="Git commitzen"></a>Git commitzen</h1><h2 id="0-Intro-1"><a href="#0-Intro-1" class="headerlink" title="0.. Intro"></a>0.. Intro</h2><p><a href="https://github.com/pigcan/blog/issues/15">用工具思路规范化 git commit massage</a><br><strong><code>Git commitzen</code></strong> 是一个 规范 commit massage 的工具。<br>掌握了 <code>git</code> 之后，自然也就知道 commit massage 是什么东西了。commit massage 需不需要规范化属于个人喜好问题，这篇笔记推荐一种目前流行的 commit massage 规范以及配置方法。</p><h2 id="1-Angular-Git-Commit-Guidelines"><a href="#1-Angular-Git-Commit-Guidelines" class="headerlink" title="1.. Angular Git Commit Guidelines"></a>1.. Angular Git Commit Guidelines</h2><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-params">&lt;type&gt;</span>(<span class="hljs-params">&lt;scope&gt;</span>): <span class="hljs-params">&lt;subject&gt;</span><br><span class="hljs-params">&lt;BLANK LINE&gt;</span><br><span class="hljs-params">&lt;body&gt;</span><br><span class="hljs-params">&lt;BLANK LINE&gt;</span><br><span class="hljs-params">&lt;footer&gt;</span><br><br><span class="hljs-symbol">type:</span> 本次 commit 的类型，诸如 bugfix docs style 等<br><span class="hljs-symbol">scope:</span> 本次 commit 波及的范围<br><span class="hljs-symbol">subject:</span> 简明扼要的阐述下本次 commit 的主旨，在原文中特意强调了几点 <span class="hljs-number">1.</span> 使用祈使句 <span class="hljs-number">2.</span> 首字母不要大写 <span class="hljs-number">3.</span> 结尾无需添加标点<br><span class="hljs-symbol">body:</span> 同样使用祈使句，在主体内容中我们需要把本次 commit 详细的描述一下，比如此次变更的动机，如需换行，则使用 |<br><span class="hljs-symbol">footer:</span> 描述下与之关联的 issue 或 break change<br></code></pre></td></tr></table></figure><h2 id="2-commitizen"><a href="#2-commitizen" class="headerlink" title="2. commitizen"></a>2. commitizen</h2><p>看完上面规范之后会不会感觉很麻烦，每次都要回忆该怎么写 commit massage。<a href="https://github.com/commitizen/cz-cli">commitzen</a> 就是一个帮助规范化 commit massage 的工具。可以在 github 上欣赏一下该项目的 commits，或者 clone 到本地用 git log 看一下。<br>下面介绍怎么使用 commitzen</p><ul><li><p>命令行安装<br>安装 commitzen-cli 需要先安装 npm</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># windows</span><br>下载安装 node.js. https://nodejs.org/en/<br><span class="hljs-comment"># linux</span><br>apt install npm<br></code></pre></td></tr></table></figure><p>安装 commitzen-cli 及 conventional change log</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cmake">npm <span class="hljs-keyword">install</span> -g commitizen<br>npm <span class="hljs-keyword">install</span> -g cz-conventional-changelog<br>echo &#x27;&#123; <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;cz-conventional-changelog&quot;</span> &#125;&#x27; &gt; ~/.czrc<br></code></pre></td></tr></table></figure><p>使用 <code>git cz</code> 代替 <code>git commit</code> 提交代码</p></li></ul><ul><li><strong><em>VSCode（推荐</em></strong><br>如果使用 vscode 的话，可以直接安装 <code>Visual Studio Code Commitizen Support</code> 插件</li></ul>]]></content>
    
    
    <categories>
      
      <category>others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tesseract-OCR</title>
    <link href="/2021/01/06/others-20210106-Tesseract-OCR/"/>
    <url>/2021/01/06/others-20210106-Tesseract-OCR/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/tesseract-ocr/tesseract">Tesseract </a>是 Github 上一个 OCR 的开源项目，目前最新版已经更新到 v4.0.x，最近小小尝试了一下怎么使用这个开源库，记一下笔记。</p><a id="more"></a><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><p><code>环境： Ubuntu16.04</code><br>Tesseract 可以使用 apt 安装，但是在 Ubuntu18.04 之前的版本，官方库里面的最新版本只有 3.x.x，作为 Ubuntu16.04 用户，需要先添加 PPA，其他系统版本的 PPA 可以在<a href="https://github.com/tesseract-ocr/tesseract/wiki#linux">这里</a>找到。<br><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smali">apt-get install python-software-properties software-properties-common<br>add-apt-repository ppa:alex-p/tesseract-ocr<br>apt-get update<br></code></pre></td></tr></table></figure><br>添加完 PPA 之后就可以直接 apt 安装了<br><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs q">apt-<span class="hljs-built_in">get</span> install tesseract-ocr libtesseract-<span class="hljs-built_in">dev</span><br></code></pre></td></tr></table></figure><br>除了 apt 安装外，也可以直接从 github clone<br>代码，然后编译安装， 不再详述。</p><h2 id="Language-support"><a href="#Language-support" class="headerlink" title="Language support"></a>Language support</h2><p>Tesseract 的模型文件放在 <code>$TESSDATA_PREFIX</code> 指定的位置，当找不到这个环境变量是，默认路径是 <code>/usr/share/tesseract-ocr/4.00/tessdata</code></p><p>Teseeract 支持的语言及对应的模型文件可以在 <a href="https://github.com/tesseract-ocr/tesseract/wiki/Data-Files#updated-data-files-for-version-400-september-15-2017">这里</a> 找到</p><p>Tesseract 的中文支持有两种方式：</p><ul><li>直接 apt 安装<br><code>apt install tesseract-ocr-chi-sim tesseract-ocr-chi-sim-vert</code></li><li><p>简体中文语言包模型  </p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">wget https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/tesseract-ocr/</span>tessdata<span class="hljs-regexp">/raw/</span><span class="hljs-number">4.00</span>/chi_sim.traineddata<br>mv chi_sim.traineddata <span class="hljs-regexp">/usr/</span>share<span class="hljs-regexp">/tesseract-ocr/</span><span class="hljs-number">4.00</span><span class="hljs-regexp">/tessdata/</span><br></code></pre></td></tr></table></figure><p><code>tesseract --list-langs</code> 查看目前支持的语言</p></li></ul><h2 id="Command-Line-Usage"><a href="#Command-Line-Usage" class="headerlink" title="Command Line Usage"></a>Command Line Usage</h2><p><a href="https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage">https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage</a></p><p>Tesseract 的命令行使用可以参考上面的链接<br>这里列举一个最简答的用法<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs vala">tesseract input.png output -l eng --oem <span class="hljs-number">3</span> --psm <span class="hljs-number">3</span><br><span class="hljs-meta"># input.png： 输入的图片</span><br><span class="hljs-meta"># output： 输出文件名（指定 stdout 输出到标准输出）</span><br><span class="hljs-meta"># -l：指定语言，eng（英语，默认），chi_sim（简体中文）</span><br><span class="hljs-meta"># --oem[0,1,2,3]： OCR Engine Mode，4.x.x 版本默认为 3, 就是使用 available 的模型，即 LSTM 模型</span><br><span class="hljs-meta"># --psm[0-13]： Page Segement Mode，默认为 3，自动分页</span><br><span class="hljs-meta"># Tesseract 还有许多许多其他的参数可以设置，具体可以参照上方的链接</span><br></code></pre></td></tr></table></figure></p><h2 id="Python-API"><a href="#Python-API" class="headerlink" title="Python API"></a>Python API</h2><p><a href="https://github.com/sirfz/tesserocr">https://github.com/sirfz/tesserocr</a><br>Tesseract 官方给出了一个使用 Python API 的 <a href="https://github.com/tesseract-ocr/tesseract/wiki/APIExample#c-api-in-python">脚本</a>，脚本的原理是使用 ctypes + .so/.dll文件，用 Python 调用 C 的接口，这种方法还需要写 platform specified 的代码，所以比较麻烦。我在网上找到有人写了一个 Python Wrapper， 在 <a href="https://github.com/tesseract-ocr/tesseract/wiki/AddOns#tesseract-wrappers">官方 repo</a> 下还可以找到更多 Wrapper 的链接，下面介绍利用这个方法使用 Python 接口。</p><ul><li>安装  <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">apt-<span class="hljs-builtin-name">get</span> install tesseract-ocr libtesseract-dev libleptonica-dev<br><span class="hljs-comment"># 前两个在装 Tesseract 的时候已经安装过了，一定记得要装第三个东西</span><br><span class="hljs-attribute">CPPFLAGS</span>=-I/usr/local/include pip3 install tesserocr<br></code></pre></td></tr></table></figure></li><li>Usage<br>官方 repo 里面给出了很多 接口示例，这里摘抄一个最基本的用法，tesserocr 支持输入 PIL Image 对象或者 图片文件。<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">from</span> tesserocr <span class="hljs-keyword">import</span> PyTessBaseAPI<br><br>images = [<span class="hljs-string">&#x27;sample.jpg&#x27;</span>, <span class="hljs-string">&#x27;sample2.jpg&#x27;</span>, <span class="hljs-string">&#x27;sample3.jpg&#x27;</span>]<br><br><span class="hljs-keyword">with</span> PyTessBaseAPI() <span class="hljs-keyword">as</span> api:<br>    <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> images:<br>        api.SetImageFile(img)<br>        <span class="hljs-built_in">print</span> api.GetUTF8Text()<br>        <span class="hljs-built_in">print</span> api.AllWordConfidences()<br><span class="hljs-comment"># api is automatically finalized when used in a with-statement (context manager).</span><br><span class="hljs-comment"># otherwise api.End() should be explicitly called when it&#x27;s no longer needed.</span><br></code></pre></td></tr></table></figure></li><li><strong>ERROR</strong><br>按照上面的步骤，确实可以安装成功 tesserocr， 然后执行 <code>python3 -c &#39;import tesserocr; print(tesserocr.tesseract_version())&#39;</code>，发现版本号竟然是 3.4.0. 于是开始了漫长的升级之旅<ul><li>卸载 Tesseract3.4.0， tesserocr<br>首先将已经安装的版本卸载掉</li><li>重新安装 tesserocr<br>安装之后又出错了， import 的时候找不着 tesseract.so.3。<br>这个就很烦了，无奈之下我只能从源码开始安装 Tesseract， 再从 源码安装 tesserocr， 最后终于成功了。</li><li>从源码安装 Tesseract<br><a href="https://github.com/tesseract-ocr/tesseract/wiki/Compiling">https://github.com/tesseract-ocr/tesseract/wiki/Compiling</a></li><li>从源码安装 tesserocr  <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">git clone https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/sirfz/</span>tesserocr<br>export LD_LIBRARY_PATH=<span class="hljs-variable">$LD_LIBRARY_PATH</span>:<span class="hljs-regexp">/usr/</span>lib<span class="hljs-regexp">/:/u</span>sr<span class="hljs-regexp">/local/</span>lib/<br>pip3 install .<br></code></pre></td></tr></table></figure><blockquote><p>最后问题是终于解决了，具体这两个东西到底哪个是必须要源码编译，哪个可以用 apt/pip 安装，我也不知道，反正两个全部都编译安装的话是可以成功的。注意安装 tesserocr 的时候，确保电脑上只有最新版的 Tesseract</p></blockquote></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>OCR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Useful bash/powershell commands</title>
    <link href="/2021/01/06/Linux-20210106-Useful-bash-powershell-commands/"/>
    <url>/2021/01/06/Linux-20210106-Useful-bash-powershell-commands/</url>
    
    <content type="html"><![CDATA[<p>收集一些常用的 Windows Powershell 和 Linux bash 命令</p><a id="more"></a><h2 id="1-Powershell"><a href="#1-Powershell" class="headerlink" title="1. Powershell"></a>1. Powershell</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建软连接</span><br>New-Item -Path images -ItemType SymbolicLink -Value E:\coco\images\<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># windows 命令行配置: 设置codepage为 936，GUI的设置经常不能生效</span><br>Windows Registry Editor Version 5.00<br><br>[HKEY_CURRENT_USER\Console\%SystemRoot%_system32_cmd.exe]<br><span class="hljs-string">&quot;CodePage&quot;</span>=dword:0000fde9<br><span class="hljs-string">&quot;FontFamily&quot;</span>=dword:00000036<br><span class="hljs-string">&quot;FontWeight&quot;</span>=dword:00000190<br><span class="hljs-string">&quot;FaceName&quot;</span>=<span class="hljs-string">&quot;Consolas&quot;</span><br><span class="hljs-string">&quot;ScreenBufferSize&quot;</span>=dword:232900d2<br><span class="hljs-string">&quot;WindowSize&quot;</span>=dword:002b00d2<br></code></pre></td></tr></table></figure><h2 id="2-Bash"><a href="#2-Bash" class="headerlink" title="2. Bash"></a>2. Bash</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 同时在 命令行 和 文件中输出</span><br>&lt;cmd&gt; | tee &lt;log-file&gt;<br><span class="hljs-comment"># echo &quot;hello&quot; 2&gt;&amp;1 | tee output.txt</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看终端大小</span><br>stty size<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看程序占用swap</span><br>awk <span class="hljs-string">&#x27;/^Swap:/ &#123;SWAP+=$2&#125;END&#123;print SWAP&quot; KB&quot;&#125;&#x27;</span> /proc/$(pid)/smaps  <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 重启 kde</span><br>kquitapp5 plasmashell &amp;&amp; kstart plasmashell<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 设置系统语言</span><br>apt install locales<br>locale-gen zh_CN.utf8<br>locale-gen zh_CN<br><span class="hljs-built_in">export</span> LANG=zh_CN.utf8<br><span class="hljs-built_in">export</span> LC_ALL=zh_CN.utf8<br><span class="hljs-built_in">export</span> LANGUAGE=zh_CN.utf8<br>update-locale LANG=zh_CN.utf8 LC_ALL=zh_CN.utf8 LANGUAGE=zh_CN.utf8<br>locale -a <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#  sed 命令操作文本</span><br>sed [-i] <span class="hljs-string">&#x27;s/^/HEAD/g&#x27;</span> test.file<br>sed [-i] <span class="hljs-string">&#x27;s/$/TAIL/g&#x27;</span> test.file<br>sed <span class="hljs-string">&#x27;10,20i new_line_between_10_and_20&#x27;</span> test.file<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># vnc</span><br>start: vncserver -geometry 1920x1080 <br><span class="hljs-built_in">exec</span>:  ip:&lt;port&gt;<br>stop: vncserver -<span class="hljs-built_in">kill</span> :&lt;port&gt;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 批量重命名文件</span><br>rename .jpeg .jpg ./*<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># nohup 后台运行程序</span><br>nohup ./run.sh &gt; log.txt &amp;<br><br><span class="hljs-comment"># 其中 &amp; 表示将命令放到后台执行; nohup 表示让进程忽略 HUP 信号。这样，关闭当前窗口后，程序仍然会执行，如果想停止程序，只能通过 ps 找到进程号，然后 kill 掉</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># `screen` 命令提供了一种更强大的后台执行命令方法</span><br><br><span class="hljs-comment"># 新建一个 会话窗口（Session）</span><br>screen -S &lt;会话名称&gt;<br><br><span class="hljs-comment"># 列出所有会话</span><br>screen -ls<br><br><span class="hljs-comment"># 进入一个会话</span><br>screen -r &lt;会话名称&gt;<br><br><span class="hljs-comment"># 进入会话窗口后，就可以执行需要执行的命令了，在执行过程中退出会话不会中断程序</span><br><br><span class="hljs-comment"># 在会话窗口中退出会话</span><br>Ctrl + a + d<br><span class="hljs-comment"># 在会话窗口外退出会话</span><br>screen -d &lt;会话名称&gt;<br><br><br><span class="hljs-comment"># 可以看到 `screen` 将任务放到后台后，支持随时随地查看任务状态，或者与任务进行交互。不需要像 `nohup` 那样必须重定向出 log 文件（当然实际情况下重定向更为方便）。</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将已经执行的前台任务放到后台</span><br><br><span class="hljs-comment"># 执行命令</span><br>./run.sh<br><br><span class="hljs-comment"># 将任务放到后台，此时任务状态是 Stopped</span><br>Ctrl + z<br><br><span class="hljs-comment"># 使用 `jobs` 可以查看任务</span><br><span class="hljs-built_in">jobs</span><br><br><span class="hljs-comment"># 将任务放到后台执行</span><br><span class="hljs-built_in">bg</span> &lt;job-id&gt;<br><span class="hljs-comment"># 将任务放到前台执行</span><br><span class="hljs-built_in">fg</span> &lt;job-id&gt;<br><br><br><span class="hljs-comment"># 任务放到后台之后，一旦关闭当前窗口，这个窗口下的所有任务都会收到 HUP 信号，从而中断运行，因此还需要另外一个命令。  </span><br><span class="hljs-comment"># 使用 disown 命令，让任务不再附属于当前窗口，这样，使用 jobs 命令会发现任务不见了，关闭当前窗口也不会中断任务。</span><br><span class="hljs-built_in">disown</span> -h &lt;job-id&gt;<br><span class="hljs-built_in">disown</span> -ah  <span class="hljs-comment"># 将所有任务移出作业列表</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 批量删除进程</span><br>ps -ef | grep firefox | awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> | xargs <span class="hljs-built_in">kill</span> -9<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># github content DNS 污染</span><br>199.232.68.133 raw.githubusercontent.com<br>199.232.68.133 userimage.githubusercontent.com<br></code></pre></td></tr></table></figure><h2 id="3-图像处理"><a href="#3-图像处理" class="headerlink" title="3. 图像处理"></a>3. 图像处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># resize 视频，保持较短边大于 256  </span><br><br>ffmpeg -i input.mp4 -vf scale=256:256:force_original_aspect_ratio=increase output.mp4<br><span class="hljs-comment"># 这条命令计算出来的 w/h 可能是奇数，对于 libx264 编码，无法生成视频</span><br><br>ffmpeg -i input.mp4 -vf scale=<span class="hljs-keyword">if</span>(lt(iw\,ih)\,256\,-2):<span class="hljs-keyword">if</span>(lt(iw\,ih)\,-2\,256) output.mp4<br><span class="hljs-comment"># 这个命令可以完美实现功能，并且只需要读取一次视频</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 提取视频帧， 设置 stride</span><br><br>ffmpeg -i input.mp4 -vf <span class="hljs-string">&quot;scale=256:256:force_original_aspect_ratio-increase, select=not(mod(n\,4))&quot;</span> -vsync vfr images/img-%06d.jpg<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ffmpeg 抽帧</span><br>ffmpeg -i test.mp4 -r 1 -ss 0:0:0 -t 0:0:3 -s 1280x720 -f image2 %4d.jpg <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ffmpeg 剪切视频</span><br>ffmpeg -ss 00:00:15 -t 00:00:05 -i input.mp4 -vcodec copy -acodec copy output.mp4<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># imagemagick 处理图片</span><br>convert in.jpg -rotate 90 out.jpg<br>convert in.jpg -resize 640x360 out.jpg<br></code></pre></td></tr></table></figure><h2 id="4-pip-镜像源"><a href="#4-pip-镜像源" class="headerlink" title="4. pip 镜像源"></a>4. pip 镜像源</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 临时用法：  </span><br>pip install example.whl -i https://pypi.douban.com/simple --trusted-host=pypi.douban.com<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#  永久配置：</span><br><br><span class="hljs-comment"># vi  ~/.pip/pip.conf </span><br>[global]<br>index-url = https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Packages Collections</title>
    <link href="/2021/01/06/Linux-20210106-Packages-Collections/"/>
    <url>/2021/01/06/Linux-20210106-Packages-Collections/</url>
    
    <content type="html"><![CDATA[<p>收集一些比较冷门的 lib, packages 安装命令</p><a id="more"></a><h2 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu:"></a>Ubuntu:</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">libgthread</span>-<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.so.<span class="hljs-number">0</span>         apt install libglib<span class="hljs-number">2</span>.<span class="hljs-number">0</span>-<span class="hljs-number">0</span><br><span class="hljs-attribute">libSM</span>.so.<span class="hljs-number">6</span>                  apt install libsm<span class="hljs-number">6</span><br><span class="hljs-attribute">libXrender</span>.so.<span class="hljs-number">1</span>             apt install libxrender<span class="hljs-number">1</span><br><span class="hljs-attribute">libXext</span>.so.<span class="hljs-number">6</span>                apt install libxext<span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><h2 id="Python3"><a href="#Python3" class="headerlink" title="Python3"></a>Python3</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pycocotools                 pip <span class="hljs-keyword">install</span> <span class="hljs-string">&quot;git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI&quot;</span><br>skbuild                     pip <span class="hljs-keyword">install</span> scikit-build<br></code></pre></td></tr></table></figure><h2 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h2><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs livescript">apt install -y <span class="hljs-string">\</span><br>    protobuf-compiler <span class="hljs-string">\</span><br>    libprotobuf-dev <span class="hljs-string">\</span><br>    libboost-all-dev <span class="hljs-string">\</span><br>    libgflags-dev <span class="hljs-string">\</span><br>    libgoogle-glog-dev <span class="hljs-string">\</span><br>    libatlas-base-dev <span class="hljs-string">\</span><br>    libopencv-dev <span class="hljs-string">\</span><br>    libhdf5-dev <span class="hljs-string">\</span><br>    libleveldb-dev <span class="hljs-string">\</span><br>    liblmdb-dev <span class="hljs-string">\</span><br>    libsnappy-dev <span class="hljs-string">\</span><br>    python-numpy <span class="hljs-string">\</span><br>    python-setuptools <span class="hljs-string">\</span><br>    python-opencv <span class="hljs-string">\</span><br>    ipython <span class="hljs-string">\</span><br><br>pip install scikit-image protobuf<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu installation</title>
    <link href="/2021/01/06/Linux-20210106-Ubuntu-installation/"/>
    <url>/2021/01/06/Linux-20210106-Ubuntu-installation/</url>
    
    <content type="html"><![CDATA[<h2 id="0-备份数据"><a href="#0-备份数据" class="headerlink" title="0. 备份数据"></a>0. 备份数据</h2><ul><li>bashrc 文件</li><li>docker<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">docker</span> save -o images.tar $image_id<span class="hljs-number">1</span> $ image_id<span class="hljs-number">2</span><br><span class="hljs-attribute">docker</span> export -o container_<span class="hljs-number">1</span>.tar container_id<span class="hljs-number">1</span><br><span class="hljs-attribute">docker</span> export -o container_<span class="hljs-number">2</span>.tar container_id<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure></li><li>记录重要文件路径</li></ul><a id="more"></a><h2 id="1-制作系统盘"><a href="#1-制作系统盘" class="headerlink" title="1. 制作系统盘"></a>1. 制作系统盘</h2><ul><li>下载  <strong>镜像文件</strong></li><li>下载安装 <strong>大白菜</strong></li><li>制作启动盘</li></ul><h2 id="2-安装系统"><a href="#2-安装系统" class="headerlink" title="2. 安装系统"></a>2. 安装系统</h2><ul><li>按照安装流程安装系统</li><li>安装 net-tools, openssh<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">sudo apt <span class="hljs-keyword">update</span> &amp;&amp; sudo apt install -y net-tools, openssh-<span class="hljs-keyword">server</span><br>service ssh <span class="hljs-keyword">start</span><br>ifconfig<br></code></pre></td></tr></table></figure></li><li>关闭图形显示，确认ip地址<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 关闭图形化显示</span><br>sudo systemctl set-default multi-user.target<br>sudo reboot<br>ifconfig<br></code></pre></td></tr></table></figure><h2 id="3-挂载硬盘"><a href="#3-挂载硬盘" class="headerlink" title="3. 挂载硬盘"></a>3. 挂载硬盘</h2></li><li>格式化硬盘，推荐 ext4</li><li>创建挂载点</li><li>挂载硬盘<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo mkfs.ext4 /dev/sda1<br>sudo mkdir /data<br>sudo mount /dev/sda1 /data<br></code></pre></td></tr></table></figure><h2 id="4-更新国内源"><a href="#4-更新国内源" class="headerlink" title="4. 更新国内源"></a>4. 更新国内源</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo mv /etc/apt/sources.list /etc/apt/sourses.list.backup<br>sudo vi /etc/apt/sources.list<br><br>deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse<br>deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse<br>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse<br><br>sudo apt update<br></code></pre></td></tr></table></figure><h2 id="5-安装显卡驱动"><a href="#5-安装显卡驱动" class="headerlink" title="5. 安装显卡驱动"></a>5. 安装显卡驱动</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs vim">下载显卡驱动<br>http<span class="hljs-variable">s:</span>//www.nvidia.<span class="hljs-keyword">cn</span>/Download/<span class="hljs-built_in">index</span>.aspx<br><br>sudo apt install gcc <span class="hljs-keyword">make</span><br>sudo ./NVIDIA-Linux-x86_64-<span class="hljs-number">455.23</span>.<span class="hljs-number">04</span>.run –<span class="hljs-keyword">no</span>-opengl-<span class="hljs-keyword">files</span> -<span class="hljs-keyword">no</span>-<span class="hljs-keyword">x</span>-check -<span class="hljs-keyword">no</span>-nouveau-check<br></code></pre></td></tr></table></figure><h2 id="6-安装-docker"><a href="#6-安装-docker" class="headerlink" title="6. 安装 docker"></a>6. 安装 docker</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 安装 docker-ce</span><br>curl -fsSL https:<span class="hljs-regexp">//g</span>et.docker.com | bash -s docker --mirror Aliyun<br>sudo usermod -aG docker <span class="hljs-variable">$USER</span><br>docker -v<br><br><span class="hljs-comment"># 安装 nvidia-docker2</span><br>curl -s -L https:<span class="hljs-regexp">//</span>nvidia.github.io<span class="hljs-regexp">/nvidia-docker/g</span>pgkey | sudo apt-key add -<br>distribution=$(. <span class="hljs-regexp">/etc/</span>os-release;echo <span class="hljs-variable">$ID</span><span class="hljs-variable">$VERSION_ID</span>)<br>curl -s -L https:<span class="hljs-regexp">//</span>nvidia.github.io<span class="hljs-regexp">/nvidia-docker/</span><span class="hljs-variable">$distribution</span><span class="hljs-regexp">/nvidia-docker.list | sudo tee /</span>etc<span class="hljs-regexp">/apt/</span>sources.list.d/nvidia-docker.list<br>sudo apt-get update<br>sudo apt-get install nvidia-docker2<br><br><span class="hljs-comment"># 配置默认 nvidia runtime</span><br>sudo vi <span class="hljs-regexp">/etc/</span>docker/daemon.json<br>&#123;<br>    <span class="hljs-string">&quot;default-runtime&quot;</span>: <span class="hljs-string">&quot;nvidia&quot;</span>,<br>    <span class="hljs-string">&quot;runtimes&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;nvidia&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;/usr/bin/nvidia-container-runtime&quot;</span>,<br>            <span class="hljs-string">&quot;runtimeArgs&quot;</span>: []<br>        &#125;<br>    &#125;<br>&#125;<br><br>service docker restart<br><br>docker run --rm hello-world nvidia-smi<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>COCO dataset format</title>
    <link href="/2021/01/06/others-20210106-COCO-dataset-format/"/>
    <url>/2021/01/06/others-20210106-COCO-dataset-format/</url>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python">- annotations<br>    - <span class="hljs-string">&#x27;info&#x27;</span>: <span class="hljs-built_in">dict</span><br>    - <span class="hljs-string">&#x27;licenses&#x27;</span> <span class="hljs-built_in">list</span><br>    - <span class="hljs-string">&#x27;images&#x27;</span><br>    [<br>        &#123;<br>        - <span class="hljs-string">&#x27;filename&#x27;</span>: <span class="hljs-built_in">str</span><br>        - <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;height&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;width&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;flickr_url&#x27;</span>: <span class="hljs-built_in">str</span><br>        - <span class="hljs-string">&#x27;license&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;coco_url&#x27;</span>: <span class="hljs-built_in">str</span><br>        - <span class="hljs-string">&#x27;date_captured&#x27;</span>: <span class="hljs-built_in">str</span><br>        &#125;, <br>        ...<br>        ...<br>    ]<br>    - <span class="hljs-string">&#x27;annotations&#x27;</span><br>    [<br>        &#123;<br>        - <span class="hljs-string">&#x27;segmentation&#x27;</span>: <span class="hljs-built_in">list</span><br>        - <span class="hljs-string">&#x27;num_keypoints&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;area&#x27;</span>: <span class="hljs-built_in">float</span><br>        - <span class="hljs-string">&#x27;is_crowd&#x27;</span>: <span class="hljs-built_in">bool</span><br>        - <span class="hljs-string">&#x27;keypoint&#x27;</span>: <br>            - x1, y1, k1   <span class="hljs-comment"># k=0 不可见未标注；k=1 不可见标注； k=2 可见标注</span><br>            - x2, y2, k2<br>            - ......<br>        - <span class="hljs-string">&#x27;bbox&#x27;</span>: <span class="hljs-built_in">list</span><br>            - ( x1, y1, w1, h1 )<br>            - ( x2, y2, w2, h2 )<br>            - ......<br>        - <span class="hljs-string">&#x27;category_id&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;image_id&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-built_in">int</span><br>        &#125;, <br>        ...<br>        ...<br>    ]<br>    - <span class="hljs-string">&#x27;categories&#x27;</span>:<br>    [<br>        &#123;<br>        - <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-built_in">int</span><br>        - <span class="hljs-string">&#x27;keypoints&#x27;</span>:<br>            - kpt-name1<br>            - kpt-name2<br>            - ...<br>        - <span class="hljs-string">&#x27;skeleton&#x27;</span>: <br>            - (pair1_index1, pair1_index2)<br>            - (pair2_index1, pair2_index2)<br>            - ...<br>        - <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;person&#x27;</span><br>        - <span class="hljs-string">&#x27;supercategory&#x27;</span>: <span class="hljs-string">&#x27;person&#x27;</span><br>        &#125;<br>    ]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>COCO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition/</url>
    
    <content type="html"><![CDATA[<h1 id="Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition"><a href="#Boxcars-3d-boxes-as-cnn-input-for-improved-fine-grained-vehicle-recognition" class="headerlink" title="Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition"></a><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Sochor_BoxCars_3D_Boxes_CVPR_2016_paper.pdf">Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition</a></h1><blockquote><ol><li><strong>CVPR 2016</strong></li><li>这篇文章提出了一种使用车辆 3D box 作为网络输入，来识别车型的方法。首先检测出车辆的 3D包围框，然后展开成 2D 图形，最后加入视角信息作为额外输入进行训练。同时还发布了一个新的数据集 <code>BoxCars</code><a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>3D 包围框：<br>文中没有介绍获得车辆的 3D 包围框 的方法。这是作者之前的一个工作。<a href="http://www.bmva.org/bmvc/2014/files/paper013.pdf">Automatic camera  calibration for traffic understanding</a></li><li>车辆图片平展：<br>获得 3D 包围框后，将其展开成 2D 图像，具体方法类似于将立体的纸盒子在平面上铺开。很简单的一个思路。</li><li>车辆的视角信息：<br>作者用三个向量来对车辆的视角信息进行编码。分别是 <code>车尾-车头向量、车内-车外向量，车底-车顶向量</code>。通过这三个向量，可以完整表示出车辆的视角。</li><li>栅格化包围框：<br>作者同时提出另外一种方法描述车辆的视角。将车辆的矩形包围框中用四种颜色进行表示。其中 车顶用黄色，车侧用红色，车头/车尾同蓝色，其他位置用白色。这样一幅新的栅格化图像同样可以确定出车辆的视角。  </li><li>平铺图 + 视角辅助信息 训练：<br>平铺图直接进入 CNN 进行卷积操作；<br>视角编码通过 6x6 的矩阵表示，使用三向量表示的情况下，矩阵的第一行表示 3 个二维向量，其他行用 0 填充；使用栅格化包围框表示的情况下，直接将 bbox rescale 成 6x6 的矩阵。<br>文中只说 view encoding 是加在卷积操作之后，但是具体怎么加没有说明。</li><li>Bbox Cars 数据集：<br>作者同时还公布了一个新的数据集，数据集中的图片是从道路车辆监控视频中获取的，主要特点就是包含了车辆的 3D box 信息。</li></ul><h2 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h2><p>  作者提出的方法需要数据集标注了车辆的 3D box 信息，具体怎么在 CompCars 数据集上做的实验阐述的不太清楚，下面是实验结果：</p><div class="table-container"><table><thead><tr><th></th><th>top1</th><th>top5</th></tr></thead><tbody><tr><td>baseline</td><td>0.767</td><td>0.917</td></tr><tr><td>ours</td><td>0.848</td><td>0.954</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Large-Scale Car Dataset for Fine-grained Categotozation and Verification</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification/</url>
    
    <content type="html"><![CDATA[<h2 id="A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification"><a href="#A-Large-Scale-Car-Dataset-for-Fine-grained-Categotozation-and-Verification" class="headerlink" title="A Large-Scale Car Dataset for Fine-grained Categotozation and Verification"></a><a href="http://arxiv.org/abs/1506.08959">A Large-Scale Car Dataset for Fine-grained Categotozation and Verification</a></h2><blockquote><ol><li><strong>CVPR 2015</strong></li><li>这篇文章提出了一个大型的数据集 CompCars，里面包含了从网络上收集的 13.6w 张整车图片， 2.7w 张车辆局部图片，还有从监控场景下拍摄的 5k 张车辆正脸图片。  </li><li>实验部分使用这个数据集分别用于 <strong>车辆细粒度分类、车辆属性所识别、车辆验证</strong> 三个任务。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>CompCars 数据集：<ul><li>品牌（Make）：163</li><li>车型（Model）：1716</li><li>年份（Year）：4455</li><li>整车图片数：136,727<ul><li>F ：18431</li><li>R ：13513</li><li>S ：23551</li><li>FS：49301</li><li>RS：31150</li></ul></li><li>局部图片数：27,618<ul><li>outter part: <code>headlight, rearlight, fog light, air intake</code></li><li>inter part: <code>console, steering wheel, dashboard, gear lever</code></li></ul></li></ul></li><li><p><strong>Fine-grained classification 数据集：</strong></p><ul><li><p>车型： 431  </p><p>  |               | Train | Test | Total |<br>  |-              |-      |-     |-      |<br>  | 图片数（整车）| 16016 | 14939| 30955 |</p></li></ul></li><li><p><strong>Baseline：</strong>  </p><ul><li>model: overfeat  </li><li>dataset: Fine-grained classification  </li><li><p>result:  </p><p>  | view | F     | R     | S     | FS    | RS    | all   |<br>  |-     |-      |-      |-      |-      |-      |-      |<br>  | top1 | 0.524 | 0.431 | 0.428 | 0.563 | 0.598 | 0.767 |<br>  | top5 | 0.748 | 0.647 | 0.602 | 0.769 | 0.777 | 0.917 |<br>  | Make | 0.701 | 0.521 | 0.507 | 0.680 | 0.656 | 0.829 |<br>单视角识别车型： <code>RS</code> 效果最好；<br>单视角识别品牌： <code>F</code> 效果最好；<br>多视角 Baseline： <strong><code>model: 0.767,  make: 0.829</code></strong></p></li></ul></li><li><p>Supplementary experiment:<br>在整个 <code>CompCars</code> 数据集上补充一个实验，比较三种模型的效果  </p><p>  | model | AlexNet | Overfeat | GoogLeNet |<br>  |-      |-        |-         |-          |<br>  | top1  | 0.819   | 0.879    | 0.912     |<br>  | top5  | 0.949   | 0.969    | 0.981     |</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<h1 id="Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition"><a href="#Multi-Attention-Multi-Class-Constraint-for-Fine-grained-Image-Recognition" class="headerlink" title="Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition"></a><a href="https://arxiv.org/pdf/1806.05372.pdf">Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition</a></h1><blockquote><ol><li><strong>arxiv(Baidu)</strong></li><li>这篇文章的思路是 attention + 度量学习。其中，attention 部分跟 SENet 如出一辙；度量学习比较像 contrastive loss。文章中定义了四种类型的特征，sasc, sadc, dasc, dadc, （s: same, a: attention, d: different, c: class），进行度量学习的时候，定义只有 sasc 是正样本，约束相同类别在相同attention区域学习到相似的特征。<br>3.s 在 CUB 数据集的准确率达到了 86.5。从 这篇文章 和 MSRA的WS-LAN 来看，attention + 度量学习 的方法是细粒度分类里面比较好的一个研究方向。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>OSME (One-Squeeze Multi-Excitation Attention Module)：<br>这一部分，按照我的理解，可以说是完全照搬的 SENet，没有任何创新点。作者说明了 SENet 中 SE module 的原理。文中提出的网络结构，将提取到的特征，分别经过两个 SE module，得到的两个 re-weighted feature map，称为 attention1， attention2.<br>我理解的 OSME module 与 SE module 的区别就是一个是单路的，一个是双路的。</li><li>Multi-Attention Multi-Class Constraint：<br>这一部分，作者使用了度量学习的方法，每次训练的时候，网络会输入 2N 对图片，其中每一对图片都来自于同一类别。然后一对图片分别经过 OSME 的上半支和下半支，这样可以得到四种类别的特征。sasc, sadc, dasc, dadc，在 softmax 的基础上，作者对 hinge loss 进行改进，提出 MAMC Constraint，四种特征。挑选出一个特征作为 anchor，然后分三种情况：<ul><li>正样本是 sasc， 负样本是 {sadc, dasc, dadc};</li><li>正样本是 sadc， 负样本是 {dadc};</li><li>正样本是 dasc， 负样本是 {dadc};<br>在每种情况下，定义出 MAMC loss，最小化与正样本的特征距离，最大化与负样本的特征距离。</li></ul></li></ul><h2 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h2><div class="table-container"><table><thead><tr><th></th><th>acc</th></tr></thead><tbody><tr><td>VGG19</td><td>79.0</td></tr><tr><td>ResNet50</td><td>81.7</td></tr><tr><td>ResNet101</td><td>82.5</td></tr><tr><td>ResNet-50 + OSME</td><td>84.9</td></tr><tr><td>ResNet-50 + OSME + MAMC_1</td><td>85.4</td></tr><tr><td>ResNet-50 + OSME + MAMC</td><td>86.2</td></tr><tr><td>ResNet-50 + OSME_3 + MAMC</td><td>86.3</td></tr><tr><td>ResNet-101 + OSME + MAMC</td><td>86.5</td></tr><tr><td>RACNN</td><td>85.3</td></tr><tr><td>MACNN</td><td>86.5</td></tr></tbody></table></div><pre><code>OSME_3: 使用 3 个 attention  MAMC_1：定义 MAMC loss 的时候，只区分第一种情况。  使用 ResNet50 作为 backbone， OSME 提升了 3.2%， MAMC 提升了 1.3% 的准确率。值得一提的是，相比于 MACNN，文中提出的网络结构要简单的多，在效率上具有很大的优势。</code></pre>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fine-grained Image Classification by Visual-Semantic Embedding</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-Fine-grained-Image-Classification-by-Visual-Semantic-Embedding/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-Fine-grained-Image-Classification-by-Visual-Semantic-Embedding/</url>
    
    <content type="html"><![CDATA[<h1 id="Fine-grained-Image-Classification-by-Visual-Semantic-Embedding"><a href="#Fine-grained-Image-Classification-by-Visual-Semantic-Embedding" class="headerlink" title="Fine-grained Image Classification by Visual-Semantic Embedding"></a><a href="https://www.ijcai.org/proceedings/2018/0145.pdf">Fine-grained Image Classification by Visual-Semantic Embedding</a></h1><blockquote><ol><li><strong>IJCAI 2018</strong></li><li>这篇文章的创新点是利用到了细粒度分类中子类别的语义信息。文中提到了两种语义信息，一种是 <strong>text context</strong>，以 CUB 数据集为例，wiki 上对某一子类鸟的描述就是 text context；另一种是 <strong>knowledge based context</strong>，收集子类鸟的各种属性，建立知识库，有点属性学习的意思。</li><li>这篇文章的想法很有创新，并且也有很好的效果，在 CUB 数据集上达到 86.2 的准确率。但是没有验证在其他数据集上是否同时有效。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>Two leval CNN:<br>作者设计了一个双层的网络结构，F(a) 是定位网络，F(b) 是回归排序网络，网络结构图参考论文。<ul><li>Localization Network：<br>这一部分是传统的目标检测网络模型。提取 定位网络的特征，与回归网络提取的特征点乘，起到 Attention 的作用。</li><li>Regression Ranking Network：<br>这一部分通过 CNN 提取网络的特征，然后加入定位网络提取的特征作为 Attention，得到视觉特征，接下来通过 FC 层，将视觉特征映射到语义空间，网络的约束条件就是特征在语义空间中的距离，优化网络，减小学习到的语义特征与 gt 在语义空间中的特征距离。作者使用了两种语义空间，因此网络的视觉特征同时平行通过了两个 FC 层。<ul><li><code>Knowledge Base Embedding</code>：<br>这里参考了 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a> 中提出的 <code>TransR</code> 方法。并在此基础上针对细粒度分类问题提出了 <code>Attribute Base Embedding</code>。这一部分属于 知识图谱 的领域，看得不是很明白。<br>首先，传统的 知识库嵌入 中，知识库由 三元组(h, r, t) 组成，其中 h，t 表示两个实体，h 是起点，t 是终点；r 表示是实体之间的关系。实体(h,t) 由 d 维数组表示，关系(r) 由 r 维数组表示，映射矩阵 M(r) 是一个 d*r 的矩阵，将实体空间映射到关系空间。知识图谱的约束条件定义为：f(h, t) = ||h(r) + r - t(r) ||。<br>基于 知识库嵌入 改进的 属性知识库嵌入，三元组(h, r, t)中 实体h 是样本标签y， 关系r 是 has_property_of， 实体t 是样本的属性。通过优化映射矩阵，将样本标签映射到 属性知识库空间。</li><li><code>Text Embedding</code>：<br>文本嵌入部分通过 word2vec 实现。作者首先 finetune 了一个 word2vec 模型，然后利用模型将 类别名称 映射到 文本语义空间。  </li></ul></li></ul></li></ul><h2 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h2><p>  作者在 CUB 数据集上做的实验，按照论文所述，训练的过程中既没有使用 bbox 信息，也没有使用 part annotation 信息，这一点不是很明白，和我理解的训练过程不太一样。<br>  <strong>CUB</strong> 准确率： <strong>0.862</strong></p>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses/</url>
    
    <content type="html"><![CDATA[<h1 id="Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses"><a href="#Fine-Grained-Image-Classification-Using-Modified-DCNNs-Trained-by-Cascaded-Softmax-and-Generalized-Large-Margin-Losses" class="headerlink" title="Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8419081">Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses</a></h1><blockquote><ol><li><strong>TNNLS 2018</strong></li><li>这篇文章提出了两个创新点，一个是 级联softmax结构，另一个是 泛化 large-margin loss。主要的思想就是细粒度分类中的标签是具有层次结构的，large-margin loss 是度量学习中提出的概念，这里在细粒度分类问题中进行了改进。</li><li>文章摆了大量的公式，所以我是没怎么仔细看的，并且它的结果并不很出色。使用 VGG 作为 backbone，在 CUB 上准确率是 77.0，结合 bilinear-CNN，从 84.1 提升到了 85.4<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>Cascaded Softmax Loss：<br>级联softmax，见文中的 <code>插图3</code>，其实是一个很简单的结构，假设识别任务中有 50 个粗分类，每个分类中有 4 个细分类，总共 200 分类。传统的网络，直接通过 fc8(200)+softmax 进行训练，这里改成了 fc8(200)+ softmax + fc9(50)+softmax 训练，并且在 fc7 和 fc(9) 之间添加了一个 skip connection. 网络的 loss 则变成了每一层 softmax 的 loss 相加。</li><li>Generalized Large-Margin Loss：<br>泛化 large-margin loss，这个用起来其实也很简单，就是在 fc7 添加一个 loss层 进行监督，large-margin loss 的作用就是增加类间距离，减小类内距离，在标签具有分层结构的情况下，对每个 level 的标签都进行这样的约束，这一部分文中使用了大量的公式，没有仔细看。</li></ul><h2 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h2><p>  作者的这个改动可以应用到任何 CNN 结构中，所以作者做了大量的实验：</p><div class="table-container"><table><thead><tr><th></th><th>w/o bbox</th><th>with bbox</th></tr></thead><tbody><tr><td>googlenet+SM</td><td>73.6</td><td>77.4</td></tr><tr><td>googlenet+CSM</td><td>74.6</td><td>78.4</td></tr><tr><td>googlenet+SC+CSM</td><td>75.3</td><td>79.0</td></tr><tr><td>googlenet+SM+GLM</td><td>76.8</td><td>80.5</td></tr><tr><td>googlenet+CSM+GLM</td><td>77.1</td><td>81.3</td></tr><tr><td>googlenet+SC+CSM+GLM</td><td>77.6</td><td>82.0</td></tr><tr><td>VGG+SM</td><td>72.5</td><td>78.6</td></tr><tr><td>VGG+SC+CSM+GLM</td><td>77.0</td><td>82.4</td></tr><tr><td>B-CNN</td><td>84.1</td><td>84.8</td></tr><tr><td>B-CNN+SC+CSM+GLM</td><td>85.4</td><td>85.7</td></tr><tr><td>googlenet+SM+contrastive</td><td>74.1</td><td>77.8</td></tr><tr><td>googlenet+SM+triplet</td><td>74.1</td><td>78.0</td></tr><tr><td>googlenet+SM+center loss</td><td>74.5</td><td>78.4</td></tr><tr><td>googlenet+SM+min-max</td><td>75.1</td><td>78.9</td></tr></tbody></table></div><pre><code>`SM: softmax`  `CSM: cascaded softmax`  `SC: skip connection`  `GLM: generlized large-margin loss`  通过对比可以看出：    + GLM 对结果的影响是最大的， 76.8  + 单独 CSM 的效果并不明显，加上 SC 后还能提高一下。 74.6 -&gt; 75.3  + 在 VGG， googlenet 上有明显的提高，但是在 B-CNN 上的提高就比较小了。  + GLM 相对于其他的 度量学习 方法效果也是最好的。</code></pre>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weakly Supervised Local Attention Network for Fine-Grained Visual Classification</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification/</url>
    
    <content type="html"><![CDATA[<h1 id="Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification"><a href="#Weakly-Supervised-Local-Attention-Network-for-Fine-Grained-Visual-Classification" class="headerlink" title="Weakly Supervised Local Attention Network for Fine-Grained Visual Classification"></a><a href="https://arxiv.org/pdf/1808.02152.pdf">Weakly Supervised Local Attention Network for Fine-Grained Visual Classification</a></h1><blockquote><ol><li><strong>arxiv (MSRA)</strong>  </li><li>这篇文章提出一种 LAP(Local Attention Pooling) 的机制，利用 attention map 去提取更具区分性的特征图；文中还提出一种弱监督的方式去训练网络的方法，在训练过程中加入了 attention dropout 和 attention center loss。  </li><li>文中提出的网络 WS-LAN(Weakly Supervised Local Attention Network)在 CUB 数据集上准确率达到了 87.9  <a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>Local Attention Pooling:<br>这一部分通过文中的 <code>插图1</code> 很容易就能看懂。<br>作者首先通过 CNN 分别提取到图片的 特征图 和 注意力图，其中每一个注意力图用于聚焦目标的一个 part 上。然后将特征图与 k 个注意力图分别点乘，得到 k 个 part 的特征图。对 k 个特征图进行卷积核池化操作，得到每一个 part 的特征，将 k 个 part 特征合并到一起形成最终的特征。<br>这一部分实际上与 SE-Net 很相似。SE-Net 将特征经过卷积之后，再经过一个 SE 结构，等同于在卷积过程中对卷积核的不同 channel 赋予不同的权重，上一层的特征会分别与卷积核的不同 channel 进行卷积操作。假设将上一层特征看做 LAN 中的特征图，channel间 attention 最大的 topk 个卷积核看做 LAN 中的注意力图，那么两个网络的区别就是一个是进行 卷积操作，一个是进行 点乘操作。<br>所以这一部分实际上可以看做是一个稍加改动，更为复杂的 SE-Net。但是这种改动是必要的，因为下面的 WS-LAN 需要在这个网络结构上进行训练。</li><li>WS-LAN：<br>这一部分，作者将两个传统网络训练中的 trick 迁移到了上面的 LAN 中，这应该是文章最大的两个提升点。<br>这部分的两个 trick 实际上想解决的问题只有一个，就是 提取到的 <strong><em>attention map 很容易只聚焦到目标的一两个最具区分度的区域</em></strong>。<ul><li>attention dropout：<br>作者将 dropout 加入到 LAP 操作中。attention map 和 feature map 进行点乘的时候，attention map 以 概率(1-p) 被 drop 掉，以概率 p 被保留，并乘以 1/p，这个操作与传统的 dropout 如出一辙。</li><li>attention center loss：<br>人脸识别文章中提出过一个 center loss，作者同样将其迁移到 WS-LAN 中。<br>attention map 与 feature map 点乘之后得到 k 个 part feature，作者为这 k 个 feature 维护 k 个 center，然后每次训练的时候，计算每个 feature 和 center 的距离，最小化同一个 part 的 feature distance，最大化不同 part 的 feature distance，作为 attention center loss 的约束条件。</li></ul></li></ul><h1 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h1><div class="table-container"><table><thead><tr><th></th><th>acc</th></tr></thead><tbody><tr><td>LAN</td><td>85.5</td></tr><tr><td>WS-LAN</td><td>87.9</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Embedding label structures for fine-grained feature representation</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-Embedding-label-structures-for-fine-grained-feature-representation/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-Embedding-label-structures-for-fine-grained-feature-representation/</url>
    
    <content type="html"><![CDATA[<h1 id="Embedding-label-structures-for-fine-grained-feature-representation"><a href="#Embedding-label-structures-for-fine-grained-feature-representation" class="headerlink" title="Embedding label structures for fine-grained feature representation"></a><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Embedding_Label_Structures_CVPR_2016_paper.pdf">Embedding label structures for fine-grained feature representation</a></h1><blockquote><ol><li><strong>CVPR 2016</strong></li><li>这篇文章提出两个创新点，一个是使用 “structured label” 改进 <code>triple loss</code>；另一个是同时使用 triple loss 和 softmax loss 进行训练。<a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>同时使用 softmax loss 和 triple loss 约束训练：<br>这个没什么好说的，一个很简单的想法。使用三路的网络，提取特征，然后分为两支，一支过 fc+softmax 然后分类，一支对比 anchor, positive, negative 的特征距离，然后用 triple loss 进行约束。网络结果参照论文链接。</li><li>结构化目标嵌入（embed label structures）:<br>这一部分是对 triple loss 的一个改进，在细粒度分类问题中，目标的分类可以看做层级的，以车辆分类为例，目标结构由粗到细可以是 品牌-模型-年份，因此做模型分类的时候，可以使用四元组进行训练：<code>r(reference), p+(same model), p-(same make, diffrent model), n(diffrent make)</code>，四元组的 loss 等价于 <code>L(r, p+, p-) + L(r, p-, n)</code>。</li></ul><h2 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h2><p>  作者在 <code>standFord Cars</code> 上面进行实验，实验结果略（笔记只记录 CUB 数据集的结果）</p>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The unreasonable effectiveness of noisy data for fine-grained recognition</title>
    <link href="/2021/01/06/Paper-Notes-FGVC-20210106-The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/"/>
    <url>/2021/01/06/Paper-Notes-FGVC-20210106-The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/</url>
    
    <content type="html"><![CDATA[<h1 id="The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition"><a href="#The-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition" class="headerlink" title="The unreasonable effectiveness of noisy data for fine-grained recognition"></a><a href="http://cn.arxiv.org/pdf/1511.06789.pdf">The unreasonable effectiveness of noisy data for fine-grained recognition</a></h1><blockquote><ol><li><strong>ECCV 2016</strong></li><li>这篇文章提出使用网络爬取的数据进行细粒度分类任务。与其说是提出一种新的方法，不如说是新建了一个数据集。  </li><li>网络爬取的图片并不完全准确，因此讨论了对这些噪声的处理方法。  </li><li>加入大量网络数据后，训练的效果得到了大幅度的提升，目前在 CUB 数据集上准确率达到了 92.3  <a id="more"></a></li></ol></blockquote><h2 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h2><ul><li>cross-domain noise:<br><code>cross-domain noise</code> 指的是不属于这一大类的图片，例如搜索某种鸟，出来的结果是昆虫的图片。通过人工标注量化了这种噪声，发现这种情况比较少，并且当图片数目增多的时候，噪声占的比例也会减少。<br>这种噪声对结果的影响也比较小。</li><li>cross-category noise:<br><code>cross-category noise</code> 指的是将其他子类的图片混入搜索结果的情况。这种噪声的比例难以量化，并且对结果的影响比较大，作者将搜索结果中重复的图片去除掉来减少这些噪声。</li><li>active learning：<br>作者还提出两种标注方法辅助去除噪声。1）使用预训练好的模型，挑选搜索结果中置信度高的结果；2）人工筛选搜索结果。</li></ul><h2 id="2-实验结果："><a href="#2-实验结果：" class="headerlink" title="2. 实验结果："></a>2. 实验结果：</h2><div class="table-container"><table><thead><tr><th></th><th>CUB</th><th>web-raw</th><th>web-filter</th><th>L-bird</th><th>L-bird(MC)</th><th>L-bird+CUB</th><th>L-bird+CUB(MC)</th></tr></thead><tbody><tr><td>acc</td><td>84.4</td><td>87.7</td><td>89.0</td><td>91.9</td><td>92.3</td><td>92.2</td><td>92.8</td></tr></tbody></table></div><pre><code>  `CUB`：CUB 数据集    `web-raw`：web 爬取数据集    `web-filter`： 去除 `cross-category noise`    `L-Bird`: 爬取所有鸟类的图片，进行预训练，然后在 web-filter 上 finetune    `MC`：测试的时候使用 multi-crop</code></pre>]]></content>
    
    
    <categories>
      
      <category>Paper Notes</category>
      
      <category>FGVC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>FGVC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo installation</title>
    <link href="/2021/01/06/hexo-20210106-hexo-installation/"/>
    <url>/2021/01/06/hexo-20210106-hexo-installation/</url>
    
    <content type="html"><![CDATA[<hr><h2 id="1-安装-git-node-js-Hexo"><a href="#1-安装-git-node-js-Hexo" class="headerlink" title="1. 安装 git, node.js, Hexo"></a>1. 安装 git, node.js, Hexo</h2><p>  参考<a href="!https://hexo.io/zh-cn/docs/"> Hexo 官方教程</a></p><hr><h2 id="2-添加-github-pages"><a href="#2-添加-github-pages" class="headerlink" title="2. 添加 github pages"></a>2. 添加 github pages</h2><ul><li>新建 github repository  </li><li>新建 hexo 分支，并设置为默认分支</li><li>添加 ssh hey  <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">ssh-keygen</span> <span class="hljs-selector-tag">-t</span> <span class="hljs-selector-tag">rsa</span><br><span class="hljs-selector-tag">cat</span> <span class="hljs-selector-class">.ssh</span>\<span class="hljs-selector-tag">id_rsa</span><span class="hljs-selector-class">.pub</span><br></code></pre></td></tr></table></figure>在 <code>settings -&gt; SSH and GPG keys -&gt; New SSH key</code> 里面添加公钥。</li><li>新建 hexo 博客  <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">git</span> <span class="hljs-selector-tag">clone</span> <span class="hljs-selector-tag">clover978</span><span class="hljs-selector-class">.github</span><span class="hljs-selector-class">.io</span><br><span class="hljs-selector-tag">hexo</span> <span class="hljs-selector-tag">init</span> <span class="hljs-selector-tag">clover978</span><span class="hljs-selector-class">.github</span><span class="hljs-selector-class">.io</span><br><span class="hljs-selector-tag">npm</span> <span class="hljs-selector-tag">install</span> <span class="hljs-selector-tag">hexo-deployer-git</span> <span class="hljs-selector-tag">--save</span><br></code></pre></td></tr></table></figure>在 <code>clover978.github.io/_config.yml</code> 里设置如下字段：    <figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs groovy"><span class="hljs-attr">deploy:</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&#x27;git&#x27;</span><br><span class="hljs-attr">repo:</span> <span class="hljs-attr">https:</span><span class="hljs-comment">//github.com/clover978/clover978.github.io</span><br><span class="hljs-attr">branch:</span> master<br></code></pre></td></tr></table></figure><a id="more"></a></li></ul><hr><h2 id="3-安装-theme"><a href="#3-安装-theme" class="headerlink" title="3. 安装 theme"></a>3. 安装 theme</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">git clone https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/theme-next/</span>hexo-theme-<span class="hljs-keyword">next</span> themes/<span class="hljs-keyword">next</span><br></code></pre></td></tr></table></figure><h2 id="4-其他设置"><a href="#4-其他设置" class="headerlink" title="4. 其他设置"></a>4. 其他设置</h2><p>  参考 <code>_config.yml</code> 和 <code>themes/next/_config.yml</code> 里的各种字段，设置页面布局。</p><h2 id="5-发布-blog"><a href="#5-发布-blog" class="headerlink" title="5. 发布 blog"></a>5. 发布 blog</h2><ul><li>编写 blog<figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gauss">hexo <span class="hljs-keyword">new</span> &lt;<span class="hljs-built_in">title</span>&gt;<br><span class="hljs-meta"># 编辑 _post 里面生成 md 文件</span><br>hexo g<br>hexo d<br></code></pre></td></tr></table></figure><ul><li>同步 hexo<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">git <span class="hljs-keyword">add</span> .<br>git commit -m <span class="hljs-string">&quot;add new blog&quot;</span><br>git <span class="hljs-keyword">push</span><br></code></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
